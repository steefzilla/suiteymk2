#!/usr/bin/env bash

# Suitey - Cross-platform test runner
# This file was generated by build.sh - do not edit directly

# Version: 0.1.0
# Built: 2026-01-14 08:43:16 UTC
# Built on: Linux 6.8.0-90-generic

# Included from: src/environment.sh
#!/usr/bin/env bash

# Suitey Environment Validation Functions
# These functions validate that the development and runtime environment
# is properly configured for Suitey to operate correctly.

# Check if Bash version is 4.0 or higher
check_bash_version() {
    local bash_version
    bash_version=$(bash --version | head -n1 | grep -oE '[0-9]+\.[0-9]+' | head -n1)

    if [[ $(echo "$bash_version >= 4.0" | bc -l) -eq 1 ]]; then
        return 0
    else
        echo "Error: Bash version $bash_version is too old. Suitey requires Bash 4.0 or higher." >&2
        echo "Current version: $bash_version" >&2
        echo "Please upgrade Bash to version 4.0 or higher." >&2
        echo "On Ubuntu/Debian: sudo apt-get install bash" >&2
        echo "On macOS with Homebrew: brew install bash" >&2
        return 1
    fi
}

# Check if Docker is installed and accessible
check_docker_installed() {
    if command -v docker >/dev/null 2>&1; then
        return 0
    else
        echo "Error: Docker is not installed. Suitey requires Docker for containerized builds and test execution." >&2
        echo "Please install Docker:" >&2
        echo "  - Ubuntu/Debian: sudo apt-get install docker.io" >&2
        echo "  - CentOS/RHEL: sudo yum install docker" >&2
        echo "  - macOS: Download from https://www.docker.com/products/docker-desktop" >&2
        echo "  - Windows: Download from https://www.docker.com/products/docker-desktop" >&2
        return 1
    fi
}

# Check if Docker daemon is running
check_docker_daemon_running() {
    if docker info >/dev/null 2>&1; then
        return 0
    else
        echo "Error: Docker daemon is not running. Suitey requires a running Docker daemon." >&2
        echo "Please start Docker:" >&2
        echo "  - Linux: sudo systemctl start docker (or sudo service docker start)" >&2
        echo "  - macOS/Windows: Start Docker Desktop application" >&2
        echo "  - Or run: sudo dockerd (in a separate terminal)" >&2
        return 1
    fi
}

# Check if required directories exist
check_required_directories() {
    local dirs=("src" "tests/bats" "mod")
    local missing_dirs=()

    for dir in "${dirs[@]}"; do
        if [[ ! -d "$dir" ]]; then
            missing_dirs+=("$dir")
        fi
    done

    if [[ ${#missing_dirs[@]} -eq 0 ]]; then
        return 0
    else
        echo "Error: Required directories are missing: ${missing_dirs[*]}" >&2
        echo "Please create the missing directories:" >&2
        for dir in "${missing_dirs[@]}"; do
            echo "  mkdir -p $dir" >&2
        done
        return 1
    fi
}

# Check if /tmp directory is writable
check_tmp_writable() {
    if [[ -w "/tmp" ]]; then
        return 0
    else
        echo "Error: /tmp directory is not writable. Suitey requires write access to /tmp for temporary files." >&2
        echo "Please check /tmp permissions:" >&2
        echo "  ls -ld /tmp" >&2
        echo "If permissions are incorrect, you may need to:" >&2
        echo "  sudo chmod 1777 /tmp" >&2
        return 1
    fi
}

# Check if required test dependencies are available
check_test_dependencies() {
    local deps=("bats")
    local missing_deps=()

    for dep in "${deps[@]}"; do
        if ! command -v "$dep" >/dev/null 2>&1; then
            missing_deps+=("$dep")
        fi
    done

    # Check for bats-support and bats-assert libraries
    if [[ ! -f "tests/bats/test_helper/bats-support/load.bash" ]]; then
        missing_deps+=("bats-support")
    fi

    if [[ ! -f "tests/bats/test_helper/bats-assert/load.bash" ]]; then
        missing_deps+=("bats-assert")
    fi

    if [[ ${#missing_deps[@]} -eq 0 ]]; then
        return 0
    else
        echo "Error: Required test dependencies are missing: ${missing_deps[*]}" >&2
        echo "Please install the missing dependencies:" >&2

        for dep in "${missing_deps[@]}"; do
            case "$dep" in
                "bats")
                    echo "  - BATS testing framework:" >&2
                    echo "    Ubuntu/Debian: sudo apt-get install bats" >&2
                    echo "    macOS: brew install bats-core" >&2
                    echo "    Or download from: https://github.com/bats-core/bats-core" >&2
                    ;;
                "bats-support")
                    echo "  - bats-support library:" >&2
                    echo "    This dependency is now automatically managed as a git submodule." >&2
                    echo "    Use 'git submodule update --init --recursive' if missing." >&2
                    ;;
                "bats-assert")
                    echo "  - bats-assert library:" >&2
                    echo "    This dependency is now automatically managed as a git submodule." >&2
                    echo "    Use 'git submodule update --init --recursive' if missing." >&2
                    ;;
            esac
        done

        return 1
    fi
}

# Check if files can be created in /tmp directory
create_test_file_in_tmp() {
    local test_file="/tmp/suitey_test_file_$$"

    # Try to create a test file in /tmp
    if echo "test content" > "$test_file" 2>/dev/null; then
        # Clean up the test file
        rm -f "$test_file"
        return 0
    else
        echo "Error: Cannot create files in /tmp directory. Suitey requires write access to /tmp." >&2
        return 1
    fi
}

# Verify that filesystem isolation principle is maintained
verify_filesystem_isolation_principle() {
    # This function verifies that Suitey respects filesystem isolation
    # Suitey should only write to /tmp, not modify the project directory
    local project_dir
    project_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

    # Check that project directory exists and is accessible for reading
    if [[ -d "$project_dir" && -r "$project_dir" ]]; then
        # Project directory should be readable for Suitey to function
        # The isolation principle means Suitey won't write here during execution
        return 0
    else
        echo "Error: Project directory is not accessible. This may indicate permission issues." >&2
        return 1
    fi
}

# Check if temporary directories can be created in /tmp
create_test_directory_in_tmp() {
    local test_dir="/tmp/suitey_test_dir_$$"

    # Try to create a test directory in /tmp
    if mkdir "$test_dir" 2>/dev/null; then
        # Clean up the test directory
        rmdir "$test_dir"
        return 0
    else
        echo "Error: Cannot create directories in /tmp. Suitey requires write access to /tmp for temporary directories." >&2
        return 1
    fi
}

# Verify that environment checks respect filesystem isolation principle
verify_environment_filesystem_isolation() {
    # This function verifies that all environment validation functions
    # only access /tmp and don't modify the project directory
    local project_dir
    project_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

    # Use a lighter approach - check for temporary files created outside /tmp
    # instead of full checksum comparison
    local temp_files_before
    temp_files_before=$(find "$project_dir" -name "suitey_*" -type f 2>/dev/null | wc -l)

    # Run all environment validation functions
    check_bash_version >/dev/null 2>&1
    check_docker_installed >/dev/null 2>&1
    check_docker_daemon_running >/dev/null 2>&1
    check_required_directories >/dev/null 2>&1
    check_tmp_writable >/dev/null 2>&1
    check_test_dependencies >/dev/null 2>&1

    local temp_files_after
    temp_files_after=$(find "$project_dir" -name "suitey_*" -type f 2>/dev/null | wc -l)

    # Verify that no suitey temporary files were created in project directory
    if [[ "$temp_files_before" -eq "$temp_files_after" ]]; then
        return 0
    else
        echo "Error: Environment validation functions created files outside /tmp. This violates filesystem isolation." >&2
        return 1
    fi
}
# End of: src/environment.sh

# Included from: src/build_manager.sh
#!/usr/bin/env bash

# Build Manager
# Manages Docker container lifecycle for build execution
# Handles container launch, tracking, and cleanup

# Source required dependencies
source "src/data_access.sh" 2>/dev/null || true

# Launch a build container with specified configuration
# Usage: launch_build_container <container_config>
# Returns: Container ID in flat data format
# Behavior: Launches Docker container with read-only project mount and read-write /tmp mount
launch_build_container() {
    local container_config="$1"

    # Parse container configuration
    local docker_image=$(echo "$container_config" | grep "^docker_image=" | cut -d'=' -f2 || echo "")
    local project_root=$(echo "$container_config" | grep "^project_root=" | cut -d'=' -f2 || echo ".")
    local working_directory=$(echo "$container_config" | grep "^working_directory=" | cut -d'=' -f2 || echo "/workspace")
    local cpu_cores=$(echo "$container_config" | grep "^cpu_cores=" | cut -d'=' -f2 || echo "0")
    local container_name=$(echo "$container_config" | grep "^container_name=" | cut -d'=' -f2 || echo "")

    # Validate required parameters
    if [[ -z "$docker_image" ]]; then
        echo "Error: docker_image is required" >&2
        echo "container_id="
        echo "container_status=error"
        echo "error_message=docker_image is required"
        return 1
    fi

    if [[ ! -d "$project_root" ]]; then
        echo "Error: Project root directory does not exist: $project_root" >&2
        echo "container_id="
        echo "container_status=error"
        echo "error_message=Project root directory does not exist"
        return 1
    fi

    # Create temporary directory for build artifacts
    local artifact_dir
    artifact_dir=$(mktemp -d -t suitey-build-XXXXXX 2>/dev/null || echo "/tmp/suitey-build-$$")

    # Build Docker run command
    local docker_cmd="docker run -d"
    
    # Mount project directory read-only
    docker_cmd="$docker_cmd --mount type=bind,source=$(realpath "$project_root"),target=/workspace,readonly"
    
    # Mount /tmp directory read-write for artifacts
    docker_cmd="$docker_cmd --mount type=bind,source=$artifact_dir,target=/tmp/build-artifacts"
    
    # Set working directory
    docker_cmd="$docker_cmd -w $working_directory"
    
    # Set CPU cores if specified (0 means use all available)
    # Use allocate_cpu_cores to handle allocation logic
    local allocated_cores
    if [[ "$cpu_cores" != "0" ]] && [[ -n "$cpu_cores" ]]; then
        local allocation_result
        allocation_result=$(allocate_cpu_cores "$cpu_cores" 2>/dev/null || echo "")
        if [[ -n "$allocation_result" ]]; then
            allocated_cores=$(echo "$allocation_result" | grep "^allocated_cores=" | cut -d'=' -f2 || echo "$cpu_cores")
        else
            allocated_cores="$cpu_cores"
        fi
    else
        # Use all available cores
        allocated_cores=$(get_available_cpu_cores)
    fi
    
    # Set CPU cores in Docker command
    if [[ -n "$allocated_cores" ]] && [[ "$allocated_cores" != "0" ]]; then
        docker_cmd="$docker_cmd --cpus=$allocated_cores"
    fi
    
    # Set container name if specified
    if [[ -n "$container_name" ]]; then
        docker_cmd="$docker_cmd --name $container_name"
    else
        # Generate unique container name
        local unique_name="suitey-build-$$-$(date +%s)"
        docker_cmd="$docker_cmd --name $unique_name"
        container_name="$unique_name"
    fi
    
    # Add image
    docker_cmd="$docker_cmd $docker_image"
    
    # Command to keep container running (will be replaced by actual build command)
    docker_cmd="$docker_cmd sleep infinity"

    # Launch container
    local container_output
    container_output=$(eval "$docker_cmd" 2>&1)
    local exit_code=$?

    if [[ $exit_code -ne 0 ]] || [[ -z "$container_output" ]]; then
        echo "Error: Failed to launch container: $container_output" >&2
        echo "container_id="
        echo "container_status=error"
        echo "error_message=Failed to launch container: $container_output"
        return 1
    fi

    # Extract container ID (Docker returns full ID, we'll use short ID for consistency)
    local container_id
    container_id=$(echo "$container_output" | head -1 | tr -d '[:space:]')
    
    # Get short ID for easier handling
    local short_id
    short_id=$(echo "$container_id" | cut -c1-12)

    # Return container information
    echo "container_id=$short_id"
    echo "container_name=$container_name"
    echo "container_status=running"
    echo "artifact_dir=$artifact_dir"
    echo "project_root=$project_root"
    echo "docker_image=$docker_image"
    echo "working_directory=$working_directory"
    
    return 0
}

# Track container status
# Usage: track_container <container_id>
# Returns: Container status in flat data format
track_container() {
    local container_id="$1"

    if [[ -z "$container_id" ]]; then
        echo "container_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    # Try to find container by short ID or full ID
    local found_id
    found_id=$(docker ps -a --format "{{.ID}}" | grep "^${container_id}" | head -1)

    if [[ -z "$found_id" ]]; then
        # Try with short ID match
        found_id=$(docker ps -a --format "{{.ID}}" | grep "^${container_id}" | head -1)
    fi

    if [[ -z "$found_id" ]]; then
        echo "container_status=not_found"
        echo "error_message=Container not found: $container_id"
        return 1
    fi

    # Use the found ID for inspection
    local inspect_id="$found_id"

    # Get container status
    local status
    status=$(docker inspect --format='{{.State.Status}}' "$inspect_id" 2>/dev/null || echo "unknown")

    echo "container_id=$container_id"
    echo "container_status=$status"
    
    # Get additional information if container is running
    if [[ "$status" == "running" ]]; then
        local exit_code
        exit_code=$(docker inspect --format='{{.State.ExitCode}}' "$inspect_id" 2>/dev/null || echo "0")
        echo "exit_code=$exit_code"
    fi

    return 0
}

# Clean up container
# Usage: cleanup_container <container_id>
# Returns: Cleanup status in flat data format
cleanup_container() {
    local container_id="$1"

    if [[ -z "$container_id" ]]; then
        echo "cleanup_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    # Try to find container by short ID or full ID
    local found_id
    found_id=$(docker ps -a --format "{{.ID}}" | grep "^${container_id}" | head -1)

    if [[ -z "$found_id" ]]; then
        # Container not found, but that's okay (might already be cleaned up)
        echo "cleanup_status=success"
        echo "container_id=$container_id"
        echo "message=Container not found (may already be cleaned up)"
        return 0
    fi

    # Use the found ID for operations
    local cleanup_id="$found_id"

    # Stop container if running
    docker stop "$cleanup_id" >/dev/null 2>&1 || true

    # Remove container
    local remove_result
    remove_result=$(docker rm "$cleanup_id" 2>&1)
    local exit_code=$?

    if [[ $exit_code -eq 0 ]]; then
        echo "cleanup_status=success"
        echo "container_id=$container_id"
    else
        echo "cleanup_status=error"
        echo "container_id=$container_id"
        echo "error_message=Failed to remove container: $remove_result"
        return 1
    fi

    return 0
}

# Clean up multiple containers
# Usage: cleanup_containers <container_ids>
# Returns: Cleanup results in flat data format
cleanup_containers() {
    local container_ids="$1"
    local result=""
    local cleaned_count=0
    local failed_count=0

    # Parse container IDs (space or newline separated)
    while IFS= read -r container_id || [[ -n "$container_id" ]]; do
        container_id=$(echo "$container_id" | tr -d '[:space:]')
        
        if [[ -z "$container_id" ]]; then
            continue
        fi

        # Clean up container
        local cleanup_result
        cleanup_result=$(cleanup_container "$container_id" 2>&1)
        local cleanup_status
        cleanup_status=$(echo "$cleanup_result" | grep "^cleanup_status=" | cut -d'=' -f2 || echo "error")

        if [[ "$cleanup_status" == "success" ]]; then
            ((cleaned_count++))
        else
            ((failed_count++))
        fi

        result="${result}${cleanup_result}"$'\n'
    done <<< "$container_ids"

    # Add summary
    result="${result}cleanup_total_count=$((cleaned_count + failed_count))"$'\n'
    result="${result}cleanup_success_count=$cleaned_count"$'\n'
    result="${result}cleanup_failed_count=$failed_count"$'\n'

    echo "$result"
    return 0
}

# Execute build command in a running container
# Usage: execute_build_command <container_id> <build_command>
# Returns: Build execution results in flat data format
# Behavior: Executes command, captures output, tracks duration, detects failures
execute_build_command() {
    local container_id="$1"
    local build_command="$2"

    # Validate required parameters
    if [[ -z "$container_id" ]]; then
        echo "Error: container_id is required" >&2
        echo "build_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    if [[ -z "$build_command" ]]; then
        echo "Error: build_command is required" >&2
        echo "build_status=error"
        echo "error_message=build_command is required"
        return 1
    fi

    # Find the full container ID first
    local found_id
    found_id=$(docker ps -a --format "{{.ID}}" | grep "^${container_id}" | head -1)

    if [[ -z "$found_id" ]]; then
        echo "Error: Container not found: $container_id" >&2
        echo "build_status=error"
        echo "container_status=error"
        echo "error_message=Container not found: $container_id"
        return 1
    fi

    # Verify container exists and is running
    local container_status
    container_status=$(track_container "$container_id" 2>/dev/null | grep "^container_status=" | cut -d'=' -f2 || echo "")
    
    if [[ "$container_status" != "running" ]]; then
        echo "Error: Container is not running: $container_id" >&2
        echo "build_status=error"
        echo "container_status=error"
        echo "error_message=Container is not running (status: $container_status)"
        return 1
    fi

    # Record start time for duration tracking
    local start_time
    start_time=$(date +%s.%N 2>/dev/null || date +%s)

    # Execute build command and capture output
    # Use docker exec to run the command in the container
    # Capture both stdout and stderr separately
    local stdout_file
    stdout_file=$(mktemp -t suitey-build-stdout-XXXXXX 2>/dev/null || echo "/tmp/suitey-build-stdout-$$")
    local stderr_file
    stderr_file=$(mktemp -t suitey-build-stderr-XXXXXX 2>/dev/null || echo "/tmp/suitey-build-stderr-$$")

    # Execute command, capturing stdout and stderr separately
    # Use sh -c to properly handle the command
    # Set PATH to include common locations for build tools
    docker exec "$found_id" sh -c "export PATH=\$PATH:/usr/local/cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin && $build_command" > "$stdout_file" 2> "$stderr_file"
    local exit_code=$?

    # Record end time
    local end_time
    end_time=$(date +%s.%N 2>/dev/null || date +%s)

    # Calculate duration (in seconds with decimal precision)
    local duration
    if command -v bc >/dev/null 2>&1; then
        duration=$(echo "scale=3; $end_time - $start_time" | bc 2>/dev/null || echo "0")
        # Ensure duration is not negative (handle clock adjustments)
        if [[ "$(echo "$duration < 0" | bc 2>/dev/null || echo "0")" = "1" ]]; then
            duration="0.001"
        fi
    else
        # Fallback for systems without bc - use integer arithmetic
        local start_int end_int
        start_int=$(echo "$start_time" | cut -d'.' -f1)
        end_int=$(echo "$end_time" | cut -d'.' -f1)
        if [[ -z "$start_int" ]] || [[ -z "$end_int" ]]; then
            duration="0.0"
        else
            duration=$((end_int - start_int))
            # Ensure non-negative
            if [[ $duration -lt 0 ]]; then
                duration=0
            fi
            duration="${duration}.0"
        fi
    fi

    # Read captured output
    local stdout_content
    stdout_content=$(cat "$stdout_file" 2>/dev/null || echo "")
    local stderr_content
    stderr_content=$(cat "$stderr_file" 2>/dev/null || echo "")

    # Clean up temporary files
    rm -f "$stdout_file" "$stderr_file" 2>/dev/null || true

    # Determine build status
    local build_status
    if [[ $exit_code -eq 0 ]]; then
        build_status="success"
    else
        build_status="failed"
    fi

    # Return results in flat data format
    echo "container_id=$container_id"
    echo "build_status=$build_status"
    echo "exit_code=$exit_code"
    echo "duration=$duration"
    echo "stdout=$stdout_content"
    echo "stderr=$stderr_content"

    return 0
}

# Get available CPU cores on the system
# Usage: get_available_cpu_cores
# Returns: Number of available CPU cores as integer
# Behavior: Detects CPU cores using nproc or fallback methods, handles single-core systems
get_available_cpu_cores() {
    local cores=1  # Default to 1 core

    # Try nproc first (most reliable on Linux)
    if command -v nproc >/dev/null 2>&1; then
        cores=$(nproc 2>/dev/null || echo "1")
    # Try sysctl on macOS/BSD
    elif command -v sysctl >/dev/null 2>&1; then
        cores=$(sysctl -n hw.ncpu 2>/dev/null || echo "1")
    # Try /proc/cpuinfo on Linux
    elif [[ -f /proc/cpuinfo ]]; then
        cores=$(grep -c "^processor" /proc/cpuinfo 2>/dev/null || echo "1")
    fi

    # Ensure we have at least 1 core
    if [[ -z "$cores" ]] || [[ "$cores" -lt 1 ]]; then
        cores=1
    fi

    # Return as integer
    echo "$cores"
    return 0
}

# Get parallel build flags for a build system
# Usage: get_parallel_build_flags <build_system>
# Returns: Parallel build flags string (e.g., "-j4" for make, "--jobs 4" for cargo)
# Behavior: Generates appropriate parallel flags based on build system and available cores
get_parallel_build_flags() {
    local build_system="$1"
    local cores
    cores=$(get_available_cpu_cores)

    # Ensure we have at least 1 core
    if [[ -z "$cores" ]] || [[ "$cores" -lt 1 ]]; then
        cores=1
    fi

    case "$build_system" in
        "cargo"|"rust")
            # Cargo uses --jobs flag
            echo "--jobs $cores"
            ;;
        "make"|"gmake")
            # Make uses -j flag
            echo "-j$cores"
            ;;
        "cmake")
            # CMake uses -j flag with build command
            echo "-j$cores"
            ;;
        "ninja")
            # Ninja uses -j flag
            echo "-j$cores"
            ;;
        "maven"|"mvn")
            # Maven uses -T flag for parallel builds
            echo "-T $cores"
            ;;
        "gradle")
            # Gradle uses --parallel and --max-workers
            echo "--parallel --max-workers=$cores"
            ;;
        *)
            # Unknown build system - return empty or default
            # Some build systems don't need explicit parallel flags
            echo ""
            ;;
    esac

    return 0
}

# Allocate CPU cores for build container
# Usage: allocate_cpu_cores [requested_cores]
# Returns: Number of cores to allocate in flat data format
# Behavior: Allocates cores based on request and availability, handles single-core systems
allocate_cpu_cores() {
    local requested_cores="$1"
    local available_cores
    available_cores=$(get_available_cpu_cores)

    # If no cores requested, use all available
    if [[ -z "$requested_cores" ]] || [[ "$requested_cores" == "0" ]]; then
        echo "allocated_cores=$available_cores"
        echo "available_cores=$available_cores"
        echo "allocation_strategy=all_available"
        return 0
    fi

    # Validate requested cores is a positive integer
    if ! [[ "$requested_cores" =~ ^[0-9]+$ ]]; then
        echo "allocated_cores=1"
        echo "available_cores=$available_cores"
        echo "allocation_strategy=default"
        echo "error_message=Invalid core count requested, using default"
        return 0
    fi

    # Allocate requested cores, but not more than available
    local allocated_cores
    if [[ "$requested_cores" -gt "$available_cores" ]]; then
        allocated_cores=$available_cores
        echo "allocated_cores=$allocated_cores"
        echo "available_cores=$available_cores"
        echo "requested_cores=$requested_cores"
        echo "allocation_strategy=limited_by_availability"
        echo "warning_message=Requested $requested_cores cores but only $available_cores available"
    else
        allocated_cores=$requested_cores
        echo "allocated_cores=$allocated_cores"
        echo "available_cores=$available_cores"
        echo "requested_cores=$requested_cores"
        echo "allocation_strategy=requested"
    fi

    # Ensure at least 1 core is allocated
    if [[ "$allocated_cores" -lt 1 ]]; then
        allocated_cores=1
        echo "allocated_cores=$allocated_cores"
        echo "allocation_strategy=minimum"
    fi

    return 0
}

# Generate Dockerfile for test image
# Usage: generate_test_image_dockerfile <image_config>
# Returns: Dockerfile content as text
# Behavior: Generates Dockerfile with base image, artifacts, source code, and test suites
generate_test_image_dockerfile() {
    local image_config="$1"

    # Parse configuration
    local base_image=$(echo "$image_config" | grep "^base_image=" | cut -d'=' -f2 || echo "")
    local artifact_dir=$(echo "$image_config" | grep "^artifact_dir=" | cut -d'=' -f2 || echo "")
    local project_root=$(echo "$image_config" | grep "^project_root=" | cut -d'=' -f2 || echo ".")
    local framework=$(echo "$image_config" | grep "^framework=" | cut -d'=' -f2 || echo "")

    # Validate required parameters
    if [[ -z "$base_image" ]]; then
        echo "Error: base_image is required" >&2
        return 1
    fi

    # Start Dockerfile
    echo "FROM $base_image"
    echo ""
    echo "# Set working directory"
    echo "WORKDIR /app"
    echo ""

    # Copy build artifacts if artifact directory is specified
    if [[ -n "$artifact_dir" ]] && [[ -d "$artifact_dir" ]]; then
        echo "# Copy build artifacts"
        echo "COPY artifacts/ /app/"
        echo ""
    fi

    # Copy source code if project root is specified
    if [[ -n "$project_root" ]] && [[ -d "$project_root" ]]; then
        echo "# Copy source code"
        # Copy source files (exclude common build/test directories)
        echo "COPY source/ /app/"
        echo ""
    fi

    # Copy test suites
    if [[ -n "$project_root" ]] && [[ -d "$project_root" ]]; then
        echo "# Copy test suites"
        echo "COPY tests/ /app/tests/"
        echo ""
    fi

    # Set environment variables based on framework
    case "$framework" in
        "rust"|"cargo")
            echo "# Rust/Cargo environment"
            echo "ENV CARGO_TARGET_DIR=/app/target"
            echo "ENV RUST_BACKTRACE=1"
            ;;
        "python")
            echo "# Python environment"
            echo "ENV PYTHONPATH=/app"
            ;;
        "node"|"javascript")
            echo "# Node.js environment"
            echo "ENV NODE_PATH=/app"
            ;;
    esac

    echo ""
    echo "# Default command (can be overridden)"
    echo "CMD [\"/bin/sh\"]"

    return 0
}

# Build Docker image from generated Dockerfile
# Usage: build_test_image <image_config>
# Returns: Image build results in flat data format
# Behavior: Builds Docker image with artifacts, source code, and test suites
build_test_image() {
    local image_config="$1"

    # Parse configuration
    local base_image=$(echo "$image_config" | grep "^base_image=" | cut -d'=' -f2 || echo "")
    local artifact_dir=$(echo "$image_config" | grep "^artifact_dir=" | cut -d'=' -f2 || echo "")
    local project_root=$(echo "$image_config" | grep "^project_root=" | cut -d'=' -f2 || echo ".")
    local framework=$(echo "$image_config" | grep "^framework=" | cut -d'=' -f2 || echo "")
    local image_tag=$(echo "$image_config" | grep "^image_tag=" | cut -d'=' -f2 || echo "")

    # Validate required parameters
    if [[ -z "$base_image" ]]; then
        echo "Error: base_image is required" >&2
        echo "build_status=error"
        echo "error_message=base_image is required"
        return 1
    fi

    # Generate image tag if not provided
    if [[ -z "$image_tag" ]]; then
        local timestamp
        timestamp=$(date +%Y%m%d-%H%M%S 2>/dev/null || echo "$(date +%s)")
        image_tag="suitey-test-${framework}-${timestamp}-$$"
    fi

    # Create temporary build context directory
    local build_context
    build_context=$(mktemp -d -t suitey-build-context-XXXXXX 2>/dev/null || echo "/tmp/suitey-build-context-$$")
    mkdir -p "$build_context"

    # Generate Dockerfile
    local dockerfile_content
    dockerfile_content=$(generate_test_image_dockerfile "$image_config" 2>/dev/null)
    if [[ $? -ne 0 ]] || [[ -z "$dockerfile_content" ]]; then
        echo "Error: Failed to generate Dockerfile" >&2
        echo "build_status=error"
        echo "error_message=Failed to generate Dockerfile"
        rm -rf "$build_context"
        return 1
    fi

    # Write Dockerfile to build context
    echo "$dockerfile_content" > "$build_context/Dockerfile"

    # Copy artifacts to build context if specified
    if [[ -n "$artifact_dir" ]] && [[ -d "$artifact_dir" ]]; then
        mkdir -p "$build_context/artifacts"
        cp -r "$artifact_dir"/* "$build_context/artifacts/" 2>/dev/null || true
    fi

    # Copy source code to build context if specified
    if [[ -n "$project_root" ]] && [[ -d "$project_root" ]]; then
        mkdir -p "$build_context/source"
        # Copy source files, excluding common build/test directories
        find "$project_root" -type f \( -name "*.rs" -o -name "*.py" -o -name "*.js" -o -name "*.ts" -o -name "*.sh" -o -name "*.toml" -o -name "*.json" -o -name "*.yaml" -o -name "*.yml" \) ! -path "*/target/*" ! -path "*/node_modules/*" ! -path "*/.git/*" ! -path "*/tests/*" 2>/dev/null | while read -r file; do
            local rel_path="${file#$project_root/}"
            local dest_dir="$build_context/source/$(dirname "$rel_path")"
            mkdir -p "$dest_dir"
            cp "$file" "$build_context/source/$rel_path" 2>/dev/null || true
        done
    fi

    # Copy test suites to build context if specified
    if [[ -n "$project_root" ]] && [[ -d "$project_root/tests" ]]; then
        mkdir -p "$build_context/tests"
        cp -r "$project_root/tests"/* "$build_context/tests/" 2>/dev/null || true
    fi

    # Build Docker image
    local build_output
    build_output=$(cd "$build_context" && docker build -t "$image_tag" . 2>&1)
    local build_exit_code=$?

    # Clean up build context
    rm -rf "$build_context" 2>/dev/null || true

    if [[ $build_exit_code -ne 0 ]]; then
        echo "Error: Failed to build Docker image: $build_output" >&2
        echo "build_status=error"
        echo "image_tag=$image_tag"
        echo "error_message=Failed to build Docker image"
        return 1
    fi

    # Get image ID
    local image_id
    image_id=$(docker images --format "{{.ID}}" "$image_tag" 2>/dev/null | head -1)

    # Return results
    echo "build_status=success"
    echo "image_tag=$image_tag"
    echo "image_id=$image_id"
    echo "base_image=$base_image"
    echo "framework=$framework"

    return 0
}

# Verify test image contains required components
# Usage: verify_test_image <verification_config>
# Returns: Verification results in flat data format
# Behavior: Verifies image contains artifacts, source code, and test suites
verify_test_image() {
    local verification_config="$1"

    # Parse configuration
    local image_tag=$(echo "$verification_config" | grep "^image_tag=" | cut -d'=' -f2 || echo "")
    local artifact_paths=$(echo "$verification_config" | grep "^artifact_paths=" | cut -d'=' -f2- || echo "")
    local source_paths=$(echo "$verification_config" | grep "^source_paths=" | cut -d'=' -f2- || echo "")
    local test_suite_paths=$(echo "$verification_config" | grep "^test_suite_paths=" | cut -d'=' -f2- || echo "")

    # Validate required parameters
    if [[ -z "$image_tag" ]]; then
        echo "Error: image_tag is required" >&2
        echo "verification_status=error"
        echo "error_message=image_tag is required"
        return 1
    fi

    # Check if image exists
    if ! docker images --format "{{.Repository}}:{{.Tag}}" | grep -q "^${image_tag}" 2>/dev/null; then
        # Try without tag (use latest)
        if ! docker images --format "{{.ID}}" | grep -q "^${image_tag}" 2>/dev/null; then
            echo "Error: Image not found: $image_tag" >&2
            echo "verification_status=error"
            echo "error_message=Image not found: $image_tag"
            return 1
        fi
    fi

    local artifacts_verified="false"
    local source_verified="false"
    local test_suites_verified="false"
    local all_verified="true"

    # Verify artifacts if specified
    if [[ -n "$artifact_paths" ]]; then
        local artifact_found="true"
        while IFS= read -r path || [[ -n "$path" ]]; do
            if [[ -n "$path" ]]; then
                path=$(echo "$path" | tr -d '[:space:]')
                if ! docker run --rm "$image_tag" test -f "$path" 2>/dev/null && ! docker run --rm "$image_tag" test -d "$path" 2>/dev/null; then
                    artifact_found="false"
                    break
                fi
            fi
        done < <(echo "$artifact_paths" | tr ':' '\n')
        if [[ "$artifact_found" == "true" ]]; then
            artifacts_verified="true"
        else
            all_verified="false"
        fi
    fi

    # Verify source code if specified
    if [[ -n "$source_paths" ]]; then
        local source_found="true"
        while IFS= read -r path || [[ -n "$path" ]]; do
            if [[ -n "$path" ]]; then
                path=$(echo "$path" | tr -d '[:space:]')
                if ! docker run --rm "$image_tag" test -f "$path" 2>/dev/null && ! docker run --rm "$image_tag" test -d "$path" 2>/dev/null; then
                    source_found="false"
                    break
                fi
            fi
        done < <(echo "$source_paths" | tr ':' '\n')
        if [[ "$source_found" == "true" ]]; then
            source_verified="true"
        else
            all_verified="false"
        fi
    fi

    # Verify test suites if specified
    if [[ -n "$test_suite_paths" ]]; then
        local test_suite_found="true"
        while IFS= read -r path || [[ -n "$path" ]]; do
            if [[ -n "$path" ]]; then
                path=$(echo "$path" | tr -d '[:space:]')
                if ! docker run --rm "$image_tag" test -f "$path" 2>/dev/null && ! docker run --rm "$image_tag" test -d "$path" 2>/dev/null; then
                    test_suite_found="false"
                    break
                fi
            fi
        done < <(echo "$test_suite_paths" | tr ':' '\n')
        if [[ "$test_suite_found" == "true" ]]; then
            test_suites_verified="true"
        else
            all_verified="false"
        fi
    fi

    # Return verification results
    if [[ "$all_verified" == "true" ]]; then
        echo "verification_status=success"
    else
        echo "verification_status=partial"
    fi
    echo "artifacts_verified=$artifacts_verified"
    echo "source_verified=$source_verified"
    echo "test_suites_verified=$test_suites_verified"
    echo "image_tag=$image_tag"

    return 0
}

# Execute builds in parallel based on dependency analysis
# Usage: execute_builds_parallel <build_config>
# Returns: Build execution results in flat data format
# Behavior: Runs independent builds in parallel, waits for dependencies, handles failures
execute_builds_parallel() {
    local build_config="$1"

    # Parse build steps count
    local build_steps_count
    build_steps_count=$(echo "$build_config" | grep "^build_steps_count=" | cut -d'=' -f2 || echo "0")

    # Handle empty configuration
    if [[ -z "$build_steps_count" ]] || [[ "$build_steps_count" -eq 0 ]]; then
        echo "builds_completed=0"
        echo "builds_failed=0"
        echo "builds_aborted=0"
        echo "execution_status=success"
        return 0
    fi

    # Parse parallel groups
    local parallel_groups_count
    parallel_groups_count=$(echo "$build_config" | grep "^parallel_groups_count=" | cut -d'=' -f2 || echo "0")

    # Track build results
    local builds_completed=0
    local builds_failed=0
    local builds_aborted=0
    local execution_status="success"
    local result=""

    # Create temporary directory for build state
    local state_dir
    state_dir=$(mktemp -d -t suitey-build-state-XXXXXX 2>/dev/null || echo "/tmp/suitey-build-state-$$")
    mkdir -p "$state_dir"

    # Cleanup function
    cleanup_state() {
        rm -rf "$state_dir" 2>/dev/null || true
    }
    trap cleanup_state EXIT

    # Execute builds by parallel groups
    local group_index=0
    while [[ $group_index -lt "$parallel_groups_count" ]]; do
        # Get steps in this parallel group
        local group_steps
        group_steps=$(echo "$build_config" | grep "^parallel_groups_${group_index}_steps=" | cut -d'=' -f2 || echo "")

        if [[ -z "$group_steps" ]]; then
            ((group_index++))
            continue
        fi

        # Check if any prerequisite builds failed (if so, abort this group)
        local should_abort_group="false"
        while IFS= read -r step_index || [[ -n "$step_index" ]]; do
            step_index=$(echo "$step_index" | tr -d '[:space:]')
            if [[ -z "$step_index" ]]; then
                continue
            fi

            # Check dependencies for this step
            local dependencies
            dependencies=$(echo "$build_config" | grep "^build_steps_${step_index}_dependencies=" | cut -d'=' -f2 || echo "")
            if [[ -n "$dependencies" ]]; then
                while IFS= read -r dep_index || [[ -n "$dep_index" ]]; do
                    dep_index=$(echo "$dep_index" | tr -d '[:space:]')
                    if [[ -n "$dep_index" ]]; then
                        # Check if dependency failed
                        if [[ -f "$state_dir/status_${dep_index}" ]]; then
                            local dep_status
                            dep_status=$(cat "$state_dir/status_${dep_index}" 2>/dev/null || echo "")
                            if [[ "$dep_status" == "failed" ]] || [[ "$dep_status" == "error" ]]; then
                                should_abort_group="true"
                                break
                            fi
                        fi
                    fi
                done < <(echo "$dependencies" | tr ',' '\n')
            fi
            if [[ "$should_abort_group" == "true" ]]; then
                break
            fi
        done < <(echo "$group_steps" | tr ',' '\n')

        # If group should be aborted, mark all steps as aborted
        if [[ "$should_abort_group" == "true" ]]; then
            while IFS= read -r step_index || [[ -n "$step_index" ]]; do
                step_index=$(echo "$step_index" | tr -d '[:space:]')
                if [[ -n "$step_index" ]]; then
                    result="${result}build_steps_${step_index}_build_status=aborted"$'\n'
                    result="${result}build_steps_${step_index}_abort_reason=prerequisite_build_failed"$'\n'
                    ((builds_aborted++))
                fi
            done < <(echo "$group_steps" | tr ',' '\n')
            ((group_index++))
            continue
        fi

        # Launch all builds in this parallel group
        local pids=()
        local step_indices=()
        
        # Collect step indices first (avoid subshell issues)
        while IFS= read -r step_index || [[ -n "$step_index" ]]; do
            step_index=$(echo "$step_index" | tr -d '[:space:]')
            if [[ -n "$step_index" ]]; then
                step_indices+=("$step_index")
            fi
        done < <(echo "$group_steps" | tr ',' '\n')

        # Launch each build in background
        for step_index in "${step_indices[@]}"; do
            # Get build step configuration
            local docker_image
            docker_image=$(echo "$build_config" | grep "^build_steps_${step_index}_docker_image=" | cut -d'=' -f2 || echo "")
            local project_root
            project_root=$(echo "$build_config" | grep "^build_steps_${step_index}_project_root=" | cut -d'=' -f2 || echo ".")
            local build_command
            build_command=$(echo "$build_config" | grep "^build_steps_${step_index}_build_command=" | cut -d'=' -f2- || echo "")

            if [[ -z "$docker_image" ]] || [[ -z "$build_command" ]]; then
                echo "error" > "$state_dir/status_${step_index}"
                {
                    echo "build_steps_${step_index}_build_status=error"
                    echo "build_steps_${step_index}_error_message=missing_configuration"
                } > "$state_dir/result_${step_index}"
                ((builds_failed++))
                continue
            fi

            # Launch build container and execute build in background
            (
                # Launch container
                local container_config="docker_image=$docker_image
project_root=$project_root
working_directory=/workspace
container_name=suitey-build-${step_index}-$$"

                local container_result
                container_result=$(launch_build_container "$container_config" 2>&1)
                local container_id
                container_id=$(echo "$container_result" | grep "^container_id=" | cut -d'=' -f2)

                if [[ -z "$container_id" ]]; then
                    echo "error" > "$state_dir/status_${step_index}"
                    {
                        echo "build_steps_${step_index}_build_status=error"
                        echo "build_steps_${step_index}_error_message=failed_to_launch_container"
                    } > "$state_dir/result_${step_index}"
                    exit 1
                fi

                # Execute build command
                local build_result
                build_result=$(execute_build_command "$container_id" "$build_command" 2>&1)
                local build_status_value
                build_status_value=$(echo "$build_result" | grep "^build_status=" | cut -d'=' -f2 || echo "unknown")

                # Store build status in state file
                echo "$build_status_value" > "$state_dir/status_${step_index}"

                # Store result in state file
                {
                    echo "build_steps_${step_index}_build_status=$build_status_value"
                    echo "$build_result" | sed "s/^/build_steps_${step_index}_/"
                    if [[ "$build_status_value" == "success" ]]; then
                        echo "build_steps_${step_index}_completed=true"
                    else
                        echo "build_steps_${step_index}_completed=false"
                    fi
                } > "$state_dir/result_${step_index}"

                # Clean up container
                cleanup_container "$container_id" >/dev/null 2>&1 || true
            ) &
            pids+=($!)
        done

        # Wait for all builds in this group to complete
        local failed_in_group=0
        for pid in "${pids[@]}"; do
            wait "$pid"
            local pid_exit=$?
            if [[ $pid_exit -ne 0 ]]; then
                ((failed_in_group++))
            fi
        done

        # Collect results from completed builds
        for step_index in "${step_indices[@]}"; do
            # Read result from state file
            if [[ -f "$state_dir/result_${step_index}" ]]; then
                result="${result}$(cat "$state_dir/result_${step_index}")"$'\n'
                
                # Update counters
                local step_status
                step_status=$(cat "$state_dir/status_${step_index}" 2>/dev/null || echo "unknown")
                if [[ "$step_status" == "success" ]]; then
                    ((builds_completed++))
                elif [[ "$step_status" == "failed" ]] || [[ "$step_status" == "error" ]]; then
                    ((builds_failed++))
                fi
            else
                # Build didn't produce a result file - mark as error
                result="${result}build_steps_${step_index}_build_status=error"$'\n'
                result="${result}build_steps_${step_index}_error_message=no_result_file"$'\n'
                ((builds_failed++))
            fi
        done

        ((group_index++))
    done

    # Aggregate final results
    result="${result}builds_completed=$builds_completed"$'\n'
    result="${result}builds_failed=$builds_failed"$'\n'
    result="${result}builds_aborted=$builds_aborted"$'\n'
    
    if [[ $builds_failed -gt 0 ]] || [[ $builds_aborted -gt 0 ]]; then
        execution_status="partial_failure"
        if [[ $builds_failed -gt 0 ]]; then
            execution_status="failure"
        fi
    fi
    
    result="${result}execution_status=$execution_status"$'\n'

    # Cleanup state directory
    cleanup_state
    trap - EXIT

    echo "$result"
    return 0
}


# End of: src/build_manager.sh

# Included from: src/build_system_detector.sh
#!/usr/bin/env bash

# Build System Detector
# Determines if and how projects need to be built before testing
# Aggregates build requirements from all detected platforms
#
# Filesystem Isolation: This module only reads from project directories.
# Build execution happens in isolated Docker containers with read-only
# project access. No modifications are made to the project filesystem.

# Source required dependencies
source "src/data_access.sh" 2>/dev/null || true

# Detect build requirements for detected platforms
# Usage: detect_build_requirements <platform_data>
# Returns: Build requirements in flat data format
# Behavior: Calls detect_build_requirements on each detected platform's module
detect_build_requirements() {
    local platform_data="$1"

    # Initialize result data
    local result=""
    local requires_build=false
    local total_build_commands_count=0
    local total_build_dependencies_count=0
    local total_build_artifacts_count=0

    # Parse platform data to get platforms count
    local platforms_count
    platforms_count=$(data_get "$platform_data" "platforms_count")

    if [[ -z "$platforms_count" ]] || [[ "$platforms_count" -eq 0 ]]; then
        # No platforms detected, no build required
        result="requires_build=false"$'\n'
        result="${result}build_commands_count=0"$'\n'
        result="${result}build_dependencies_count=0"$'\n'
        result="${result}build_artifacts_count=0"
        echo "$result"
        return 0
    fi

    # Process each detected platform
    local i=0
    while [[ $i -lt "$platforms_count" ]]; do
        local language
        local framework
        local module_type

        language=$(data_get "$platform_data" "platforms_${i}_language")
        framework=$(data_get "$platform_data" "platforms_${i}_framework")
        module_type=$(data_get "$platform_data" "platforms_${i}_module_type")

        # Determine module file path based on platform type
        local module_file=""
        case "$module_type" in
            "language")
                module_file="mod/languages/${language}/mod.sh"
                ;;
            "framework")
                module_file="mod/frameworks/${framework}/mod.sh"
                ;;
            "tool")
                module_file="mod/tools/${framework}/mod.sh"
                ;;
        esac

        # If module file exists, source it and call detect_build_requirements
        if [[ -n "$module_file" ]] && [[ -f "$module_file" ]]; then
            # Clean up any existing module functions to avoid conflicts
            for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
                unset -f "$method" 2>/dev/null || true
            done

            # Source the module
            source "$module_file" 2>/dev/null || continue

            # Get the project root from the platform data
            local project_root
            project_root=$(data_get "$platform_data" "project_root" || echo ".")

            # Call the module's detect_build_requirements method
            local module_result
            module_result=$(detect_build_requirements "$project_root" "$platform_data" 2>/dev/null || echo "requires_build=false")

            # Check if this platform requires building
            local platform_requires_build
            platform_requires_build=$(echo "$module_result" | grep "^requires_build=" | cut -d'=' -f2 || echo "false")

            if [[ "$platform_requires_build" == "true" ]]; then
                requires_build=true

                # Aggregate build commands
                local build_commands_count
                build_commands_count=$(echo "$module_result" | grep "^build_commands_count=" | cut -d'=' -f2 || echo "0")

                local j=0
                while [[ $j -lt "$build_commands_count" ]]; do
                    local build_command
                    build_command=$(echo "$module_result" | grep "^build_commands_${j}=" | cut -d'=' -f2 || echo "")

                    if [[ -n "$build_command" ]]; then
                        result=$(data_set "$result" "build_commands_${total_build_commands_count}" "$build_command")
                        ((total_build_commands_count++))
                    fi
                    ((j++))
                done

                # Aggregate build dependencies
                local build_dependencies_count
                build_dependencies_count=$(echo "$module_result" | grep "^build_dependencies_count=" | cut -d'=' -f2 || echo "0")

                local j=0
                while [[ $j -lt "$build_dependencies_count" ]]; do
                    local build_dependency
                    build_dependency=$(echo "$module_result" | grep "^build_dependencies_${j}=" | cut -d'=' -f2 || echo "")

                    if [[ -n "$build_dependency" ]]; then
                        result=$(data_set "$result" "build_dependencies_${total_build_dependencies_count}" "$build_dependency")
                        ((total_build_dependencies_count++))
                    fi
                    ((j++))
                done

                # Aggregate build artifacts
                local build_artifacts_count
                build_artifacts_count=$(echo "$module_result" | grep "^build_artifacts_count=" | cut -d'=' -f2 || echo "0")

                local j=0
                while [[ $j -lt "$build_artifacts_count" ]]; do
                    local build_artifact
                    build_artifact=$(echo "$module_result" | grep "^build_artifacts_${j}=" | cut -d'=' -f2 || echo "")

                    if [[ -n "$build_artifact" ]]; then
                        result=$(data_set "$result" "build_artifacts_${total_build_artifacts_count}" "$build_artifact")
                        ((total_build_artifacts_count++))
                    fi
                    ((j++))
                done
            fi
        fi

        ((i++))
    done

    # Set final results
    result=$(data_set "$result" "requires_build" "$requires_build")
    result=$(data_set "$result" "build_commands_count" "$total_build_commands_count")
    result=$(data_set "$result" "build_dependencies_count" "$total_build_dependencies_count")
    result=$(data_set "$result" "build_artifacts_count" "$total_build_artifacts_count")

    echo "$result"
    return 0
}

# Get detailed build steps for detected platforms
# Usage: get_build_steps <platform_data> <build_requirements>
# Returns: Detailed build steps in flat data format
# Behavior: Returns containerized build specifications. Build execution
#           happens in isolated Docker containers with read-only project
#           access. Project directories are never modified.
get_build_steps() {
    local platform_data="$1"
    local build_requirements="$2"

    # Initialize result data
    local result=""
    local total_build_steps_count=0

    # Parse platform data to get platforms count
    local platforms_count
    platforms_count=$(data_get "$platform_data" "platforms_count")

    if [[ -z "$platforms_count" ]] || [[ "$platforms_count" -eq 0 ]]; then
        # No platforms detected, no build steps
        result="build_steps_count=0"
        echo "$result"
        return 0
    fi

    # Check if building is required overall
    local requires_build
    requires_build=$(echo "$build_requirements" | grep "^requires_build=" | cut -d'=' -f2 || echo "false")

    if [[ "$requires_build" != "true" ]]; then
        result="build_steps_count=0"
        echo "$result"
        return 0
    fi

    # Process each detected platform
    local i=0
    while [[ $i -lt "$platforms_count" ]]; do
        local language
        local framework
        local module_type

        language=$(data_get "$platform_data" "platforms_${i}_language")
        framework=$(data_get "$platform_data" "platforms_${i}_framework")
        module_type=$(data_get "$platform_data" "platforms_${i}_module_type")

        # Determine module file path based on platform type
        local module_file=""
        case "$module_type" in
            "language")
                module_file="mod/languages/${language}/mod.sh"
                ;;
            "framework")
                module_file="mod/frameworks/${framework}/mod.sh"
                ;;
            "tool")
                module_file="mod/tools/${framework}/mod.sh"
                ;;
        esac

        # If module file exists, source it and call get_build_steps
        if [[ -n "$module_file" ]] && [[ -f "$module_file" ]]; then
            # Clean up any existing module functions to avoid conflicts
            for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
                unset -f "$method" 2>/dev/null || true
            done

            # Source the module
            source "$module_file" 2>/dev/null || continue

            # Get the project root from the platform data
            local project_root
            project_root=$(data_get "$platform_data" "project_root" || echo ".")

            # Call the module's get_build_steps method
            local module_result
            module_result=$(get_build_steps "$project_root" "$build_requirements" 2>/dev/null || echo "build_steps_count=0")

            # Check if this platform has build steps
            local build_steps_count
            build_steps_count=$(echo "$module_result" | grep "^build_steps_count=" | cut -d'=' -f2 || echo "0")

            local j=0
            while [[ $j -lt "$build_steps_count" ]]; do
                # Copy all build step data with updated index
                local step_prefix="build_steps_${j}"
                local new_step_prefix="build_steps_${total_build_steps_count}"

                # Get all lines for this build step
                local step_lines
                step_lines=$(echo "$module_result" | grep "^${step_prefix}_")

                # Add each line with updated index
                while IFS= read -r line; do
                    if [[ -n "$line" ]]; then
                        local new_line="${line/${step_prefix}_/${new_step_prefix}_}"
                        result=$(data_set "$result" "${new_line%%=*}" "${new_line#*=}")
                    fi
                done <<< "$step_lines"

                ((total_build_steps_count++))
                ((j++))
            done
        fi

        ((i++))
    done

    # Set final results
    result=$(data_set "$result" "build_steps_count" "$total_build_steps_count")

    echo "$result"
    return 0
}

# Analyze dependencies between build steps
# Usage: analyze_build_dependencies <build_steps>
# Returns: Dependency analysis in flat data format
# Behavior: Analyzes build step dependencies and determines execution order
analyze_build_dependencies() {
    local build_steps="$1"

    # Initialize result data
    local result=""
    local execution_order=""
    local parallel_groups_count=0
    local dependency_graph=""

    # Parse build steps count
    local build_steps_count
    build_steps_count=$(echo "$build_steps" | grep "^build_steps_count=" | cut -d'=' -f2 || echo "0")

    if [[ -z "$build_steps_count" ]] || [[ "$build_steps_count" -eq 0 ]]; then
        # No build steps, no dependencies to analyze
        result="execution_order_count=0"$'\n'
        result="${result}parallel_groups_count=0"$'\n'
        result="${result}dependency_graph_count=0"
        echo "$result"
        return 0
    fi

    # For now, implement a simple dependency analysis
    # In a real implementation, this would analyze actual dependencies between build steps
    # For this phase, we'll assume all build steps can run in parallel (no dependencies)

    # Create execution order (simple sequential for now)
    local execution_order_list=""
    local i=0
    while [[ $i -lt "$build_steps_count" ]]; do
        if [[ -n "$execution_order_list" ]]; then
            execution_order_list="${execution_order_list},"
        fi
        execution_order_list="${execution_order_list}${i}"
        ((i++))
    done

    result="execution_order_count=${build_steps_count}"$'\n'
    result="${result}execution_order_steps=${execution_order_list}"$'\n'

    # For now, assume all builds can run in parallel (no dependencies)
    # In a real implementation, this would analyze dependencies and create groups
    result="${result}parallel_groups_count=1"$'\n'
    result="${result}parallel_groups_0_step_count=${build_steps_count}"$'\n'
    result="${result}parallel_groups_0_steps=${execution_order_list}"$'\n'

    # Simple dependency graph (no dependencies for now)
    result="${result}dependency_graph_count=0"

    echo "$result"
    return 0
}

# End of: src/build_system_detector.sh

# Included from: src/data_access.sh
#!/usr/bin/env bash

# Suitey Data Access Functions
# Pure Bash data manipulation utilities for the flat data format
# No external dependencies

# Extract a value from data using a key path
# Usage: data_get <data> <key>
# Returns: Extracted value as string, or empty string if not found
# Exit code: 0 on success, 1 on error (empty inputs)
data_get() {
    local data="$1"
    local key="$2"

    # Validate inputs
    if [[ -z "$data" ]] || [[ -z "$key" ]]; then
        return 1
    fi

    # Search for the first occurrence of key= in the data
    local line
    line=$(echo "$data" | grep -m 1 "^${key}=" || true)

    # If not found, return empty string
    if [[ -z "$line" ]]; then
        echo ""
        return 0
    fi

    # Extract value after the = sign
    local value="${line#${key}=}"

    # Remove surrounding quotes if present (both double and single quotes)
    # This handles values like "quoted value" or 'single quoted'
    if [[ "$value" =~ ^\".*\"$ ]]; then
        # Remove double quotes
        value="${value#\"}"
        value="${value%\"}"
    elif [[ "$value" =~ ^\'.*\'$ ]]; then
        # Remove single quotes
        value="${value#\'}"
        value="${value%\'}"
    fi

    # Output the value
    echo "$value"
    return 0
}

# Extract an array element from data by index
# Usage: data_get_array <data> <array_name> <index>
# Returns: Array element value, or empty if not found
# Exit code: 0 on success, 1 on error (invalid inputs)
# Behavior: Constructs key as ${array_name}_${index} and calls data_get()
data_get_array() {
    local data="$1"
    local array_name="$2"
    local index="$3"

    # Validate inputs
    if [[ -z "$data" ]] || [[ -z "$array_name" ]] || [[ -z "$index" ]]; then
        return 1
    fi

    # Validate index is numeric (0-based, non-negative integer)
    if ! [[ "$index" =~ ^[0-9]+$ ]]; then
        return 1
    fi

    # Construct key as ${array_name}_${index} (e.g., "test_files_0")
    local key="${array_name}_${index}"

    # Call data_get() with the constructed key
    data_get "$data" "$key"
    return $?
}

# Get the count of elements in an array
# Usage: data_array_count <data> <array_name>
# Returns: Array count as integer string, or "0" if not found/invalid
# Exit code: 0 on success, 1 if inputs are empty
# Behavior: Looks for ${array_name}_count key and validates it's numeric
data_array_count() {
    local data="$1"
    local array_name="$2"

    # Validate inputs
    if [[ -z "$data" ]] || [[ -z "$array_name" ]]; then
        return 1
    fi

    # Look for ${array_name}_count key (e.g., "test_files_count")
    local count_key="${array_name}_count"
    local count_value
    count_value=$(data_get "$data" "$count_key")

    # If not found, return "0"
    if [[ -z "$count_value" ]]; then
        echo "0"
        return 0
    fi

    # Validate that the value is numeric (non-negative integer)
    if ! [[ "$count_value" =~ ^[0-9]+$ ]]; then
        echo "0"
        return 0
    fi

    # Return the count
    echo "$count_value"
    return 0
}

# Extract all elements from an array
# Usage: data_get_array_all <data> <array_name>
# Returns: Array elements, one per line (stdout), or empty if array doesn't exist
# Exit code: 0 on success
# Behavior: Gets array count, then iterates from 0 to count-1, calling data_get_array() for each index
data_get_array_all() {
    local data="$1"
    local array_name="$2"

    # Validate inputs (but don't fail - return empty if invalid)
    if [[ -z "$data" ]] || [[ -z "$array_name" ]]; then
        return 0
    fi

    # Get array count using data_array_count()
    local count
    count=$(data_array_count "$data" "$array_name")

    # If count is 0, return empty (no elements to return)
    if [[ "$count" == "0" ]]; then
        return 0
    fi

    # Iterate from 0 to count-1, retrieving each element
    local i=0
    while [[ $i -lt $count ]]; do
        # Get array element at index i using data_get_array()
        local element
        element=$(data_get_array "$data" "$array_name" "$i")
        echo "$element"
        i=$((i + 1))
    done

    return 0
}

# Set a value in data, creating new data with updated value
# Usage: data_set <data> <key> <value>
# Returns: Updated data string (stdout)
# Exit code: 0 on success, 1 if key is empty
# Behavior: Removes existing key if present, escapes value if needed, appends new key-value pair
data_set() {
    local data="$1"
    local key="$2"
    local value="$3"

    # Validate key is not empty
    if [[ -z "$key" ]]; then
        return 1
    fi

    # Remove existing key if present (including multi-line heredoc blocks)
    # Remove lines matching "^${key}="
    local filtered_data
    filtered_data=$(echo "$data" | grep -v "^${key}=" || true)
    
    # Remove heredoc blocks for this key (lines between "${key}<<EOF" and "EOF")
    local cleaned_data=""
    local in_heredoc=false
    local heredoc_start="${key}<<EOF"
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Check if line starts with heredoc marker
        local line_prefix="${line%%<<*}"
        if [[ "$line_prefix" == "$key" ]] && [[ "$line" == *"<<"* ]]; then
            in_heredoc=true
            continue
        fi
        
        # Check if we're ending a heredoc block
        if [[ "$in_heredoc" == true ]] && [[ "$line" == "EOF" ]]; then
            in_heredoc=false
            continue
        fi
        
        # Only include lines that are not part of a heredoc block
        if [[ "$in_heredoc" == false ]]; then
            if [[ -n "$cleaned_data" ]]; then
                cleaned_data="${cleaned_data}"$'\n'"${line}"
            else
                cleaned_data="${line}"
            fi
        fi
    done <<< "$filtered_data"
    
    # If cleaned_data is empty, use filtered_data
    if [[ -z "$cleaned_data" ]]; then
        cleaned_data="$filtered_data"
    fi

    # Escape value if it contains special characters (spaces, $, `, ", \)
    local escaped_value="$value"
    # Check if value contains special characters that need escaping
    if [[ "$value" =~ [[:space:]] ]] || [[ "$value" =~ \$ ]] || [[ "$value" =~ \` ]] || [[ "$value" =~ \" ]] || [[ "$value" =~ \\ ]]; then
        # Wrap in quotes and escape internal quotes and backslashes
        escaped_value=$(echo "$value" | sed 's/\\/\\\\/g; s/"/\\"/g')
        escaped_value="\"${escaped_value}\""
    fi

    # Append new key-value pair to data
    local result
    if [[ -n "$cleaned_data" ]]; then
        result="${cleaned_data}"$'\n'"${key}=${escaped_value}"
    else
        result="${key}=${escaped_value}"
    fi

    # Output the updated data
    echo "$result"
    return 0
}

# Append a value to an array
# Usage: data_array_append <data> <array_name> <value>
# Returns: Updated data string with new array element
# Exit code: 0 on success
# Behavior: Gets current count, sets new element at index count, updates count to count+1
data_array_append() {
    local data="$1"
    local array_name="$2"
    local value="$3"

    # Get current array count
    local count
    count=$(data_array_count "$data" "$array_name")
    
    # If count failed or returned error, default to 0
    if [[ $? -ne 0 ]] || [[ -z "$count" ]]; then
        count=0
    fi

    # Set new element at index count using data_set()
    data=$(data_set "$data" "${array_name}_${count}" "$value")

    # Update count to count + 1 using data_set()
    local new_count=$((count + 1))
    data=$(data_set "$data" "${array_name}_count" "$new_count")

    # Return updated data string
    echo "$data"
    return 0
}

# Set an entire array, replacing any existing array entries
# Usage: data_set_array <data> <array_name> <value1> [value2] [value3] ...
# Returns: Updated data string
# Exit code: 0 on success
# Behavior: Removes all existing array entries, adds new entries at sequential indices
data_set_array() {
    local data="$1"
    local array_name="$2"
    shift 2  # Remove first two arguments, leaving only values

    # Get current count to know how many entries to remove
    local old_count
    old_count=$(data_array_count "$data" "$array_name")
    
    # Remove all existing array entries (${array_name}_N= and ${array_name}_count=)
    # Remove entries from 0 to old_count-1
    if [[ "$old_count" -gt 0 ]]; then
        local i=0
        while [[ $i -lt $old_count ]]; do
            # Remove the array element line
            data=$(echo "$data" | grep -v "^${array_name}_${i}=" || true)
            i=$((i + 1))
        done
    fi
    
    # Remove the count line
    data=$(echo "$data" | grep -v "^${array_name}_count=" || true)
    
    # Remove empty lines that might have been created
    data=$(echo "$data" | grep -v "^$" || true)
    
    # If data is now empty or only whitespace, reset it
    if [[ -z "${data// }" ]]; then
        data=""
    fi

    # Add new entries for each value at sequential indices
    local index=0
    local values=("$@")
    
    for value in "${values[@]}"; do
        data=$(data_set "$data" "${array_name}_${index}" "$value")
        index=$((index + 1))
    done

    # Set count to number of values added
    data=$(data_set "$data" "${array_name}_count" "$index")

    # Return updated data string
    echo "$data"
    return 0
}

# Set a multi-line value using heredoc syntax
# Usage: data_set_multiline <data> <key> <value>
# Returns: Updated data string with heredoc syntax
# Exit code: 0 on success
# Behavior: Removes existing heredoc block or regular key=value entry, appends new heredoc block
data_set_multiline() {
    local data="$1"
    local key="$2"
    local value="$3"

    # Validate key is not empty
    if [[ -z "$key" ]]; then
        return 1
    fi

    # Remove existing heredoc block if present (lines between `${key}<<EOF` and `EOF`)
    # Also remove regular key=value entry for this key
    local cleaned_data=""
    local in_heredoc=false
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Check if line starts with heredoc marker for this key (e.g., "key<<EOF")
        if [[ "${line%%<<*}" == "$key" ]] && [[ "$line" == *"<<"* ]]; then
            in_heredoc=true
            continue
        fi
        
        # Check if we're ending a heredoc block
        if [[ "$in_heredoc" == true ]] && [[ "$line" == "EOF" ]]; then
            in_heredoc=false
            continue
        fi
        
        # Skip lines that are part of a heredoc block
        if [[ "$in_heredoc" == true ]]; then
            continue
        fi
        
        # Skip regular key=value entry for this key
        if [[ "$line" =~ ^${key}= ]]; then
            continue
        fi
        
        # Include all other lines
        if [[ -n "$cleaned_data" ]]; then
            cleaned_data="${cleaned_data}"$'\n'"${line}"
        else
            cleaned_data="${line}"
        fi
    done <<< "$data"
    
    # If cleaned_data is empty, reset it
    if [[ -z "${cleaned_data// }" ]]; then
        cleaned_data=""
    fi

    # Append new heredoc block: `${key}<<EOF`, value lines, `EOF`
    local result
    if [[ -n "$cleaned_data" ]]; then
        result="${cleaned_data}"$'\n'"${key}<<EOF"
    else
        result="${key}<<EOF"
    fi
    
    # Add value lines if not empty
    if [[ -n "$value" ]]; then
        result="${result}"$'\n'"${value}"
    fi
    
    # Add EOF marker
    result="${result}"$'\n'"EOF"

    # Output the updated data
    echo "$result"
    return 0
}

# Get a multi-line value, handling heredoc syntax
# Usage: data_get_multiline <data> <key>
# Returns: Multi-line value (without heredoc markers), or single-line value
# Exit code: 0 on success, 1 on error (invalid inputs)
# Behavior: Checks if key uses heredoc syntax, extracts content between markers or calls data_get()
data_get_multiline() {
    local data="$1"
    local key="$2"

    # Validate inputs
    if [[ -z "$data" ]] || [[ -z "$key" ]]; then
        return 1
    fi

    # Check if key uses heredoc syntax (`${key}<<EOF`)
    local in_heredoc=false
    local heredoc_content=""
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Check if line starts with heredoc marker for this key (e.g., "key<<EOF")
        if [[ "${line%%<<*}" == "$key" ]] && [[ "$line" == *"<<"* ]]; then
            in_heredoc=true
            continue
        fi
        
        # Check if we're ending a heredoc block
        if [[ "$in_heredoc" == true ]] && [[ "$line" == "EOF" ]]; then
            # Found complete heredoc block, return content
            echo "$heredoc_content"
            return 0
        fi
        
        # Collect content between heredoc markers
        if [[ "$in_heredoc" == true ]]; then
            if [[ -n "$heredoc_content" ]]; then
                heredoc_content="${heredoc_content}"$'\n'"${line}"
            else
                heredoc_content="${line}"
            fi
        fi
    done <<< "$data"
    
    # If we were in a heredoc but didn't find EOF, return what we collected
    if [[ "$in_heredoc" == true ]]; then
        echo "$heredoc_content"
        return 0
    fi
    
    # If not heredoc, call data_get() for regular single-line value
    data_get "$data" "$key"
    return $?
}

# Validate that a string conforms to the data format specification
# Usage: data_validate <data>
# Exit code: 0 if valid data format, 1 if invalid
# Behavior: Validates each line against allowed patterns (key=value, comments, sections, heredoc)
data_validate() {
    local data="$1"

    # Empty input is considered valid
    if [[ -z "$data" ]]; then
        return 0
    fi

    # Validate each line
    local in_heredoc=false
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Skip empty lines
        if [[ -z "${line// }" ]]; then
            continue
        fi

        # Allow comment lines (starting with #)
        if [[ "$line" =~ ^# ]]; then
            continue
        fi

        # Allow section headers ([section_name])
        if [[ "$line" =~ ^\[.*\]$ ]]; then
            continue
        fi

        # Allow heredoc start markers (key<<EOF)
        # Check if line contains << and doesn't start with =
        if [[ "$line" == *"<<"* ]] && [[ "$line" != "="* ]]; then
            in_heredoc=true
            continue
        fi

        # Allow heredoc end markers (EOF)
        if [[ "$line" == "EOF" ]]; then
            in_heredoc=false
            continue
        fi

        # Skip validation for lines inside heredoc blocks
        if [[ "$in_heredoc" == true ]]; then
            continue
        fi

        # Require other lines to match key=value pattern
        if [[ ! "$line" =~ ^[^=]+= ]]; then
            return 1
        fi
    done <<< "$data"

    # All lines are valid
    return 0
}

# Check if a key exists in data
# Usage: data_has_key <data> <key>
# Exit code: 0 if key exists, 1 if not found or invalid inputs
# Behavior: Searches for lines starting with ${key}=
data_has_key() {
    local data="$1"
    local key="$2"

    # Validate inputs
    if [[ -z "$data" ]] || [[ -z "$key" ]]; then
        return 1
    fi

    # Search for lines starting with ${key}=
    # Use grep to find exact match (key= at start of line)
    if echo "$data" | grep -q "^${key}="; then
        return 0
    fi

    # Key not found
    return 1
}

# End of: src/data_access.sh

# Included from: src/execution_system.sh
#!/usr/bin/env bash

# Execution System
# Manages test container lifecycle and test execution
# Handles container launch, test execution, and result collection

# Source required dependencies
source "src/data_access.sh" 2>/dev/null || true
source "src/mod_registry.sh" 2>/dev/null || true

# Launch a test container with pre-built test image
# Usage: launch_test_container <container_config>
# Returns: Container ID in flat data format
# Behavior: Launches Docker container with pre-built test image (no volume mounts needed)
launch_test_container() {
    local container_config="$1"

    # Parse container configuration
    local test_image=$(echo "$container_config" | grep "^test_image=" | cut -d'=' -f2 || echo "")
    local working_directory=$(echo "$container_config" | grep "^working_directory=" | cut -d'=' -f2 || echo "/app")
    local cpu_cores=$(echo "$container_config" | grep "^cpu_cores=" | cut -d'=' -f2 || echo "0")
    local container_name=$(echo "$container_config" | grep "^container_name=" | cut -d'=' -f2 || echo "")

    # Validate required parameters
    if [[ -z "$test_image" ]]; then
        echo "Error: test_image is required" >&2
        echo "container_id="
        echo "container_status=error"
        echo "error_message=test_image is required"
        return 1
    fi

    # Verify test image exists
    if ! docker images --format "{{.Repository}}:{{.Tag}}" | grep -q "^${test_image}" 2>/dev/null && \
       ! docker images --format "{{.ID}}" | grep -q "^${test_image}" 2>/dev/null; then
        echo "Error: Test image not found: $test_image" >&2
        echo "container_id="
        echo "container_status=error"
        echo "error_message=Test image not found: $test_image"
        return 1
    fi

    # Build Docker run command
    local docker_cmd="docker run -d"
    
    # Set working directory
    docker_cmd="$docker_cmd -w $working_directory"
    
    # Set CPU cores if specified (0 means use all available)
    if [[ "$cpu_cores" != "0" ]] && [[ -n "$cpu_cores" ]]; then
        # Use allocate_cpu_cores if available (from build_manager.sh)
        if command -v allocate_cpu_cores >/dev/null 2>&1; then
            local allocation_result
            allocation_result=$(allocate_cpu_cores "$cpu_cores" 2>/dev/null || echo "")
            if [[ -n "$allocation_result" ]]; then
                local allocated_cores
                allocated_cores=$(echo "$allocation_result" | grep "^allocated_cores=" | cut -d'=' -f2 || echo "$cpu_cores")
                if [[ -n "$allocated_cores" ]] && [[ "$allocated_cores" != "0" ]]; then
                    docker_cmd="$docker_cmd --cpus=$allocated_cores"
                fi
            fi
        else
            docker_cmd="$docker_cmd --cpus=$cpu_cores"
        fi
    fi
    
    # Set container name if specified
    if [[ -n "$container_name" ]]; then
        docker_cmd="$docker_cmd --name $container_name"
    else
        # Generate unique container name
        local unique_name="suitey-test-$$-$(date +%s)"
        docker_cmd="$docker_cmd --name $unique_name"
        container_name="$unique_name"
    fi
    
    # Add test image
    docker_cmd="$docker_cmd $test_image"
    
    # Command to keep container running (will be replaced by actual test command)
    docker_cmd="$docker_cmd sleep infinity"

    # Launch container
    local container_output
    container_output=$(eval "$docker_cmd" 2>&1)
    local exit_code=$?

    if [[ $exit_code -ne 0 ]] || [[ -z "$container_output" ]]; then
        echo "Error: Failed to launch container: $container_output" >&2
        echo "container_id="
        echo "container_status=error"
        echo "error_message=Failed to launch container: $container_output"
        return 1
    fi

    # Extract container ID (Docker returns full ID, we'll use short ID for consistency)
    local container_id
    container_id=$(echo "$container_output" | head -1 | tr -d '[:space:]')
    
    # Get short ID for easier handling
    local short_id
    short_id=$(echo "$container_id" | cut -c1-12)

    # Return container information
    echo "container_id=$short_id"
    echo "container_name=$container_name"
    echo "container_status=running"
    echo "test_image=$test_image"
    echo "working_directory=$working_directory"
    
    return 0
}

# Execute test command in a running test container
# Usage: execute_test_command <container_id> <test_command>
# Returns: Test execution results in flat data format
# Behavior: Executes test command, captures output, tracks duration, detects test status
execute_test_command() {
    local container_id="$1"
    local test_command="$2"

    # Validate required parameters
    if [[ -z "$container_id" ]]; then
        echo "Error: container_id is required" >&2
        echo "test_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    if [[ -z "$test_command" ]]; then
        echo "Error: test_command is required" >&2
        echo "test_status=error"
        echo "error_message=test_command is required"
        return 1
    fi

    # Find the full container ID
    local found_id
    found_id=$(docker ps -a --format "{{.ID}}" | grep "^${container_id}" | head -1)

    if [[ -z "$found_id" ]]; then
        echo "Error: Container not found: $container_id" >&2
        echo "test_status=error"
        echo "error_message=Container not found: $container_id"
        return 1
    fi

    # Verify container exists and is running
    local container_status
    container_status=$(docker inspect --format='{{.State.Status}}' "$found_id" 2>/dev/null || echo "unknown")
    
    if [[ "$container_status" != "running" ]]; then
        echo "Error: Container is not running: $container_id" >&2
        echo "test_status=error"
        echo "container_status=$container_status"
        echo "error_message=Container is not running (status: $container_status)"
        return 1
    fi

    # Record start time for duration tracking
    local start_time
    start_time=$(date +%s.%N 2>/dev/null || date +%s)

    # Execute test command and capture output
    # Use docker exec to run the command in the container
    # Capture both stdout and stderr separately
    local stdout_file
    stdout_file=$(mktemp -t suitey-test-stdout-XXXXXX 2>/dev/null || echo "/tmp/suitey-test-stdout-$$")
    local stderr_file
    stderr_file=$(mktemp -t suitey-test-stderr-XXXXXX 2>/dev/null || echo "/tmp/suitey-test-stderr-$$")

    # Execute command, capturing stdout and stderr separately
    # Use sh -c to properly handle the command
    # Set PATH to include common locations for test runners
    docker exec "$found_id" sh -c "export PATH=\$PATH:/usr/local/cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin && $test_command" > "$stdout_file" 2> "$stderr_file"
    local exit_code=$?

    # Record end time
    local end_time
    end_time=$(date +%s.%N 2>/dev/null || date +%s)

    # Calculate duration (in seconds with decimal precision)
    local duration
    if command -v bc >/dev/null 2>&1; then
        duration=$(echo "scale=3; $end_time - $start_time" | bc 2>/dev/null || echo "0")
        # Ensure duration is not negative (handle clock adjustments)
        if [[ "$(echo "$duration < 0" | bc 2>/dev/null || echo "0")" = "1" ]]; then
            duration="0.001"
        fi
    else
        # Fallback for systems without bc - use integer arithmetic
        local start_int end_int
        start_int=$(echo "$start_time" | cut -d'.' -f1)
        end_int=$(echo "$end_time" | cut -d'.' -f1)
        if [[ -z "$start_int" ]] || [[ -z "$end_int" ]]; then
            duration="0.0"
        else
            duration=$((end_int - start_int))
            # Ensure non-negative
            if [[ $duration -lt 0 ]]; then
                duration=0
            fi
            duration="${duration}.0"
        fi
    fi

    # Read captured output
    local stdout_content
    stdout_content=$(cat "$stdout_file" 2>/dev/null || echo "")
    local stderr_content
    stderr_content=$(cat "$stderr_file" 2>/dev/null || echo "")

    # Clean up temporary files
    rm -f "$stdout_file" "$stderr_file" 2>/dev/null || true

    # Determine test status
    local test_status
    if [[ $exit_code -eq 0 ]]; then
        test_status="passed"
    else
        test_status="failed"
    fi

    # Return results in flat data format
    echo "container_id=$container_id"
    echo "test_status=$test_status"
    echo "exit_code=$exit_code"
    echo "duration=$duration"
    echo "stdout=$stdout_content"
    echo "stderr=$stderr_content"

    return 0
}

# Track test container status
# Usage: track_test_container <container_id>
# Returns: Container status in flat data format
track_test_container() {
    local container_id="$1"

    if [[ -z "$container_id" ]]; then
        echo "container_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    # Try to find container by short ID or full ID
    local found_id
    found_id=$(docker ps -a --format "{{.ID}}" | grep "^${container_id}" | head -1)

    if [[ -z "$found_id" ]]; then
        echo "container_status=not_found"
        echo "error_message=Container not found: $container_id"
        return 1
    fi

    # Use the found ID for inspection
    local inspect_id="$found_id"

    # Get container status
    local status
    status=$(docker inspect --format='{{.State.Status}}' "$inspect_id" 2>/dev/null || echo "unknown")

    echo "container_id=$container_id"
    echo "container_status=$status"
    
    # Get additional information if container is running
    if [[ "$status" == "running" ]]; then
        local exit_code
        exit_code=$(docker inspect --format='{{.State.ExitCode}}' "$inspect_id" 2>/dev/null || echo "0")
        echo "exit_code=$exit_code"
    fi

    return 0
}

# Clean up test container
# Usage: cleanup_test_container <container_id>
# Returns: Cleanup status in flat data format
cleanup_test_container() {
    local container_id="$1"

    if [[ -z "$container_id" ]]; then
        echo "cleanup_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    # Try to find container by short ID or full ID
    local found_id
    found_id=$(docker ps -a --format "{{.ID}}" | grep "^${container_id}" | head -1)

    if [[ -z "$found_id" ]]; then
        # Container not found, but that's okay (might already be cleaned up)
        echo "cleanup_status=success"
        echo "container_id=$container_id"
        echo "message=Container not found (may already be cleaned up)"
        return 0
    fi

    # Use the found ID for operations
    local cleanup_id="$found_id"

    # Stop container if running
    docker stop "$cleanup_id" >/dev/null 2>&1 || true

    # Remove container
    local remove_result
    remove_result=$(docker rm "$cleanup_id" 2>&1)
    local exit_code=$?

    if [[ $exit_code -eq 0 ]]; then
        echo "cleanup_status=success"
        echo "container_id=$container_id"
    else
        echo "cleanup_status=error"
        echo "container_id=$container_id"
        echo "error_message=Failed to remove container: $remove_result"
        return 1
    fi

    return 0
}

# Clean up multiple test containers
# Usage: cleanup_test_containers <container_ids>
# Returns: Cleanup results in flat data format
cleanup_test_containers() {
    local container_ids="$1"
    local result=""
    local cleaned_count=0
    local failed_count=0

    # Parse container IDs (space or newline separated)
    while IFS= read -r container_id || [[ -n "$container_id" ]]; do
        container_id=$(echo "$container_id" | tr -d '[:space:]')
        
        if [[ -z "$container_id" ]]; then
            continue
        fi

        # Clean up container
        local cleanup_result
        cleanup_result=$(cleanup_test_container "$container_id" 2>&1)
        local cleanup_status
        cleanup_status=$(echo "$cleanup_result" | grep "^cleanup_status=" | cut -d'=' -f2 || echo "error")

        if [[ "$cleanup_status" == "success" ]]; then
            ((cleaned_count++))
        else
            ((failed_count++))
        fi

        result="${result}${cleanup_result}"$'\n'
    done <<< "$container_ids"

    # Add summary
    result="${result}cleanup_total_count=$((cleaned_count + failed_count))"$'\n'
    result="${result}cleanup_success_count=$cleaned_count"$'\n'
    result="${result}cleanup_failed_count=$failed_count"$'\n'

    echo "$result"
    return 0
}

# Collect test results and write to /tmp with unique filenames
# Usage: collect_test_results <suite_id> <test_result_data>
# Returns: Result file paths in flat data format
# Behavior: Writes structured results to /tmp following Test Guidelines for Parallel Execution
#           Uses unique filenames ($$ and $RANDOM) and atomic writes (temp file, then mv)
collect_test_results() {
    local suite_id="$1"
    local test_result_data="$2"

    # Validate required parameters
    if [[ -z "$suite_id" ]]; then
        echo "Error: suite_id is required" >&2
        echo "result_file="
        echo "output_file="
        echo "error_message=suite_id is required"
        return 1
    fi

    # Generate unique filenames following Test Guidelines for Parallel Execution
    # Pattern: /tmp/suitey_test_result_<suite_id>_$$_$RANDOM
    local result_filename="/tmp/suitey_test_result_${suite_id}_$$_$RANDOM"
    local output_filename="/tmp/suitey_test_output_${suite_id}_$$_$RANDOM"

    # Extract stdout and stderr from test result data
    local stdout_content
    stdout_content=$(echo "$test_result_data" | grep "^stdout=" | cut -d'=' -f2- || echo "")
    local stderr_content
    stderr_content=$(echo "$test_result_data" | grep "^stderr=" | cut -d'=' -f2- || echo "")

    # Create temporary files for atomic writes
    local result_temp_file
    result_temp_file=$(mktemp -t suitey-result-temp-XXXXXX 2>/dev/null || echo "/tmp/suitey-result-temp-$$-$RANDOM")
    local output_temp_file
    output_temp_file=$(mktemp -t suitey-output-temp-XXXXXX 2>/dev/null || echo "/tmp/suitey-output-temp-$$-$RANDOM")

    # Write structured result data to temp file
    echo "$test_result_data" > "$result_temp_file"

    # Write output data (stdout and stderr) to temp file
    {
        if [[ -n "$stdout_content" ]]; then
            echo "=== STDOUT ==="
            echo "$stdout_content"
        fi
        if [[ -n "$stderr_content" ]]; then
            echo "=== STDERR ==="
            echo "$stderr_content"
        fi
    } > "$output_temp_file"

    # Atomic write: move temp files to final location
    if ! mv "$result_temp_file" "$result_filename" 2>/dev/null; then
        rm -f "$result_temp_file" "$output_temp_file" 2>/dev/null || true
        echo "Error: Failed to write result file" >&2
        echo "result_file="
        echo "output_file="
        echo "error_message=Failed to write result file"
        return 1
    fi

    if ! mv "$output_temp_file" "$output_filename" 2>/dev/null; then
        rm -f "$result_filename" "$output_temp_file" 2>/dev/null || true
        echo "Error: Failed to write output file" >&2
        echo "result_file=$result_filename"
        echo "output_file="
        echo "error_message=Failed to write output file"
        return 1
    fi

    # Return file paths in flat data format
    echo "result_file=$result_filename"
    echo "output_file=$output_filename"
    echo "suite_id=$suite_id"
    echo "status=success"

    return 0
}

# Parse test results using module's parse_test_results() method
# Usage: parse_test_results_with_module <module_identifier> <test_output> <exit_code>
# Returns: Parsed test results in flat data format
# Behavior: Sources the module, calls its parse_test_results() method, and returns parsed results
parse_test_results_with_module() {
    local module_identifier="$1"
    local test_output="$2"
    local exit_code="$3"

    # Validate required parameters
    if [[ -z "$module_identifier" ]]; then
        echo "Error: module_identifier is required" >&2
        echo "status=error"
        echo "error_message=module_identifier is required"
        return 1
    fi

    if [[ -z "$test_output" ]] && [[ -z "$exit_code" ]]; then
        echo "Error: test_output or exit_code is required" >&2
        echo "status=error"
        echo "error_message=test_output or exit_code is required"
        return 1
    fi

    # Get module file path from identifier
    # Module identifiers follow pattern: {name}-module
    # Module files are at: mod/languages/{name}/mod.sh, mod/frameworks/{name}/mod.sh, or mod/tools/{name}/mod.sh
    local module_file=""
    local name="${module_identifier%-module}"

    # Try framework modules first (most common for test parsing)
    if [[ -f "mod/frameworks/${name}/mod.sh" ]]; then
        module_file="mod/frameworks/${name}/mod.sh"
    # Try language modules
    elif [[ -f "mod/languages/${name}/mod.sh" ]]; then
        module_file="mod/languages/${name}/mod.sh"
    # Try tool modules
    elif [[ -f "mod/tools/${name}/mod.sh" ]]; then
        module_file="mod/tools/${name}/mod.sh"
    fi

    # If module file not found, try to get from registry
    if [[ -z "$module_file" ]] || [[ ! -f "$module_file" ]]; then
        # Check if module is registered
        if declare -f get_module >/dev/null 2>&1; then
            local module_name
            module_name=$(get_module "$module_identifier" 2>/dev/null || echo "")
            if [[ -n "$module_name" ]]; then
                # Extract path from module name (format: mod/.../mod.sh)
                module_file="$module_name"
            fi
        fi
    fi

    # If still not found, return error
    if [[ -z "$module_file" ]] || [[ ! -f "$module_file" ]]; then
        echo "Error: Module not found: $module_identifier" >&2
        echo "status=error"
        echo "error_message=Module not found: $module_identifier"
        return 1
    fi

    # Clean up any existing module functions to avoid conflicts
    for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
        unset -f "$method" 2>/dev/null || true
    done

    # Source the module
    if ! source "$module_file" 2>/dev/null; then
        echo "Error: Failed to load module: $module_file" >&2
        echo "status=error"
        echo "error_message=Failed to load module: $module_file"
        return 1
    fi

    # Verify parse_test_results function exists
    if ! declare -f parse_test_results >/dev/null 2>&1; then
        echo "Error: Module does not implement parse_test_results() method" >&2
        echo "status=error"
        echo "error_message=Module does not implement parse_test_results() method"
        return 1
    fi

    # Call module's parse_test_results() method
    # Use subshell to avoid polluting current environment
    local parsed_result
    parsed_result=$(parse_test_results "$test_output" "$exit_code" 2>&1)
    local parse_exit_code=$?

    # Clean up module functions after use
    for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
        unset -f "$method" 2>/dev/null || true
    done

    # Return parsed results
    if [[ $parse_exit_code -eq 0 ]]; then
        echo "$parsed_result"
        return 0
    else
        # Even if parsing failed, return what we got (module may have returned partial results)
        echo "$parsed_result"
        echo "parse_status=error" >> /dev/stderr || true
        return 1
    fi
}

# Execute test suite using module's execute_test_suite() method
# Usage: execute_test_suite_with_module <module_identifier> <test_suite> <test_image> <execution_config>
# Returns: Execution configuration in flat data format
# Behavior: Sources the module, calls its execute_test_suite() method, and returns execution configuration
execute_test_suite_with_module() {
    local module_identifier="$1"
    local test_suite="$2"
    local test_image="$3"
    local execution_config="$4"

    # Validate required parameters
    if [[ -z "$module_identifier" ]]; then
        echo "Error: module_identifier is required" >&2
        echo "status=error"
        echo "error_message=module_identifier is required"
        return 1
    fi

    if [[ -z "$test_suite" ]]; then
        echo "Error: test_suite is required" >&2
        echo "status=error"
        echo "error_message=test_suite is required"
        return 1
    fi

    # Get module file path from identifier
    # Module identifiers follow pattern: {name}-module
    # Module files are at: mod/languages/{name}/mod.sh, mod/frameworks/{name}/mod.sh, or mod/tools/{name}/mod.sh
    local module_file=""
    local name="${module_identifier%-module}"

    # Try framework modules first (most common for test execution)
    if [[ -f "mod/frameworks/${name}/mod.sh" ]]; then
        module_file="mod/frameworks/${name}/mod.sh"
    # Try language modules
    elif [[ -f "mod/languages/${name}/mod.sh" ]]; then
        module_file="mod/languages/${name}/mod.sh"
    # Try tool modules
    elif [[ -f "mod/tools/${name}/mod.sh" ]]; then
        module_file="mod/tools/${name}/mod.sh"
    fi

    # If module file not found, try to get from registry
    if [[ -z "$module_file" ]] || [[ ! -f "$module_file" ]]; then
        # Check if module is registered
        if declare -f get_module >/dev/null 2>&1; then
            local module_name
            module_name=$(get_module "$module_identifier" 2>/dev/null || echo "")
            if [[ -n "$module_name" ]]; then
                # Extract path from module name (format: mod/.../mod.sh)
                module_file="$module_name"
            fi
        fi
    fi

    # If still not found, return error
    if [[ -z "$module_file" ]] || [[ ! -f "$module_file" ]]; then
        echo "Error: Module not found: $module_identifier" >&2
        echo "status=error"
        echo "error_message=Module not found: $module_identifier"
        return 1
    fi

    # Clean up any existing module functions to avoid conflicts
    for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
        unset -f "$method" 2>/dev/null || true
    done

    # Source the module
    if ! source "$module_file" 2>/dev/null; then
        echo "Error: Failed to load module: $module_file" >&2
        echo "status=error"
        echo "error_message=Failed to load module: $module_file"
        return 1
    fi

    # Verify execute_test_suite function exists
    if ! declare -f execute_test_suite >/dev/null 2>&1; then
        echo "Error: Module does not implement execute_test_suite() method" >&2
        echo "status=error"
        echo "error_message=Module does not implement execute_test_suite() method"
        return 1
    fi

    # Call module's execute_test_suite() method
    # Use subshell to avoid polluting current environment
    local execution_result
    execution_result=$(execute_test_suite "$test_suite" "$test_image" "$execution_config" 2>&1)
    local execution_exit_code=$?

    # Clean up module functions after use
    for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
        unset -f "$method" 2>/dev/null || true
    done

    # Return execution results
    if [[ $execution_exit_code -eq 0 ]]; then
        echo "$execution_result"
        return 0
    else
        # Even if execution failed, return what we got (module may have returned partial results)
        echo "$execution_result"
        echo "execution_status=error" >> /dev/stderr || true
        return 1
    fi
}


# End of: src/execution_system.sh

# Included from: src/mod_registry.sh
#!/usr/bin/env bash

# Suitey Modules Registry
# Centralized registry for Suitey Modules with registration, lookup, and lifecycle management
# No external dependencies

# Registry storage (using associative arrays)
declare -A MODULE_REGISTRY
declare -A MODULE_METADATA

# Required interface methods that all modules must implement
readonly REQUIRED_METHODS=(
    "detect"
    "check_binaries"
    "discover_test_suites"
    "detect_build_requirements"
    "get_build_steps"
    "execute_test_suite"
    "parse_test_results"
    "get_metadata"
)

# Reset the registry (for testing)
reset_registry() {
    # Clear arrays (use -g to ensure we're modifying global arrays)
    unset MODULE_REGISTRY MODULE_METADATA
    declare -gA MODULE_REGISTRY
    declare -gA MODULE_METADATA
}

# Validate that a module implements all required interface methods
# Usage: validate_module_interface
# Exit code: 0 if valid, 1 if invalid
# Note: This should be called after sourcing a module script
validate_module_interface() {
    # Check if all required methods exist as functions
    for method in "${REQUIRED_METHODS[@]}"; do
        # Check if method exists as a function
        if ! declare -f "$method" >/dev/null 2>&1; then
            return 1
        fi
    done

    return 0
}

# Validate module metadata structure
# Usage: validate_module_metadata <metadata_data>
# Exit code: 0 if valid, 1 if invalid
validate_module_metadata() {
    local metadata="$1"

    # Basic validation: check that metadata is not empty
    if [[ -z "$metadata" ]]; then
        return 1
    fi

    # Check for required fields (can be lenient for now)
    # At minimum, should have language field (for backward compatibility)
    # module_type is optional but recommended
    if ! echo "$metadata" | grep -q "^language="; then
        return 1
    fi

    return 0
}

# Validate module method signature (parameter count)
# Usage: validate_module_method_signature <method_name> <expected_param_count>
# Exit code: 0 if valid, 1 if invalid
# Note: In Bash, we can't easily check parameter count at runtime without calling the function
# This is a placeholder for signature validation - actual validation would require static analysis
validate_module_method_signature() {
    local method_name="$1"
    local expected_param_count="$2"

    # Check if method exists
    if ! declare -f "$method_name" >/dev/null 2>&1; then
        return 1
    fi

    # In Bash, we can't easily check parameter count without parsing the function definition
    # For now, we just verify the method exists
    # Full signature validation would require parsing the function definition
    return 0
}

# Validate module return format (flat data format)
# Usage: validate_module_return_format <return_value>
# Exit code: 0 if valid flat data format, 1 if invalid
validate_module_return_format() {
    local return_value="$1"

    # Empty return is valid (methods may return empty when not applicable)
    if [[ -z "$return_value" ]]; then
        return 0
    fi

    # Check if return value contains key=value pairs (flat data format)
    # Should not contain JSON-like structures
    if echo "$return_value" | grep -q '[{}]'; then
        # Contains braces, likely JSON - invalid
        return 1
    fi

    # Check if it contains at least one key=value pair
    if ! echo "$return_value" | grep -q "^[^=]*="; then
        # No key=value pairs found - may be invalid
        # But allow empty or single-line values
        if [[ -n "${return_value// }" ]]; then
            # Non-empty but no = sign - might be invalid
            # For now, be lenient and allow it
            return 0
        fi
    fi

    return 0
}

# Perform complete interface validation for a module
# Usage: validate_module_interface_complete <identifier>
# Exit code: 0 if valid, 1 if invalid
validate_module_interface_complete() {
    local identifier="$1"

    if [[ -z "$identifier" ]]; then
        echo "Error: Module identifier cannot be empty" >&2
        return 1
    fi

    # Check if module is registered
    if [[ -z "${MODULE_REGISTRY[$identifier]}" ]]; then
        echo "Error: Module '$identifier' is not registered" >&2
        return 1
    fi

    # Validate all required methods exist
    if ! validate_module_interface; then
        echo "Error: Module '$identifier' does not implement all required methods" >&2
        return 1
    fi

    # Validate each method's return format (sample validation)
    # For detect() method
    if declare -f "detect" >/dev/null 2>&1; then
        local sample_result
        sample_result=$(detect "/tmp" 2>/dev/null || echo "")
        if ! validate_module_return_format "$sample_result"; then
            echo "Error: Module '$identifier' method 'detect()' returns invalid format" >&2
            return 1
        fi
    fi

    # Validate get_metadata() return format
    if declare -f "get_metadata" >/dev/null 2>&1; then
        local metadata
        metadata=$(get_metadata 2>/dev/null || echo "")
        if ! validate_module_return_format "$metadata"; then
            echo "Error: Module '$identifier' method 'get_metadata()' returns invalid format" >&2
            return 1
        fi
    fi

    return 0
}

# Register a Suitey module
# Usage: register_module <identifier> <module_name>
# Exit code: 0 on success, 1 on error
# Note: Module should be sourced before calling this function
register_module() {
    local identifier="$1"
    local module_name="$2"

    # Validate identifier is not empty
    if [[ -z "$identifier" ]]; then
        echo "Error: Module identifier cannot be empty" >&2
        return 1
    fi

    # Check if module is already registered
    if [[ -n "${MODULE_REGISTRY[$identifier]}" ]]; then
        echo "Error: Module with identifier '$identifier' is already registered" >&2
        return 1
    fi

    # Validate module interface (check if required methods exist as functions)
    # Modules are sourced before registration, so functions should be available
    local interface_valid=true
    for method in "${REQUIRED_METHODS[@]}"; do
        # Check if method exists as a function
        if ! declare -f "$method" >/dev/null 2>&1; then
            echo "Error: Module '$identifier' is missing required method '$method'" >&2
            interface_valid=false
        fi
    done

    if [[ "$interface_valid" == false ]]; then
        return 1
    fi

    # Get module metadata
    local metadata=""
    if declare -f "get_metadata" >/dev/null 2>&1; then
        metadata=$(get_metadata)
    else
        echo "Error: Module '$identifier' does not provide get_metadata() method" >&2
        return 1
    fi

    # Validate metadata
    if ! validate_module_metadata "$metadata"; then
        echo "Error: Module '$identifier' has invalid metadata" >&2
        return 1
    fi

    # Register the module
    MODULE_REGISTRY[$identifier]="$module_name"
    MODULE_METADATA[$identifier]="$metadata"

    return 0
}

# Get a module by identifier
# Usage: get_module <identifier>
# Exit code: 0 on success, 1 if not found
get_module() {
    local identifier="$1"

    if [[ -z "$identifier" ]]; then
        echo "Error: Module identifier cannot be empty" >&2
        return 1
    fi

    if [[ -z "${MODULE_REGISTRY[$identifier]}" ]]; then
        echo "Error: Module '$identifier' not found" >&2
        return 1
    fi

    echo "${MODULE_REGISTRY[$identifier]}"
    return 0
}

# Get module metadata by identifier
# Usage: get_module_metadata <identifier>
# Exit code: 0 on success, 1 if not found
get_module_metadata() {
    local identifier="$1"

    if [[ -z "$identifier" ]]; then
        echo "Error: Module identifier cannot be empty" >&2
        return 1
    fi

    if [[ -z "${MODULE_METADATA[$identifier]}" ]]; then
        echo "Error: Module '$identifier' not found" >&2
        return 1
    fi

    echo "${MODULE_METADATA[$identifier]}"
    return 0
}

# Get all registered module identifiers
# Usage: get_all_modules
# Returns: List of module identifiers, one per line
get_all_modules() {
    # Ensure arrays are initialized
    if [[ -z "${MODULE_REGISTRY[*]}" ]]; then
        return 0
    fi

    local identifier
    for identifier in "${!MODULE_REGISTRY[@]}"; do
        echo "$identifier"
    done
}

# Get modules by capability
# Usage: get_modules_by_capability <capability>
# Returns: List of module identifiers that have the specified capability, one per line
get_modules_by_capability() {
    local capability="$1"

    if [[ -z "$capability" ]]; then
        return 0
    fi

    local identifier
    local modules=""
    for identifier in "${!MODULE_REGISTRY[@]}"; do
        # Get module metadata
        local metadata="${MODULE_METADATA[$identifier]}"
        
        if [[ -z "$metadata" ]]; then
            continue
        fi

        # Check if metadata contains the capability
        # Capabilities are stored as capabilities_0=..., capabilities_1=..., etc.
        # Use grep to check for capability in metadata
        if echo "$metadata" | grep -q "^capabilities_[0-9]*=${capability}$"; then
            if [[ -z "$modules" ]]; then
                modules="$identifier"
            else
                modules="${modules}"$'\n'"${identifier}"
            fi
        fi
    done

    if [[ -n "$modules" ]]; then
        echo "$modules"
    fi

    return 0
}

# Get all registered capabilities
# Usage: get_capabilities
# Returns: List of all capabilities from all modules, one per line (deduplicated)
get_capabilities() {
    local identifier
    local all_capabilities=""
    
    for identifier in "${!MODULE_REGISTRY[@]}"; do
        # Get module metadata
        local metadata="${MODULE_METADATA[$identifier]}"
        
        if [[ -z "$metadata" ]]; then
            continue
        fi

        # Extract capabilities from metadata using grep
        # Capabilities are stored as capabilities_0=..., capabilities_1=..., etc.
        local capabilities
        capabilities=$(echo "$metadata" | grep --color=never "^capabilities_[0-9]*=" | sed 's/^capabilities_[0-9]*=//')
        
        # Add capabilities to the list
        if [[ -n "$capabilities" ]]; then
            if [[ -z "$all_capabilities" ]]; then
                all_capabilities="$capabilities"
            else
                all_capabilities="${all_capabilities}"$'\n'"${capabilities}"
            fi
        fi
    done

    # Deduplicate and sort capabilities
    if [[ -n "$all_capabilities" ]]; then
        echo "$all_capabilities" | sort -u
    fi

    return 0
}

# Get modules by type
# Usage: get_modules_by_type <module_type>
# Returns: List of module identifiers of the specified type, one per line
# Module types: language, framework, project
get_modules_by_type() {
    local module_type="$1"
    local identifier
    local modules=""
    
    if [[ -z "$module_type" ]]; then
        return 0
    fi
    
    for identifier in "${!MODULE_REGISTRY[@]}"; do
        # Get module metadata
        local metadata="${MODULE_METADATA[$identifier]}"
        
        if [[ -z "$metadata" ]]; then
            continue
        fi

        # Extract module_type from metadata
        local metadata_type
        metadata_type=$(echo "$metadata" | grep --color=never "^module_type=" | cut -d'=' -f2 || echo "")
        
        # Match module type (case-sensitive)
        if [[ "$metadata_type" == "$module_type" ]]; then
            if [[ -z "$modules" ]]; then
                modules="$identifier"
            else
                modules="${modules}"$'\n'"${identifier}"
            fi
        fi
    done

    if [[ -n "$modules" ]]; then
        echo "$modules"
    fi

    return 0
}

# Get language modules (convenience method)
# Usage: get_language_modules
# Returns: List of language module identifiers, one per line
get_language_modules() {
    get_modules_by_type "language"
    return 0
}

# Get framework modules (convenience method)
# Usage: get_framework_modules
# Returns: List of framework module identifiers, one per line
get_framework_modules() {
    get_modules_by_type "framework"
    return 0
}

# Get project modules (convenience method)
# Usage: get_project_modules
# Returns: List of project module identifiers, one per line
get_project_modules() {
    get_modules_by_type "project"
    return 0
}

# Get tool modules (convenience method)
# Usage: get_tool_modules
# Returns: List of tool module identifiers, one per line
get_tool_modules() {
    get_modules_by_type "tool"
    return 0
}


# End of: src/mod_registry.sh

# Included from: src/parallel_execution.sh
#!/usr/bin/env bash

# Parallel Execution Manager
# Coordinates parallel execution of multiple test suites
# Handles CPU core limiting, container tracking, and result aggregation

# Source required dependencies
source "src/execution_system.sh" 2>/dev/null || true
source "src/build_manager.sh" 2>/dev/null || true

# Launch multiple test suites in parallel Docker containers
# Usage: launch_test_suites_parallel <suite_configs>
# Arguments: suite_configs - Newline-separated list of suite configurations
# Each suite config is a flat data format string with:
#   suite_id=<id>
#   test_command=<command>
#   test_image=<image>
#   working_directory=<path>
#   cpu_cores=<count>
# Returns: Execution results in flat data format
launch_test_suites_parallel() {
    local suite_configs="$1"

    # If no argument provided, read from stdin
    if [[ -z "$suite_configs" ]]; then
        suite_configs=$(cat)
    fi

    # Initialize counters and tracking
    local total_suites=0
    local launched_suites=0
    local container_ids=""
    local execution_status="success"

    # Parse suite configurations
    local suite_array=()
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Skip empty lines
        [[ -z "$line" ]] && continue

        # If line starts with "suite_id=", it's a new suite configuration
        if [[ "$line" =~ ^suite_id= ]]; then
            # If we have a previous suite, add it to the array
            if [[ -n "${current_suite:-}" ]]; then
                suite_array+=("$current_suite")
            fi
            # Start new suite
            current_suite="$line"
            ((total_suites++))
        else
            # Continue building current suite
            current_suite="$current_suite"$'\n'"$line"
        fi
    done <<< "$suite_configs"

    # Add the last suite if it exists
    if [[ -n "${current_suite:-}" ]]; then
        suite_array+=("$current_suite")
    fi

    # Handle empty input
    if [[ ${#suite_array[@]} -eq 0 ]]; then
        echo "total_suites=0"
        echo "launched_suites=0"
        echo "container_ids="
        echo "execution_status=success"
        return 0
    fi

    # Get available CPU cores for parallelism limiting
    local max_parallel
    max_parallel=$(get_available_cpu_cores 2>/dev/null || echo "4")

    # Launch suites in batches based on available cores
    local current_batch=()
    local batch_size=0

    for suite_config in "${suite_array[@]}"; do
        # Parse suite configuration
        local suite_id
        local test_command
        local test_image
        local working_directory
        local cpu_cores

        suite_id=$(echo "$suite_config" | grep "^suite_id=" | cut -d'=' -f2 || echo "")
        test_command=$(echo "$suite_config" | grep "^test_command=" | cut -d'=' -f2- || echo "")
        test_image=$(echo "$suite_config" | grep "^test_image=" | cut -d'=' -f2 || echo "")
        working_directory=$(echo "$suite_config" | grep "^working_directory=" | cut -d'=' -f2 || echo "/app")
        cpu_cores=$(echo "$suite_config" | grep "^cpu_cores=" | cut -d'=' -f2 || echo "1")

        # Validate required fields
        if [[ -z "$suite_id" ]] || [[ -z "$test_command" ]] || [[ -z "$test_image" ]]; then
            echo "Error: Invalid suite configuration - missing required fields" >&2
            execution_status="error"
            continue
        fi

        # Check if we need to launch current batch
        if [[ $batch_size -ge $max_parallel ]]; then
            # Launch current batch and wait for completion
            local batch_containers
            batch_containers=$(_launch_batch "${current_batch[@]}")
            local batch_result=$?
            if [[ $batch_result -ne 0 ]]; then
                execution_status="error"
            fi

            # Add batch containers to tracking
            if [[ -n "$batch_containers" ]]; then
                if [[ -z "$container_ids" ]]; then
                    container_ids="$batch_containers"
                else
                    container_ids="$container_ids,$batch_containers"
                fi
            fi

            # Reset batch
            current_batch=()
            batch_size=0
        fi

        # Add suite to current batch (single line format)
        local container_config="test_image=$test_image|working_directory=$working_directory|cpu_cores=$cpu_cores|container_name=suitey-test-$suite_id-$$"

        current_batch+=("$suite_id|$test_command|$container_config")
        ((batch_size++))
    done

    # Launch final batch
    if [[ ${#current_batch[@]} -gt 0 ]]; then
        local batch_containers
        batch_containers=$(_launch_batch "${current_batch[@]}")
        local batch_result=$?
        if [[ $batch_result -ne 0 ]]; then
            execution_status="error"
        fi

        # Add final batch containers to tracking
        if [[ -n "$batch_containers" ]]; then
            if [[ -z "$container_ids" ]]; then
                container_ids="$batch_containers"
            else
                container_ids="$container_ids,$batch_containers"
            fi
        fi
    fi

    # Count total launched suites (this would be tracked in the batch launch function)
    launched_suites=$total_suites

    # Return results
    echo "total_suites=$total_suites"
    echo "launched_suites=$launched_suites"
    echo "container_ids=$container_ids"
    echo "execution_status=$execution_status"

    return 0
}

# Internal function to launch a batch of test suites
# Usage: _launch_batch suite_specs...
# Arguments: suite_specs - Array of "suite_id|test_command|container_config" strings
# Returns: Comma-separated list of container IDs
_launch_batch() {
    local suite_specs=("$@")
    local pids=()
    local temp_containers=()

    # Launch all suites in this batch
    for spec in "${suite_specs[@]}"; do
        # Parse spec
        IFS='|' read -r suite_id test_command container_config_str <<< "$spec"

        # Parse container config from single line format
        local container_config=""
        IFS='|' read -r test_image_part working_dir_part cpu_cores_part container_name_part <<< "$container_config_str"
        local test_image=$(echo "$test_image_part" | cut -d'=' -f2)
        local working_directory=$(echo "$working_dir_part" | cut -d'=' -f2)
        local cpu_cores=$(echo "$cpu_cores_part" | cut -d'=' -f2)
        local container_name=$(echo "$container_name_part" | cut -d'=' -f2)

        container_config="test_image=$test_image
working_directory=$working_directory
cpu_cores=$cpu_cores
container_name=$container_name"

        # Launch container
        local launch_result
        launch_result=$(launch_test_container "$container_config" 2>&1)
        local launch_status=$?

        if [[ $launch_status -eq 0 ]]; then
            # Extract container ID
            local container_id
            container_id=$(echo "$launch_result" | grep "^container_id=" | cut -d'=' -f2 || echo "")

            if [[ -n "$container_id" ]]; then
                temp_containers+=("$container_id")

                # Launch test execution in background
                (
                    # Execute test command
                    local test_result
                    test_result=$(execute_test_command "$container_id" "$test_command" 2>&1)

                    # Collect test results
                    collect_test_results "$suite_id" "$test_result" >/dev/null 2>&1

                    # Clean up container
                    cleanup_test_container "$container_id" >/dev/null 2>&1 || true
                ) &
                pids+=($!)
            fi
        fi
    done

    # Wait for all suites in this batch to complete
    for pid in "${pids[@]}"; do
        wait "$pid" 2>/dev/null || true
    done

    # Return comma-separated container IDs
    local result=""
    for container_id in "${temp_containers[@]}"; do
        if [[ -z "$result" ]]; then
            result="$container_id"
        else
            result="$result,$container_id"
        fi
    done

    echo "$result"
    return 0
}

# Global variables for signal handling
PROCESSED_RESULT_FILES=""
SIGNAL_RECEIVED=""
ACTIVE_CONTAINERS=""
FORCE_KILL_TRIGGERED=""

# Poll result files in /tmp as tests complete
# Usage: poll_test_results
# Returns: Flat data format with completed test results
# Looks for files matching pattern: suitey_test_result_*
# Tracks processed files to avoid re-reading
poll_test_results() {
    local results_found=0
    local all_results=""

    # Find all result files matching the pattern
    local result_files
    result_files=$(find /tmp -name "suitey_test_result_*" -type f 2>/dev/null || true)

    # Process each result file
    while IFS= read -r result_file; do
        [[ -z "$result_file" ]] && continue

        # Extract suite_id and unique identifiers from filename
        local filename
        filename=$(basename "$result_file")
        local suite_id=""
        local pid_part=""
        local random_part=""

        # Parse filename: suitey_test_result_<suite_id>_<pid>_<random>
        if [[ "$filename" =~ suitey_test_result_(.+)_(.+)_(.+)$ ]]; then
            suite_id="${BASH_REMATCH[1]}"
            pid_part="${BASH_REMATCH[2]}"
            random_part="${BASH_REMATCH[3]}"
        else
            # Skip files that don't match expected pattern
            continue
        fi

        # Check if we've already processed this file
        local file_key="$suite_id:$pid_part:$random_part"
        if [[ "$PROCESSED_RESULT_FILES" == *"$file_key"* ]]; then
            continue
        fi

        # Check if file is fully written (atomic write check)
        # For now, assume file is complete if it exists and has content
        if [[ ! -s "$result_file" ]]; then
            # File is empty, skip for now
            continue
        fi

        # Read result file content
        local result_content=""
        if result_content=$(cat "$result_file" 2>/dev/null); then
            # Mark file as processed
            PROCESSED_RESULT_FILES="$PROCESSED_RESULT_FILES|$file_key"

            # Find corresponding output file
            local output_file="/tmp/suitey_test_output_${suite_id}_${pid_part}_${random_part}"

            # Build result output
            local result_output="suite_id=$suite_id
result_file=$result_file
output_file=$output_file
$result_content"

            # Append to all results
            if [[ -z "$all_results" ]]; then
                all_results="$result_output"
            else
                all_results="$all_results
---
$result_output"
            fi

            ((results_found++))
        fi
    done <<< "$result_files"

    # Return results
    if [[ $results_found -eq 0 ]]; then
        echo "results_found=0"
        echo "status=no_results"
    else
        echo "results_found=$results_found"
        echo "$all_results"
        echo "status=results_found"
    fi

    return 0
}

# Set up signal handlers for graceful shutdown
# Usage: setup_signal_handlers
# Sets up SIGINT handler and cleanup traps
setup_signal_handlers() {
    # Set up SIGINT handler
    trap 'handle_sigint' INT

    # Set up cleanup on exit
    trap 'cleanup_on_exit' EXIT

    echo "signal_handlers_setup=success"
    echo "trap_int=installed"
    echo "trap_exit=installed"

    return 0
}

# Handle SIGINT (Ctrl+C) signal
# Usage: handle_sigint
# First SIGINT: graceful termination
# Second SIGINT: force kill
handle_sigint() {
    if [[ -z "$SIGNAL_RECEIVED" ]]; then
        # First SIGINT - graceful termination
        SIGNAL_RECEIVED="first"

        echo "signal_received=first" >&2
        echo "graceful_termination=initiated" >&2

        # Count active containers
        local container_count=0
        if [[ -n "$ACTIVE_CONTAINERS" ]]; then
            container_count=$(echo "$ACTIVE_CONTAINERS" | wc -w)
        fi

        echo "active_containers=$container_count" >&2

        # Send graceful termination to containers
        graceful_terminate_containers

        echo "waiting_for_graceful_shutdown=10_seconds" >&2
        # In a real implementation, we'd sleep here, but for testing we'll skip

    else
        # Second SIGINT - force kill
        SIGNAL_RECEIVED="second"
        FORCE_KILL_TRIGGERED="true"

        echo "signal_received=second" >&2
        echo "force_kill=triggered" >&2
        echo "immediate_termination=initiated" >&2

        # Force kill all containers
        force_kill_containers
    fi

    return 0
}

# Gracefully terminate all active containers
# Usage: graceful_terminate_containers
# Sends SIGTERM to containers for clean shutdown
graceful_terminate_containers() {
    local terminated_count=0

    for container_id in $ACTIVE_CONTAINERS; do
        if [[ -n "$container_id" ]]; then
            echo "gracefully_terminating_container=$container_id" >&2
            docker stop "$container_id" >/dev/null 2>&1 || true
            ((terminated_count++))
        fi
    done

    echo "containers_gracefully_terminated=$terminated_count" >&2
}

# Force kill all active containers
# Usage: force_kill_containers
# Sends SIGKILL to containers for immediate termination
force_kill_containers() {
    local killed_count=0

    for container_id in $ACTIVE_CONTAINERS; do
        if [[ -n "$container_id" ]]; then
            echo "force_killing_container=$container_id" >&2
            docker kill "$container_id" >/dev/null 2>&1 || true
            ((killed_count++))
        fi
    done

    echo "containers_force_killed=$killed_count" >&2
}

# Clean up all active containers
# Usage: cleanup_containers
# Removes stopped containers and cleans up resources
cleanup_containers() {
    local cleaned_count=0

    for container_id in $ACTIVE_CONTAINERS; do
        if [[ -n "$container_id" ]]; then
            echo "cleaning_up_container=$container_id" >&2
            # First stop if running
            docker stop "$container_id" >/dev/null 2>&1 || true
            # Then remove
            docker rm "$container_id" >/dev/null 2>&1 || true
            ((cleaned_count++))
        fi
    done

    echo "containers_cleaned=$cleaned_count"
    echo "cleanup_completed=true"

    # Clear active containers list
    ACTIVE_CONTAINERS=""

    return 0
}

# Clean up temporary files from /tmp
# Usage: cleanup_temp_files
# Removes suitey temporary files
cleanup_temp_files() {
    local files_removed=0

    # Remove all suitey temporary files (comprehensive cleanup)
    local suitey_files
    suitey_files=$(find /tmp -name "suitey_*" -type f 2>/dev/null || true)
    for file in $suitey_files; do
        rm -f "$file" 2>/dev/null || true
        ((files_removed++))
    done

    echo "temp_files_removed=$files_removed"
    echo "temp_cleanup_completed=true"

    return 0
}

# Clean up on exit (called by EXIT trap)
# Usage: cleanup_on_exit
# Performs final cleanup when suitey exits
cleanup_on_exit() {
    echo "performing_exit_cleanup" >&2

    # Clean up containers if any are still active
    if [[ -n "$ACTIVE_CONTAINERS" ]]; then
        echo "cleaning_up_containers_on_exit" >&2
        cleanup_containers >/dev/null 2>&1 || true
    fi

    # Clean up temporary files
    echo "cleaning_up_temp_files_on_exit" >&2
    cleanup_temp_files >/dev/null 2>&1 || true

    echo "exit_cleanup_completed" >&2
}

# =============================================================================
# Resource Management Functions (3.3.4)
# =============================================================================

# Global variables for resource pool management
RESOURCE_POOL_CAPACITY=0
RESOURCE_POOL_AVAILABLE=0
RESOURCE_POOL_IN_USE=0
RESOURCE_POOL_INITIALIZED=""

# Get maximum number of concurrent containers
# Usage: get_max_concurrent_containers [explicit_limit]
# Returns: Maximum container count in flat data format
# Behavior: Returns CPU core count or explicit limit (whichever is smaller)
get_max_concurrent_containers() {
    local explicit_limit="${1:-}"
    local available_cores

    # Get available CPU cores
    available_cores=$(get_available_cpu_cores 2>/dev/null || echo "4")

    # Ensure we have at least 1 core
    if [[ -z "$available_cores" ]] || [[ "$available_cores" -lt 1 ]]; then
        available_cores=1
    fi

    local max_containers="$available_cores"
    local limited_by_cpu="false"

    # If explicit limit is provided
    if [[ -n "$explicit_limit" ]]; then
        # Validate it's a positive integer
        if [[ "$explicit_limit" =~ ^[0-9]+$ ]] && [[ "$explicit_limit" -gt 0 ]]; then
            if [[ "$explicit_limit" -gt "$available_cores" ]]; then
                # Limit to available cores
                max_containers="$available_cores"
                limited_by_cpu="true"
            else
                max_containers="$explicit_limit"
            fi
        else
            # Invalid limit, use all available
            max_containers="$available_cores"
        fi
    fi

    # Ensure minimum of 1
    if [[ "$max_containers" -lt 1 ]]; then
        max_containers=1
    fi

    echo "max_containers=$max_containers"
    echo "available_cores=$available_cores"
    echo "limited_by_cpu=$limited_by_cpu"

    return 0
}

# Get the resource pool state file path
# Usage: _get_pool_state_file
# Returns: Path to pool state file
# Note: Can be overridden by SUITEY_POOL_STATE_FILE environment variable for testing
_get_pool_state_file() {
    echo "${SUITEY_POOL_STATE_FILE:-/tmp/suitey_resource_pool_$$}"
}

# Initialize resource pool with given capacity
# Usage: resource_pool_init [capacity]
# Returns: Pool initialization status in flat data format
# Behavior: Initializes pool with CPU cores or custom capacity
resource_pool_init() {
    local capacity="${1:-}"
    local available_cores

    # Get available CPU cores
    available_cores=$(get_available_cpu_cores 2>/dev/null || echo "4")

    # Ensure we have at least 1 core
    if [[ -z "$available_cores" ]] || [[ "$available_cores" -lt 1 ]]; then
        available_cores=1
    fi

    # Use custom capacity or default to available cores
    if [[ -n "$capacity" ]] && [[ "$capacity" =~ ^[0-9]+$ ]] && [[ "$capacity" -gt 0 ]]; then
        # Limit capacity to available cores
        if [[ "$capacity" -gt "$available_cores" ]]; then
            capacity="$available_cores"
        fi
    else
        capacity="$available_cores"
    fi

    # Ensure minimum of 1
    if [[ "$capacity" -lt 1 ]]; then
        capacity=1
    fi

    # Initialize pool state
    RESOURCE_POOL_CAPACITY="$capacity"
    RESOURCE_POOL_AVAILABLE="$capacity"
    RESOURCE_POOL_IN_USE=0
    RESOURCE_POOL_INITIALIZED="true"

    # Create pool state file for persistence across subshells
    local pool_state_file
    pool_state_file=$(_get_pool_state_file)
    echo "capacity=$capacity" > "$pool_state_file"
    echo "available=$capacity" >> "$pool_state_file"
    echo "in_use=0" >> "$pool_state_file"

    echo "pool_capacity=$capacity"
    echo "pool_available=$capacity"
    echo "pool_in_use=0"
    echo "pool_status=initialized"

    return 0
}

# Acquire resources from pool
# Usage: resource_pool_acquire <count> [no_wait]
# Returns: Acquisition status in flat data format
# Behavior: Acquires specified number of resources if available
resource_pool_acquire() {
    local count="${1:-1}"
    local no_wait="${2:-}"

    # Validate count
    if ! [[ "$count" =~ ^[0-9]+$ ]] || [[ "$count" -lt 1 ]]; then
        count=1
    fi

    # Read current pool state
    local pool_state_file
    pool_state_file=$(_get_pool_state_file)
    local available=0
    local capacity=0
    local in_use=0

    if [[ -f "$pool_state_file" ]]; then
        available=$(grep "^available=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
        capacity=$(grep "^capacity=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
        in_use=$(grep "^in_use=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
    else
        # Pool not initialized, use global variables
        available="$RESOURCE_POOL_AVAILABLE"
        capacity="$RESOURCE_POOL_CAPACITY"
        in_use="$RESOURCE_POOL_IN_USE"
    fi

    # Ensure numeric values
    available="${available:-0}"
    capacity="${capacity:-0}"
    in_use="${in_use:-0}"

    # Check if resources are available
    if [[ "$available" -lt "$count" ]]; then
        echo "acquired=0"
        echo "requested=$count"
        echo "available=$available"
        echo "acquire_status=exhausted"
        return 1
    fi

    # Acquire resources
    local new_available=$((available - count))
    local new_in_use=$((in_use + count))

    # Update pool state
    RESOURCE_POOL_AVAILABLE="$new_available"
    RESOURCE_POOL_IN_USE="$new_in_use"

    # Update state file
    if [[ -f "$pool_state_file" ]]; then
        echo "capacity=$capacity" > "$pool_state_file"
        echo "available=$new_available" >> "$pool_state_file"
        echo "in_use=$new_in_use" >> "$pool_state_file"
    fi

    echo "acquired=$count"
    echo "pool_available=$new_available"
    echo "pool_in_use=$new_in_use"
    echo "acquire_status=success"

    return 0
}

# Release resources back to pool
# Usage: resource_pool_release <count>
# Returns: Release status in flat data format
# Behavior: Releases specified number of resources back to pool
resource_pool_release() {
    local count="${1:-1}"

    # Validate count
    if ! [[ "$count" =~ ^[0-9]+$ ]] || [[ "$count" -lt 1 ]]; then
        count=1
    fi

    # Read current pool state
    local pool_state_file
    pool_state_file=$(_get_pool_state_file)
    local available=0
    local capacity=0
    local in_use=0

    if [[ -f "$pool_state_file" ]]; then
        available=$(grep "^available=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
        capacity=$(grep "^capacity=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
        in_use=$(grep "^in_use=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
    else
        # Pool not initialized, use global variables
        available="$RESOURCE_POOL_AVAILABLE"
        capacity="$RESOURCE_POOL_CAPACITY"
        in_use="$RESOURCE_POOL_IN_USE"
    fi

    # Ensure numeric values
    available="${available:-0}"
    capacity="${capacity:-0}"
    in_use="${in_use:-0}"

    # Don't release more than in use
    if [[ "$count" -gt "$in_use" ]]; then
        count="$in_use"
    fi

    # Release resources
    local new_available=$((available + count))
    local new_in_use=$((in_use - count))

    # Don't exceed capacity
    if [[ "$new_available" -gt "$capacity" ]]; then
        new_available="$capacity"
    fi

    # Ensure non-negative
    if [[ "$new_in_use" -lt 0 ]]; then
        new_in_use=0
    fi

    # Update pool state
    RESOURCE_POOL_AVAILABLE="$new_available"
    RESOURCE_POOL_IN_USE="$new_in_use"

    # Update state file
    if [[ -f "$pool_state_file" ]]; then
        echo "capacity=$capacity" > "$pool_state_file"
        echo "available=$new_available" >> "$pool_state_file"
        echo "in_use=$new_in_use" >> "$pool_state_file"
    fi

    echo "released=$count"
    echo "pool_available=$new_available"
    echo "pool_in_use=$new_in_use"
    echo "release_status=success"

    return 0
}

# Get resource pool status
# Usage: resource_pool_status
# Returns: Current pool state in flat data format
resource_pool_status() {
    # Read current pool state
    local pool_state_file
    pool_state_file=$(_get_pool_state_file)
    local available=0
    local capacity=0
    local in_use=0

    if [[ -f "$pool_state_file" ]]; then
        available=$(grep "^available=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
        capacity=$(grep "^capacity=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
        in_use=$(grep "^in_use=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
    else
        # Pool not initialized, use global variables
        available="$RESOURCE_POOL_AVAILABLE"
        capacity="$RESOURCE_POOL_CAPACITY"
        in_use="$RESOURCE_POOL_IN_USE"
    fi

    echo "pool_capacity=${capacity:-0}"
    echo "pool_available=${available:-0}"
    echo "pool_in_use=${in_use:-0}"

    if [[ -n "$RESOURCE_POOL_INITIALIZED" ]] || [[ -f "$pool_state_file" ]]; then
        echo "pool_status=active"
    else
        echo "pool_status=not_initialized"
    fi

    return 0
}

# Clean up only completed (stopped) containers
# Usage: cleanup_completed_containers
# Returns: Cleanup status in flat data format
# Behavior: Only cleans up stopped containers, leaves running ones alone
cleanup_completed_containers() {
    local cleaned_count=0
    local skipped_count=0

    for container_id in $ACTIVE_CONTAINERS; do
        if [[ -n "$container_id" ]]; then
            # Check container status
            local container_status
            container_status=$(docker inspect --format='{{.State.Status}}' "$container_id" 2>/dev/null || echo "unknown")

            if [[ "$container_status" != "running" ]] && [[ "$container_status" != "unknown" ]]; then
                # Container is stopped, clean it up
                docker rm "$container_id" >/dev/null 2>&1 || true
                ((cleaned_count++))
            else
                ((skipped_count++))
            fi
        fi
    done

    echo "containers_cleaned=$cleaned_count"
    echo "containers_skipped=$skipped_count"
    echo "cleanup_status=success"

    return 0
}

# Clean up all suitey containers (by name pattern)
# Usage: cleanup_all_suitey_containers
# Returns: Cleanup status in flat data format
# Behavior: Finds and removes all containers matching suitey naming pattern
cleanup_all_suitey_containers() {
    local cleaned_count=0

    # Find all suitey containers by name pattern
    local suitey_containers
    suitey_containers=$(docker ps -a --filter "name=suitey-" --format "{{.ID}}" 2>/dev/null || echo "")

    for container_id in $suitey_containers; do
        if [[ -n "$container_id" ]]; then
            # Stop if running
            docker stop "$container_id" >/dev/null 2>&1 || true
            # Remove container
            docker rm "$container_id" >/dev/null 2>&1 || true
            ((cleaned_count++))
        fi
    done

    echo "containers_cleaned=$cleaned_count"
    echo "cleanup_status=success"

    return 0
}

# Register container for tracking and cleanup
# Usage: register_container_for_cleanup <container_id>
# Returns: Registration status in flat data format
register_container_for_cleanup() {
    local container_id="$1"

    if [[ -z "$container_id" ]]; then
        echo "registered="
        echo "register_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    # Add to tracking list (avoid duplicates)
    if [[ "$ACTIVE_CONTAINERS" != *"$container_id"* ]]; then
        if [[ -z "$ACTIVE_CONTAINERS" ]]; then
            ACTIVE_CONTAINERS="$container_id"
        else
            ACTIVE_CONTAINERS="$ACTIVE_CONTAINERS $container_id"
        fi
    fi

    echo "registered=$container_id"
    echo "register_status=success"
    echo "total_tracked=$(echo "$ACTIVE_CONTAINERS" | wc -w | tr -d ' ')"

    return 0
}

# Unregister container from tracking
# Usage: unregister_container <container_id>
# Returns: Unregistration status in flat data format
unregister_container() {
    local container_id="$1"

    if [[ -z "$container_id" ]]; then
        echo "unregistered="
        echo "unregister_status=error"
        echo "error_message=container_id is required"
        return 1
    fi

    # Remove from tracking list
    local new_list=""
    for tracked_id in $ACTIVE_CONTAINERS; do
        if [[ "$tracked_id" != "$container_id" ]]; then
            if [[ -z "$new_list" ]]; then
                new_list="$tracked_id"
            else
                new_list="$new_list $tracked_id"
            fi
        fi
    done

    ACTIVE_CONTAINERS="$new_list"

    echo "unregistered=$container_id"
    echo "unregister_status=success"
    echo "remaining_containers=$(echo "$ACTIVE_CONTAINERS" | wc -w | tr -d ' ')"

    return 0
}

# Clean up on completion (containers and temp files)
# Usage: cleanup_on_completion
# Returns: Cleanup status in flat data format
# Behavior: Cleans up all resources at end of execution
cleanup_on_completion() {
    local containers_cleaned=0
    local temp_files_removed=0
    local pool_released="false"

    # Clean up containers first
    if [[ -n "$ACTIVE_CONTAINERS" ]]; then
        local cleanup_result
        cleanup_result=$(cleanup_containers 2>&1)
        containers_cleaned=$(echo "$cleanup_result" | grep "^containers_cleaned=" | cut -d'=' -f2 || echo "0")
    fi

    # Release resource pool BEFORE cleaning temp files
    # (cleanup_temp_files removes all suitey_* files including pool state)
    local pool_state_file
    pool_state_file=$(_get_pool_state_file)
    if [[ -f "$pool_state_file" ]]; then
        rm -f "$pool_state_file" 2>/dev/null || true
        pool_released="true"
    fi

    # Reset pool state
    RESOURCE_POOL_CAPACITY=0
    RESOURCE_POOL_AVAILABLE=0
    RESOURCE_POOL_IN_USE=0
    RESOURCE_POOL_INITIALIZED=""

    # Clean up remaining temp files
    local temp_result
    temp_result=$(cleanup_temp_files 2>&1)
    temp_files_removed=$(echo "$temp_result" | grep "^temp_files_removed=" | cut -d'=' -f2 || echo "0")

    echo "cleanup_status=complete"
    echo "containers_cleaned=${containers_cleaned:-0}"
    echo "temp_files_removed=${temp_files_removed:-0}"
    echo "pool_released=$pool_released"

    return 0
}

# Get count of active (tracked) containers
# Usage: get_active_container_count
# Returns: Count in flat data format
get_active_container_count() {
    local count=0

    if [[ -n "$ACTIVE_CONTAINERS" ]]; then
        count=$(echo "$ACTIVE_CONTAINERS" | wc -w | tr -d ' ')
    fi

    echo "active_count=$count"

    return 0
}

# Check if resources are available in pool
# Usage: is_resource_available
# Returns: Availability status in flat data format
is_resource_available() {
    # Read current pool state
    local pool_state_file
    pool_state_file=$(_get_pool_state_file)
    local available=0

    if [[ -f "$pool_state_file" ]]; then
        available=$(grep "^available=" "$pool_state_file" 2>/dev/null | cut -d'=' -f2 || echo "0")
    else
        available="$RESOURCE_POOL_AVAILABLE"
    fi

    # Ensure numeric
    available="${available:-0}"

    if [[ "$available" -gt 0 ]]; then
        echo "available=true"
        echo "pool_available=$available"
    else
        echo "available=false"
        echo "pool_available=$available"
    fi

    return 0
}

# =============================================================================
# Memory Resource Management Functions (3.3.5)
# =============================================================================

# Get available memory in GB
# Usage: get_available_memory_gb
# Returns: Available memory in GB as floating point
# Behavior: Detects available system memory using various methods
get_available_memory_gb() {
    local available_kb=0
    local detection_method="unknown"

    # Try /proc/meminfo (Linux)
    if [[ -f /proc/meminfo ]]; then
        # Get available memory (includes buffers/cache)
        available_kb=$(grep "^MemAvailable:" /proc/meminfo 2>/dev/null | awk '{print $2}' || echo "0")
        if [[ "$available_kb" -gt 0 ]]; then
            detection_method="proc_meminfo"
        else
            # Fallback to free memory
            available_kb=$(grep "^MemFree:" /proc/meminfo 2>/dev/null | awk '{print $2}' || echo "0")
            detection_method="proc_meminfo_free"
        fi
    fi

    # Try sysctl (macOS/BSD)
    if [[ "$available_kb" -eq 0 ]] && command -v sysctl >/dev/null 2>&1; then
        # Get total memory and estimate available (rough approximation)
        local total_kb
        total_kb=$(sysctl -n hw.memsize 2>/dev/null | awk '{print $1 / 1024}' || echo "0")
        if [[ "$total_kb" -gt 0 ]]; then
            # Estimate available as 75% of total (rough heuristic)
            available_kb=$(echo "$total_kb * 0.75" | bc -l 2>/dev/null | cut -d'.' -f1 || echo "$total_kb")
            detection_method="sysctl_estimate"
        fi
    fi

    # Fallback estimate
    if [[ "$available_kb" -eq 0 ]]; then
        # Assume 4GB as reasonable default
        available_kb=4194304  # 4GB in KB
        detection_method="fallback_estimate"
    fi

    # Convert KB to GB with 2 decimal places
    local available_gb
    if command -v bc >/dev/null 2>&1; then
        available_gb=$(echo "scale=2; $available_kb / 1048576" | bc -l 2>/dev/null || echo "4.00")
    else
        # Fallback calculation
        available_gb=$((available_kb / 1048576))
        available_gb="${available_gb}.00"
    fi

    echo "available_memory_gb=$available_gb"
    echo "available_memory_kb=$available_kb"
    echo "detection_method=$detection_method"

    return 0
}

# Get total memory in GB
# Usage: get_total_memory_gb
# Returns: Total system memory in GB
# Behavior: Detects total system memory
get_total_memory_gb() {
    local total_kb=0
    local detection_method="unknown"

    # Try /proc/meminfo (Linux)
    if [[ -f /proc/meminfo ]]; then
        total_kb=$(grep "^MemTotal:" /proc/meminfo 2>/dev/null | awk '{print $2}' || echo "0")
        if [[ "$total_kb" -gt 0 ]]; then
            detection_method="proc_meminfo"
        fi
    fi

    # Try sysctl (macOS/BSD)
    if [[ "$total_kb" -eq 0 ]] && command -v sysctl >/dev/null 2>&1; then
        total_kb=$(sysctl -n hw.memsize 2>/dev/null | awk '{print $1 / 1024}' || echo "0")
        if [[ "$total_kb" -gt 0 ]]; then
            detection_method="sysctl"
        fi
    fi

    # Fallback estimate
    if [[ "$total_kb" -eq 0 ]]; then
        total_kb=4194304  # 4GB in KB
        detection_method="fallback_estimate"
    fi

    # Convert KB to GB
    local total_gb
    if command -v bc >/dev/null 2>&1; then
        total_gb=$(echo "scale=2; $total_kb / 1048576" | bc -l 2>/dev/null || echo "4.00")
    else
        total_gb=$((total_kb / 1048576))
        total_gb="${total_gb}.00"
    fi

    echo "total_memory_gb=$total_gb"
    echo "total_memory_kb=$total_kb"
    echo "detection_method=$detection_method"

    return 0
}

# Calculate memory per container in GB
# Usage: calculate_memory_per_container_gb <total_memory_gb> <parallel_jobs> <memory_headroom>
# Returns: Memory allocation details
# Behavior: Uses conservative calculation: (total_memory * (1 - headroom)) / max_parallel_jobs
calculate_memory_per_container_gb() {
    local total_memory_gb="$1"
    local parallel_jobs="$2"
    local memory_headroom="$3"

    # Validate inputs
    if ! [[ "$total_memory_gb" =~ ^[0-9]*\.?[0-9]+$ ]] || [[ "$(echo "$total_memory_gb <= 0" | bc -l 2>/dev/null || echo "1")" = "1" ]]; then
        echo "error_message=Invalid total_memory_gb: $total_memory_gb"
        echo "memory_per_container_gb=0.1"
        return 1
    fi

    if ! [[ "$parallel_jobs" =~ ^[0-9]+$ ]] || [[ "$parallel_jobs" -le 0 ]]; then
        echo "error_message=Invalid parallel_jobs: $parallel_jobs"
        echo "memory_per_container_gb=0.1"
        return 1
    fi

    if ! [[ "$memory_headroom" =~ ^[0-9]*\.?[0-9]+$ ]] || [[ "$(echo "$memory_headroom < 0 || $memory_headroom >= 1" | bc -l 2>/dev/null || echo "1")" = "1" ]]; then
        echo "error_message=Invalid memory_headroom: $memory_headroom (must be 0.0-0.99)"
        echo "memory_per_container_gb=0.1"
        return 1
    fi

    # Conservative calculation: (total_memory * (1 - headroom)) / max_parallel_jobs
    local available_memory
    local memory_per_container

    if command -v bc >/dev/null 2>&1; then
        available_memory=$(echo "scale=2; $total_memory_gb * (1 - $memory_headroom)" | bc -l 2>/dev/null)
        memory_per_container=$(echo "scale=2; $available_memory / $parallel_jobs" | bc -l 2>/dev/null)
    else
        # Fallback calculation
        available_memory=$((total_memory_gb * (100 - memory_headroom * 100) / 100))
        memory_per_container=$((available_memory / parallel_jobs))
    fi

    # Ensure minimum allocation (100MB = 0.1GB)
    local min_memory="0.1"
    if [[ "$(echo "$memory_per_container < $min_memory" | bc -l 2>/dev/null || echo "0")" = "1" ]]; then
        memory_per_container="$min_memory"
        echo "warning_message=Memory per container limited to minimum: ${min_memory}GB"
    fi

    echo "memory_per_container_gb=$memory_per_container"
    echo "total_memory_gb=$total_memory_gb"
    echo "parallel_jobs=$parallel_jobs"
    echo "memory_headroom=$memory_headroom"
    echo "available_memory_after_headroom=$available_memory"

    return 0
}

# Apply memory limits to Docker container command
# Usage: apply_memory_limits_to_container <docker_cmd> <memory_limit_gb> [memory_swap_gb]
# Returns: Modified Docker command with memory limits
# Behavior: Adds --memory and --memory-swap flags to Docker command
apply_memory_limits_to_container() {
    local docker_cmd="$1"
    local memory_limit_gb="$2"
    local memory_swap_gb="$3"

    # Validate memory limit
    if [[ -z "$memory_limit_gb" ]] || [[ "$memory_limit_gb" == "0" ]]; then
        # No memory limit specified
        echo "$docker_cmd"
        return 0
    fi

    # Validate numeric
    if ! [[ "$memory_limit_gb" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        echo "Error: Invalid memory limit: $memory_limit_gb" >&2
        echo "$docker_cmd"
        return 1
    fi

    # Convert GB to bytes (1GB = 1073741824 bytes)
    local memory_limit_bytes
    if command -v bc >/dev/null 2>&1; then
        memory_limit_bytes=$(echo "$memory_limit_gb * 1073741824" | bc -l 2>/dev/null | cut -d'.' -f1)
    else
        # Rough approximation
        memory_limit_bytes=$((memory_limit_gb * 1073741824))
    fi

    # Add --memory flag
    docker_cmd="$docker_cmd --memory=${memory_limit_bytes}"

    # Add --memory-swap if specified
    if [[ -n "$memory_swap_gb" ]] && [[ "$memory_swap_gb" != "0" ]]; then
        if [[ "$memory_swap_gb" =~ ^[0-9]*\.?[0-9]+$ ]]; then
            local memory_swap_bytes
            if command -v bc >/dev/null 2>&1; then
                memory_swap_bytes=$(echo "$memory_swap_gb * 1073741824" | bc -l 2>/dev/null | cut -d'.' -f1)
            else
                memory_swap_bytes=$((memory_swap_gb * 1073741824))
            fi
            docker_cmd="$docker_cmd --memory-swap=${memory_swap_bytes}"
        fi
    fi

    echo "$docker_cmd"
    return 0
}

# Allocate memory for containers
# Usage: allocate_memory_for_containers <total_memory_gb> <num_containers> <memory_headroom>
# Returns: Memory allocation results
# Behavior: Validates and allocates memory for parallel container execution
allocate_memory_for_containers() {
    local total_memory_gb="$1"
    local num_containers="$2"
    local memory_headroom="$3"

    # Validate inputs
    if ! [[ "$total_memory_gb" =~ ^[0-9]*\.?[0-9]+$ ]] || [[ "$(echo "$total_memory_gb <= 0" | bc -l 2>/dev/null || echo "1")" = "1" ]]; then
        echo "allocation_status=error"
        echo "error_message=Invalid total memory: $total_memory_gb"
        return 1
    fi

    if ! [[ "$num_containers" =~ ^[0-9]+$ ]] || [[ "$num_containers" -le 0 ]]; then
        echo "allocation_status=error"
        echo "error_message=Invalid number of containers: $num_containers"
        return 1
    fi

    if ! [[ "$memory_headroom" =~ ^[0-9]*\.?[0-9]+$ ]] || [[ "$(echo "$memory_headroom < 0 || $memory_headroom >= 1" | bc -l 2>/dev/null || echo "1")" = "1" ]]; then
        echo "allocation_status=error"
        echo "error_message=Invalid memory headroom: $memory_headroom"
        return 1
    fi

    # Calculate memory per container
    local calc_result
    calc_result=$(calculate_memory_per_container_gb "$total_memory_gb" "$num_containers" "$memory_headroom")

    if [[ $? -ne 0 ]]; then
        echo "allocation_status=error"
        echo "$calc_result"
        return 1
    fi

    # Extract memory per container
    local memory_per_container
    memory_per_container=$(echo "$calc_result" | grep "^memory_per_container_gb=" | cut -d'=' -f2)

    # Check if allocation is reasonable
    local warning_message=""
    if [[ "$(echo "$memory_per_container < 0.2" | bc -l 2>/dev/null || echo "0")" = "1" ]]; then
        warning_message="Low memory per container (${memory_per_container}GB) may cause performance issues"
    fi

    echo "allocation_status=success"
    echo "total_containers=$num_containers"
    echo "$calc_result"

    if [[ -n "$warning_message" ]]; then
        echo "warning_message=$warning_message"
    fi

    return 0
}

# Parse CLI memory options
# Usage: parse_memory_cli_options [options...]
# Returns: Parsed memory options in flat data format
# Behavior: Parses --max-memory-per-container, --total-memory-limit, --memory-headroom options
parse_memory_cli_options() {
    local max_memory_per_container=""
    local total_memory_limit=""
    local memory_headroom=""

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --max-memory-per-container)
                if [[ -n "$2" ]] && [[ "$2" != --* ]]; then
                    max_memory_per_container="$2"
                    shift 2
                else
                    echo "error_message=Missing value for --max-memory-per-container"
                    echo "parse_status=error"
                    return 1
                fi
                ;;
            --total-memory-limit)
                if [[ -n "$2" ]] && [[ "$2" != --* ]]; then
                    total_memory_limit="$2"
                    shift 2
                else
                    echo "error_message=Missing value for --total-memory-limit"
                    echo "parse_status=error"
                    return 1
                fi
                ;;
            --memory-headroom)
                if [[ -n "$2" ]] && [[ "$2" != --* ]]; then
                    memory_headroom="$2"
                    shift 2
                else
                    echo "error_message=Missing value for --memory-headroom"
                    echo "parse_status=error"
                    return 1
                fi
                ;;
            *)
                # Unknown option, skip
                shift
                ;;
        esac
    done

    # Validate and set defaults
    local errors=()

    # Validate max memory per container
    if [[ -n "$max_memory_per_container" ]]; then
        if ! [[ "$max_memory_per_container" =~ ^[0-9]*\.?[0-9]+$ ]]; then
            errors+=("Invalid max-memory-per-container: $max_memory_per_container")
        elif [[ "$(echo "$max_memory_per_container <= 0" | bc -l 2>/dev/null || echo "1")" = "1" ]]; then
            errors+=("max-memory-per-container must be greater than 0")
        fi
    else
        # Default: use available memory calculation
        local available_memory
        available_memory=$(get_available_memory_gb 2>/dev/null | grep "^available_memory_gb=" | cut -d'=' -f2 || echo "1.0")
        max_memory_per_container="$available_memory"
    fi

    # Validate total memory limit
    if [[ -n "$total_memory_limit" ]]; then
        if ! [[ "$total_memory_limit" =~ ^[0-9]*\.?[0-9]+$ ]]; then
            errors+=("Invalid total-memory-limit: $total_memory_limit")
        elif [[ "$(echo "$total_memory_limit <= 0" | bc -l 2>/dev/null || echo "1")" = "1" ]]; then
            errors+=("total-memory-limit must be greater than 0")
        fi
    else
        # Default: use total system memory
        local total_memory
        total_memory=$(get_total_memory_gb 2>/dev/null | grep "^total_memory_gb=" | cut -d'=' -f2 || echo "4.0")
        total_memory_limit="$total_memory"
    fi

    # Validate memory headroom
    if [[ -n "$memory_headroom" ]]; then
        if ! [[ "$memory_headroom" =~ ^[0-9]*\.?[0-9]+$ ]]; then
            errors+=("Invalid memory-headroom: $memory_headroom")
        elif [[ "$(echo "$memory_headroom < 0 || $memory_headroom >= 1" | bc -l 2>/dev/null || echo "1")" = "1" ]]; then
            errors+=("memory-headroom must be between 0.0 and 0.99")
        fi
    else
        # Default headroom: 20%
        memory_headroom="0.2"
    fi

    # Check for errors
    if [[ ${#errors[@]} -gt 0 ]]; then
        echo "parse_status=error"
        for error in "${errors[@]}"; do
            echo "error_message=$error"
        done
        return 1
    fi

    # Return parsed options
    echo "parse_status=success"
    echo "max_memory_per_container_gb=$max_memory_per_container"
    echo "total_memory_limit_gb=$total_memory_limit"
    echo "memory_headroom=$memory_headroom"

    return 0
}

# End of: src/parallel_execution.sh

# Included from: src/platform_detector.sh
#!/usr/bin/env bash

# Suitey Platform Detector
# Identifies which programming languages/frameworks are present in a project
# Uses Suitey Modules Registry to coordinate language-specific detection
# No external dependencies

# Source data access functions if available (for parsing flat data)
if [[ -f "src/data_access.sh" ]]; then
    source "src/data_access.sh" 2>/dev/null || true
fi

# Check container environment readiness
# Usage: check_container_environment
# Returns: Container environment status as flat data
# Behavior: Verifies Docker daemon accessibility, basic container operations, and network connectivity
check_container_environment() {
    local results=""
    local warnings=""

    # Check Docker command availability
    local docker_command_available="false"
    if command -v docker >/dev/null 2>&1; then
        docker_command_available="true"
    fi
    results="${results}docker_command_available=${docker_command_available}"$'\n'

    # Check Docker daemon accessibility (only if command is available)
    local docker_daemon_available="false"
    if [[ "$docker_command_available" == "true" ]]; then
        # Check if docker daemon is accessible
        if docker info >/dev/null 2>&1; then
            docker_daemon_available="true"
        else
            warnings="${warnings}Docker daemon is not running or accessible. Suitey requires Docker for test execution."$'\n'
        fi
    else
        warnings="${warnings}Docker command not found. Install Docker to enable test execution in containers."$'\n'
    fi
    results="${results}docker_daemon_available=${docker_daemon_available}"$'\n'

    # Check basic container operations (only if daemon is available)
    local container_operations="false"
    if [[ "$docker_daemon_available" == "true" ]]; then
        # Try to run a simple container operation
        if docker run --rm alpine:latest echo "test" >/dev/null 2>&1; then
            container_operations="true"
        else
            warnings="${warnings}Cannot execute basic container operations. Check Docker permissions and configuration."$'\n'
        fi
    fi
    results="${results}container_operations=${container_operations}"$'\n'

    # Check network connectivity for image pulls (only if daemon is available)
    local network_access="false"
    if [[ "$docker_daemon_available" == "true" ]]; then
        # Try to pull a small image to test network access
        if docker pull alpine:latest >/dev/null 2>&1; then
            network_access="true"
            # Clean up the pulled image
            docker rmi alpine:latest >/dev/null 2>&1 || true
        else
            warnings="${warnings}Cannot pull Docker images. Check network connectivity and Docker registry access."$'\n'
        fi
    fi
    results="${results}network_access=${network_access}"$'\n'

    # Provide backward compatibility aliases
    results="${results}docker_available=${docker_daemon_available}"$'\n'

    # Add warnings to results
    if [[ -n "$warnings" ]]; then
        # Convert warnings to flat data format
        local warning_count=0
        while IFS= read -r warning_line || [[ -n "$warning_line" ]]; do
            if [[ -n "$warning_line" ]]; then
                results="${results}docker_warnings_${warning_count}=${warning_line}"$'\n'
                ((warning_count++))
            fi
        done <<< "$warnings"
        results="${results}docker_warnings_count=${warning_count}"$'\n'
    else
        results="${results}docker_warnings_count=0"$'\n'
    fi

    echo "$results"
}

# Detect platforms in a project
# Usage: detect_platforms <project_root>
# Returns: Detection results as flat data
# Behavior: Uses Modules Registry to get all modules, calls each module's detect() method, aggregates results
detect_platforms() {
    local project_root="$1"

    # Validate input
    if [[ -z "$project_root" ]]; then
        echo "platforms_count=0"
        return 0
    fi

    # Get all registered modules
    local modules
    modules=$(get_all_modules 2>/dev/null || echo "")

    if [[ -z "$modules" ]]; then
        echo "platforms_count=0"
        return 0
    fi

    # Track detected platforms
    local platforms_count=0
    local platform_index=0
    local results=""

    # Check container environment readiness
    local container_env_status
    container_env_status=$(check_container_environment 2>/dev/null || echo "")
    if [[ -n "$container_env_status" ]]; then
        results="${results}${container_env_status}"
    fi

    # Process each module
    while IFS= read -r module_id || [[ -n "$module_id" ]]; do
        # Determine module file path based on module_id
        # Module IDs follow pattern: {language}-module or {framework}-module
        # Module files are at: mod/languages/{language}/mod.sh or mod/frameworks/{framework}/mod.sh
        local module_file=""

        # Try language modules first
        if [[ "$module_id" == *"-module" ]]; then
            local language="${module_id%-module}"
            module_file="mod/languages/${language}/mod.sh"
        fi

        # If not a language module, try framework modules
        if [[ ! -f "$module_file" ]] && [[ "$module_id" == *"-module" ]]; then
            local framework="${module_id%-module}"
            module_file="mod/frameworks/${framework}/mod.sh"
        fi

        # If not a framework module, try tool modules
        if [[ ! -f "$module_file" ]] && [[ "$module_id" == *"-module" ]]; then
            local tool="${module_id%-module}"
            module_file="mod/tools/${tool}/mod.sh"
        fi

        # If not found, skip
        if [[ ! -f "$module_file" ]]; then
            continue
        fi

        # Clean up any existing module functions to avoid conflicts
        for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
            unset -f "$method" 2>/dev/null || true
        done

        # Source the module
        source "$module_file" 2>/dev/null || continue

        # Call module's detect() method
        local detection_result
        detection_result=$(detect "$project_root" 2>/dev/null || echo "")

        if [[ -z "$detection_result" ]]; then
            continue
        fi

        # Check if platform was detected
        local detected
        if declare -f data_get >/dev/null 2>&1; then
            detected=$(data_get "$detection_result" "detected" || echo "false")
        else
            detected=$(echo "$detection_result" | grep "^detected=" | cut -d'=' -f2 || echo "false")
        fi

        if [[ "$detected" == "true" ]]; then
            # Platform detected - extract language and framework from detection result
            local language
            local framework
            local confidence

            if declare -f data_get >/dev/null 2>&1; then
                language=$(data_get "$detection_result" "language" || echo "")
                framework=$(data_get "$detection_result" "frameworks_0" || echo "")
                confidence=$(data_get "$detection_result" "confidence" || echo "low")
            else
                language=$(echo "$detection_result" | grep "^language=" | cut -d'=' -f2 || echo "")
                framework=$(echo "$detection_result" | grep "^frameworks_0=" | cut -d'=' -f2 || echo "")
                confidence=$(echo "$detection_result" | grep "^confidence=" | cut -d'=' -f2 || echo "low")
            fi

            # Validate required metadata for detected platforms
            if [[ -z "$language" ]]; then
                echo "Warning: Module '$module_id' detected platform but did not specify language. Skipping platform." >&2
                continue
            fi

            # Add platform to results
            if [[ -n "$results" ]]; then
                results="${results}"$'\n'"platforms_${platform_index}_language=${language}"
            else
                results="platforms_${platform_index}_language=${language}"
            fi

            if [[ -n "$framework" ]]; then
                results="${results}"$'\n'"platforms_${platform_index}_framework=${framework}"
            fi

            results="${results}"$'\n'"platforms_${platform_index}_confidence=${confidence}"
            results="${results}"$'\n'"platforms_${platform_index}_module_id=${module_id}"

            # Add module metadata from get_metadata()
            local module_metadata
            if declare -f get_metadata >/dev/null 2>&1; then
                module_metadata=$(get_metadata 2>/dev/null || echo "")
                if [[ -n "$module_metadata" ]]; then
                    # Parse and prefix each metadata line with platform index
                    while IFS= read -r metadata_line || [[ -n "$metadata_line" ]]; do
                        if [[ -n "$metadata_line" ]]; then
                            results="${results}"$'\n'"platforms_${platform_index}_${metadata_line}"
                        fi
                    done <<< "$module_metadata"
                fi
            fi

            # Add detection indicators
            local indicators_count
            if declare -f data_array_count >/dev/null 2>&1; then
                indicators_count=$(data_array_count "$detection_result" "indicators" || echo "0")
            else
                indicators_count=$(echo "$detection_result" | grep "^indicators_count=" | cut -d'=' -f2 || echo "0")
            fi
            results="${results}"$'\n'"platforms_${platform_index}_indicators_count=${indicators_count}"

            # Add individual indicators
            local i=0
            while [[ $i -lt "$indicators_count" ]]; do
                local indicator
                if declare -f data_get_array >/dev/null 2>&1; then
                    indicator=$(data_get_array "$detection_result" "indicators" "$i" || echo "")
                else
                    indicator=$(echo "$detection_result" | grep "^indicators_${i}=" | cut -d'=' -f2 || echo "")
                fi
                if [[ -n "$indicator" ]]; then
                    results="${results}"$'\n'"platforms_${platform_index}_indicators_${i}=${indicator}"
                fi
                i=$((i + 1))
            done


            platforms_count=$((platforms_count + 1))
            platform_index=$((platform_index + 1))
        fi
    done <<< "$modules"

    # Output results
    echo "platforms_count=${platforms_count}"
    if [[ -n "$results" ]]; then
        echo "$results"
    fi

    return 0
}

# End of: src/platform_detector.sh

# Included from: src/project_scanner.sh
#!/usr/bin/env bash

# Project Scanner (Orchestrator)
# Coordinates Platform Detection, Test Suite Detection, and Build System Detection
# Provides unified interface for project analysis

# Source required dependencies
source "src/mod_registry.sh" 2>/dev/null || true
source "src/platform_detector.sh" 2>/dev/null || true
source "src/test_suite_detector.sh" 2>/dev/null || true
source "src/build_system_detector.sh" 2>/dev/null || true
source "src/data_access.sh" 2>/dev/null || true

# Aggregate results from all detection phases into unified data structure
# Usage: aggregate_scan_results <platform_data> <suite_data> <build_data> <build_steps_data> <dependency_data>
# Returns: Unified aggregated results in flat data format
aggregate_scan_results() {
    local platform_data="$1"
    local suite_data="$2"
    local build_data="$3"
    local build_steps_data="$4"
    local dependency_data="$5"

    local result=""

    # Set overall scan status
    result=$(data_set "$result" "scan_result" "success")

    # Aggregate platform detection results
    result=$(data_set "$result" "platform_detection_status" "success")
    # Parse and merge platform data
    while IFS='=' read -r key value; do
        if [[ -n "$key" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$platform_data"

    # Aggregate test suite detection results
    result=$(data_set "$result" "test_suite_detection_status" "success")
    # Parse and merge suite data
    while IFS='=' read -r key value; do
        if [[ -n "$key" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$suite_data"

    # Aggregate build system detection results
    result=$(data_set "$result" "build_system_detection_status" "success")
    # Parse and merge build data
    while IFS='=' read -r key value; do
        if [[ -n "$key" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$build_data"

    # Aggregate build steps detection results
    result=$(data_set "$result" "build_steps_detection_status" "success")
    # Parse and merge build steps data
    while IFS='=' read -r key value; do
        if [[ -n "$key" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$build_steps_data"

    # Aggregate build dependency analysis results
    result=$(data_set "$result" "build_dependency_analysis_status" "success")
    # Parse and merge dependency data
    while IFS='=' read -r key value; do
        if [[ -n "$key" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$dependency_data"

    # Add summary information
    local platforms_count=$(echo "$platform_data" | grep "^platforms_count=" | cut -d'=' -f2 || echo "0")
    local suites_count=$(echo "$suite_data" | grep "^suites_count=" | cut -d'=' -f2 || echo "0")
    local requires_build=$(echo "$build_data" | grep "^requires_build=" | cut -d'=' -f2 || echo "false")
    local build_steps_count=$(echo "$build_steps_data" | grep "^build_steps_count=" | cut -d'=' -f2 || echo "0")

    result=$(data_set "$result" "summary_platforms_detected" "$platforms_count")
    result=$(data_set "$result" "summary_test_suites_found" "$suites_count")
    result=$(data_set "$result" "summary_build_required" "$requires_build")
    result=$(data_set "$result" "summary_build_steps_defined" "$build_steps_count")

    echo "$result"
}

# Scan project and detect all aspects
# Usage: scan_project <project_root>
# Returns: Unified project scan results in flat data format
# Behavior: Orchestrates all detection phases in correct order
scan_project() {
    local project_root="$1"

    # Validate input
    if [[ -z "$project_root" ]]; then
        echo "Error: Project root is required" >&2
        echo "scan_result=error"
        echo "error_message=Project root is required"
        return 1
    fi

    if [[ ! -d "$project_root" ]]; then
        echo "Error: Project root directory does not exist: $project_root" >&2
        echo "scan_result=error"
        echo "error_message=Project root directory does not exist"
        return 1
    fi

    # Initialize module registry
    echo "Initializing module registry..." >&2
    reset_registry

    # Load and register available modules
    if [[ -f "mod/languages/rust/mod.sh" ]]; then
        source "mod/languages/rust/mod.sh" 2>/dev/null || echo "Warning: Failed to load rust module" >&2
        register_module "rust-module" "rust-module" 2>/dev/null || echo "Warning: Failed to register rust module" >&2
    fi

    if [[ -f "mod/languages/bash/mod.sh" ]]; then
        source "mod/languages/bash/mod.sh" 2>/dev/null || echo "Warning: Failed to load bash module" >&2
        register_module "bash-module" "bash-module" 2>/dev/null || echo "Warning: Failed to register bash module" >&2
    fi

    if [[ -f "mod/frameworks/cargo/mod.sh" ]]; then
        source "mod/frameworks/cargo/mod.sh" 2>/dev/null || echo "Warning: Failed to load cargo module" >&2
        register_module "cargo-module" "cargo-module" 2>/dev/null || echo "Warning: Failed to register cargo module" >&2
    fi

    if [[ -f "mod/frameworks/bats/mod.sh" ]]; then
        source "mod/frameworks/bats/mod.sh" 2>/dev/null || echo "Warning: Failed to load bats module" >&2
        register_module "bats-module" "bats-module" 2>/dev/null || echo "Warning: Failed to register bats module" >&2
    fi

    if [[ -f "mod/tools/shellcheck/mod.sh" ]]; then
        source "mod/tools/shellcheck/mod.sh" 2>/dev/null || echo "Warning: Failed to load shellcheck module" >&2
        register_module "shellcheck-module" "shellcheck-module" 2>/dev/null || echo "Warning: Failed to register shellcheck module" >&2
    fi

    echo "Module registry initialized" >&2

    # Initialize result data
    local result=""
    local scan_success=true

    # Phase 1: Platform Detection
    result=$(data_set "$result" "scan_result" "success")
    result=$(data_set "$result" "project_root" "$project_root")

    echo "Starting platform detection for: $project_root" >&2

    local platform_data
    if platform_data=$(detect_platforms "$project_root" 2>&1); then
        result=$(data_set "$result" "platform_detection_status" "success")
        echo "Platform detection completed successfully" >&2
    else
        local error_msg="Platform detection failed"
        if [[ -n "$platform_data" ]]; then
            error_msg="$error_msg: $platform_data"
        fi
        result=$(data_set "$result" "platform_detection_status" "failed")
        result=$(data_set "$result" "platform_detection_error" "$error_msg")
        result=$(data_set "$result" "scan_result" "partial")
        scan_success=false
        echo "$error_msg" >&2
        echo "Continuing with other detection phases..." >&2
        # Continue with empty platform data for other phases
        platform_data="platforms_count=0"
    fi

    # Include platform detection results by setting individual fields
    # Parse and set platform data fields
    while IFS='=' read -r key value; do
        if [[ -n "$key" && "$key" != "platform_detection_status" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$platform_data"

    # Phase 2: Test Suite Detection (depends on platform detection)
    echo "Starting test suite detection" >&2

    local suite_data
    if suite_data=$(discover_test_suites "$platform_data" 2>&1); then
        result=$(data_set "$result" "test_suite_detection_status" "success")
        echo "Test suite detection completed successfully" >&2
    else
        local error_msg="Test suite detection failed"
        if [[ -n "$suite_data" ]]; then
            error_msg="$error_msg: $suite_data"
        fi
        result=$(data_set "$result" "test_suite_detection_status" "failed")
        result=$(data_set "$result" "test_suite_detection_error" "$error_msg")
        result=$(data_set "$result" "scan_result" "partial")
        scan_success=false
        echo "$error_msg" >&2
        echo "Continuing with build system detection..." >&2
        # Continue with empty suite data
        suite_data="suites_count=0"
    fi

    # Include test suite detection results by setting individual fields
    while IFS='=' read -r key value; do
        if [[ -n "$key" && "$key" != "test_suite_detection_status" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$suite_data"

    # Phase 3: Build System Detection (depends on platform detection)
    echo "Starting build system detection" >&2

    local build_data
    if build_data=$(detect_build_requirements "$platform_data" 2>&1); then
        result=$(data_set "$result" "build_system_detection_status" "success")
        echo "Build system detection completed successfully" >&2
    else
        local error_msg="Build system detection failed"
        if [[ -n "$build_data" ]]; then
            error_msg="$error_msg: $build_data"
        fi
        result=$(data_set "$result" "build_system_detection_status" "failed")
        result=$(data_set "$result" "build_system_detection_error" "$error_msg")
        result=$(data_set "$result" "scan_result" "partial")
        scan_success=false
        echo "$error_msg" >&2
        echo "Continuing with build steps detection..." >&2
        # Continue with safe defaults
        build_data="requires_build=false"$'\n'"build_commands_count=0"$'\n'"build_dependencies_count=0"$'\n'"build_artifacts_count=0"
    fi

    # Include build system detection results by setting individual fields
    while IFS='=' read -r key value; do
        if [[ -n "$key" && "$key" != "build_system_detection_status" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$build_data"

    # Phase 4: Build Steps Detection (depends on build requirements)
    echo "Starting build steps detection" >&2

    local build_steps_data
    if build_steps_data=$(get_build_steps "$platform_data" "$build_data" 2>&1); then
        result=$(data_set "$result" "build_steps_detection_status" "success")
        echo "Build steps detection completed successfully" >&2
    else
        local error_msg="Build steps detection failed"
        if [[ -n "$build_steps_data" ]]; then
            error_msg="$error_msg: $build_steps_data"
        fi
        result=$(data_set "$result" "build_steps_detection_status" "failed")
        result=$(data_set "$result" "build_steps_detection_error" "$error_msg")
        result=$(data_set "$result" "scan_result" "partial")
        scan_success=false
        echo "$error_msg" >&2
        echo "Continuing with build dependency analysis..." >&2
        # Continue with empty build steps
        build_steps_data="build_steps_count=0"
    fi

    # Include build steps detection results by setting individual fields
    while IFS='=' read -r key value; do
        if [[ -n "$key" && "$key" != "build_steps_detection_status" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$build_steps_data"

    # Phase 5: Build Dependency Analysis (depends on build steps)
    echo "Starting build dependency analysis" >&2

    local dependency_data
    if dependency_data=$(analyze_build_dependencies "$build_steps_data" 2>&1); then
        result=$(data_set "$result" "build_dependency_analysis_status" "success")
        echo "Build dependency analysis completed successfully" >&2
    else
        local error_msg="Build dependency analysis failed"
        if [[ -n "$dependency_data" ]]; then
            error_msg="$error_msg: $dependency_data"
        fi
        result=$(data_set "$result" "build_dependency_analysis_status" "failed")
        result=$(data_set "$result" "build_dependency_analysis_error" "$error_msg")
        result=$(data_set "$result" "scan_result" "partial")
        scan_success=false
        echo "$error_msg" >&2
        echo "Scan completed with some failures" >&2
        # Continue with safe defaults
        dependency_data="execution_order_count=0"$'\n'"parallel_groups_count=0"$'\n'"dependency_graph_count=0"
    fi

    # Include build dependency analysis results by setting individual fields
    while IFS='=' read -r key value; do
        if [[ -n "$key" && "$key" != "build_dependency_analysis_status" ]]; then
            result=$(data_set "$result" "$key" "$value")
        fi
    done <<< "$dependency_data"

    # Add summary information
    local summary_platforms_detected=$(echo "$platform_data" | grep "^platforms_count=" | cut -d'=' -f2 || echo "0")
    local summary_test_suites_found=$(echo "$suite_data" | grep "^suites_count=" | cut -d'=' -f2 || echo "0")
    local summary_requires_build=$(echo "$build_data" | grep "^requires_build=" | cut -d'=' -f2 || echo "false")
    local summary_build_steps_defined=$(echo "$build_steps_data" | grep "^build_steps_count=" | cut -d'=' -f2 || echo "0")

    result=$(data_set "$result" "summary_platforms_detected" "$summary_platforms_detected")
    result=$(data_set "$result" "summary_test_suites_found" "$summary_test_suites_found")
    result=$(data_set "$result" "summary_build_required" "$summary_requires_build")
    result=$(data_set "$result" "summary_build_steps_defined" "$summary_build_steps_defined")

    # Final status
    if [[ "$scan_success" == true ]]; then
        result=$(data_set "$result" "scan_result" "success")
        echo "Project scan completed successfully" >&2
    else
        echo "Project scan completed with partial failures" >&2
    fi

    echo "$result"
    return 0
}

# End of: src/project_scanner.sh

# Included from: src/suite_grouping.sh
#!/usr/bin/env bash

# Suite Grouping Functions
# Implements adaptive detection strategies for grouping test files into suites
# No external dependencies

# Source required dependencies
source "src/data_access.sh" 2>/dev/null || true

# Check if configuration file exists
# Usage: has_configuration_file <project_root>
# Returns: 0 if config file exists, 1 otherwise
# Outputs: "suitey.toml" or ".suiteyrc" if found
has_configuration_file() {
    local project_root="$1"
    
    if [[ -z "$project_root" ]] || [[ ! -d "$project_root" ]]; then
        return 1
    fi
    
    if [[ -f "$project_root/suitey.toml" ]]; then
        echo "suitey.toml"
        return 0
    elif [[ -f "$project_root/.suiteyrc" ]]; then
        echo ".suiteyrc"
        return 0
    fi
    
    return 1
}

# Parse TOML configuration file (simplified parser for suitey.toml)
# Usage: parse_toml_config <config_file>
# Returns: Suite definitions in flat data format
# Note: This is a simplified TOML parser that handles only the subset needed for Suitey
parse_toml_config() {
    local config_file="$1"
    
    if [[ -z "$config_file" ]] || [[ ! -f "$config_file" ]]; then
        return 1
    fi
    
    local suites_count=0
    local suite_index=0
    local in_suite=false
    local current_suite_name=""
    local current_suite_files=()
    local result=""
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Remove leading/trailing whitespace
        line="${line#"${line%%[![:space:]]*}"}"
        line="${line%"${line##*[![:space:]]}"}"
        
        # Skip empty lines and comments
        if [[ -z "$line" ]] || [[ "$line" =~ ^[[:space:]]*# ]]; then
            continue
        fi
        
        # Check for suite table array start: [[suites]]
        if [[ "$line" =~ ^\[\[suites\]\] ]]; then
            # Save previous suite if we were in one
            if [[ "$in_suite" == true ]] && [[ -n "$current_suite_name" ]] && [[ ${#current_suite_files[@]} -gt 0 ]]; then
                result=$(data_set "$result" "suites_${suite_index}_name" "$current_suite_name")
                local file_idx=0
                for file_pattern in "${current_suite_files[@]}"; do
                    result=$(data_set "$result" "suites_${suite_index}_files_${file_idx}" "$file_pattern")
                    ((file_idx++))
                done
                result=$(data_set "$result" "suites_${suite_index}_files_count" "$file_idx")
                ((suite_index++))
                ((suites_count++))
            fi
            
            # Start new suite
            in_suite=true
            current_suite_name=""
            current_suite_files=()
            continue
        fi
        
        # If we're in a suite, parse fields
        if [[ "$in_suite" == true ]]; then
            # Parse name = "value"
            if [[ "$line" =~ ^name[[:space:]]*=[[:space:]]*\"(.*)\" ]]; then
                current_suite_name="${BASH_REMATCH[1]}"
            elif [[ "$line" =~ ^name[[:space:]]*=[[:space:]]*\'([^\']*)\' ]]; then
                current_suite_name="${BASH_REMATCH[1]}"
            elif [[ "$line" =~ ^name[[:space:]]*=[[:space:]]*([^[:space:]]+) ]]; then
                current_suite_name="${BASH_REMATCH[1]}"
            fi
            
            # Parse files = ["pattern1", "pattern2"]
            if [[ "$line" =~ ^files[[:space:]]*=[[:space:]]*\[ ]]; then
                # Multi-line array - collect until closing bracket
                local array_content="${line#*\[}"
                array_content="${array_content%\]}"
                
                # Extract quoted strings
                while [[ "$array_content" =~ \"([^\"]+)\" ]]; do
                    current_suite_files+=("${BASH_REMATCH[1]}")
                    array_content="${array_content#*\"${BASH_REMATCH[1]}\"}"
                    array_content="${array_content#*,}"
                    array_content="${array_content#"${array_content%%[![:space:]]*}"}"
                done
            fi
            
            # Check if we've hit another section (starts with [)
            if [[ "$line" =~ ^\[ ]]; then
                # Save current suite
                if [[ -n "$current_suite_name" ]] && [[ ${#current_suite_files[@]} -gt 0 ]]; then
                    result=$(data_set "$result" "suites_${suite_index}_name" "$current_suite_name")
                    local file_idx=0
                    for file_pattern in "${current_suite_files[@]}"; do
                        result=$(data_set "$result" "suites_${suite_index}_files_${file_idx}" "$file_pattern")
                        ((file_idx++))
                    done
                    result=$(data_set "$result" "suites_${suite_index}_files_count" "$file_idx")
                    ((suite_index++))
                    ((suites_count++))
                fi
                in_suite=false
                current_suite_name=""
                current_suite_files=()
            fi
        fi
    done < "$config_file"
    
    # Save last suite if we were in one
    if [[ "$in_suite" == true ]] && [[ -n "$current_suite_name" ]] && [[ ${#current_suite_files[@]} -gt 0 ]]; then
        result=$(data_set "$result" "suites_${suite_index}_name" "$current_suite_name")
        local file_idx=0
        for file_pattern in "${current_suite_files[@]}"; do
            result=$(data_set "$result" "suites_${suite_index}_files_${file_idx}" "$file_pattern")
            ((file_idx++))
        done
        result=$(data_set "$result" "suites_${suite_index}_files_count" "$file_idx")
        ((suites_count++))
    fi
    
    # Set suites count
    result=$(data_set "$result" "suites_count" "$suites_count")
    
    echo "$result"
    return 0
}

# Check if directory matches conventional test suite names
# Usage: is_conventional_directory <directory_name>
# Returns: 0 if conventional, 1 otherwise
# Outputs: normalized suite name if conventional
is_conventional_directory() {
    local dir_name="$1"
    local normalized_name
    
    case "$dir_name" in
        unit|units)
            echo "unit"
            return 0
            ;;
        integration|integrations)
            echo "integration"
            return 0
            ;;
        e2e|end-to-end|end_to_end)
            echo "e2e"
            return 0
            ;;
        performance|perf)
            echo "performance"
            return 0
            ;;
        *)
            return 1
            ;;
    esac
}

# Group test files using convention-based strategy
# Usage: group_by_convention <project_root> <test_files>
# Returns: Grouped suites in flat data format
group_by_convention() {
    local project_root="$1"
    local test_files="$2"
    
    if [[ -z "$test_files" ]]; then
        echo "suites_count=0"
        return 0
    fi
    
    local suites_count=0
    local suite_index=0
    local result=""
    
    # Group files by their parent directory if it's conventional
    declare -A suite_files
    
    while IFS= read -r file || [[ -n "$file" ]]; do
        if [[ -z "$file" ]]; then
            continue
        fi
        
        # Get directory name
        local file_dir
        file_dir=$(dirname "$file")
        file_dir="${file_dir#$project_root/}"
        
        # Check each level of the path for conventional names
        local suite_name=""
        local path_parts
        IFS='/' read -ra path_parts <<< "$file_dir"
        
        for part in "${path_parts[@]}"; do
            if is_conventional_directory "$part" >/dev/null 2>&1; then
                suite_name=$(is_conventional_directory "$part")
                break
            fi
        done
        
        # If no conventional name found, use directory basename
        if [[ -z "$suite_name" ]]; then
            suite_name=$(basename "$file_dir")
            if [[ -z "$suite_name" ]] || [[ "$suite_name" == "." ]]; then
                suite_name="default"
            fi
        fi
        
        # Add file to suite
        if [[ -z "${suite_files[$suite_name]}" ]]; then
            suite_files[$suite_name]="$file"
        else
            suite_files[$suite_name]="${suite_files[$suite_name]}"$'\n'"$file"
        fi
    done <<< "$test_files"
    
    # Convert to flat data format
    for suite_name in "${!suite_files[@]}"; do
        local files_list="${suite_files[$suite_name]}"
        local file_count=0
        local file_idx=0
        
        result=$(data_set "$result" "suites_${suite_index}_name" "$suite_name")
        
        while IFS= read -r file || [[ -n "$file" ]]; do
            if [[ -n "$file" ]]; then
                result=$(data_set "$result" "suites_${suite_index}_files_${file_idx}" "$file")
                ((file_idx++))
                ((file_count++))
            fi
        done <<< "$files_list"
        
        result=$(data_set "$result" "suites_${suite_index}_files_count" "$file_count")
        ((suite_index++))
        ((suites_count++))
    done
    
    result=$(data_set "$result" "suites_count" "$suites_count")
    echo "$result"
    return 0
}

# Group test files by subdirectory structure (preserves user organization)
# Usage: group_by_subdirectory <project_root> <test_files>
# Returns: Grouped suites in flat data format
group_by_subdirectory() {
    local project_root="$1"
    local test_files="$2"
    
    if [[ -z "$test_files" ]]; then
        echo "suites_count=0"
        return 0
    fi
    
    local suites_count=0
    local suite_index=0
    local result=""
    declare -A suite_files
    
    while IFS= read -r file || [[ -n "$file" ]]; do
        if [[ -z "$file" ]]; then
            continue
        fi
        
        # Get relative directory path
        local file_dir
        file_dir=$(dirname "$file")
        file_dir="${file_dir#$project_root/}"
        
        # Use the directory path as suite name (normalize)
        local suite_name="$file_dir"
        if [[ -z "$suite_name" ]] || [[ "$suite_name" == "." ]]; then
            suite_name="root"
        fi
        
        # Normalize path separators
        suite_name="${suite_name//\//_}"
        
        # Add file to suite
        if [[ -z "${suite_files[$suite_name]}" ]]; then
            suite_files[$suite_name]="$file"
        else
            suite_files[$suite_name]="${suite_files[$suite_name]}"$'\n'"$file"
        fi
    done <<< "$test_files"
    
    # Convert to flat data format
    for suite_name in "${!suite_files[@]}"; do
        local files_list="${suite_files[$suite_name]}"
        local file_count=0
        local file_idx=0
        
        result=$(data_set "$result" "suites_${suite_index}_name" "$suite_name")
        
        while IFS= read -r file || [[ -n "$file" ]]; do
            if [[ -n "$file" ]]; then
                result=$(data_set "$result" "suites_${suite_index}_files_${file_idx}" "$file")
                ((file_idx++))
                ((file_count++))
            fi
        done <<< "$files_list"
        
        result=$(data_set "$result" "suites_${suite_index}_files_count" "$file_count")
        ((suite_index++))
        ((suites_count++))
    done
    
    result=$(data_set "$result" "suites_count" "$suites_count")
    echo "$result"
    return 0
}

# Group test files by directory (all files in a directory = one suite)
# Usage: group_by_directory <project_root> <test_files>
# Returns: Grouped suites in flat data format
group_by_directory() {
    local project_root="$1"
    local test_files="$2"
    
    if [[ -z "$test_files" ]]; then
        echo "suites_count=0"
        return 0
    fi
    
    local suites_count=0
    local suite_index=0
    local result=""
    declare -A suite_files
    
    while IFS= read -r file || [[ -n "$file" ]]; do
        if [[ -z "$file" ]]; then
            continue
        fi
        
        # Get directory basename as suite name
        local file_dir
        file_dir=$(dirname "$file")
        local suite_name
        suite_name=$(basename "$file_dir")
        
        if [[ -z "$suite_name" ]] || [[ "$suite_name" == "." ]]; then
            suite_name="root"
        fi
        
        # Add file to suite
        if [[ -z "${suite_files[$suite_name]}" ]]; then
            suite_files[$suite_name]="$file"
        else
            suite_files[$suite_name]="${suite_files[$suite_name]}"$'\n'"$file"
        fi
    done <<< "$test_files"
    
    # Convert to flat data format
    for suite_name in "${!suite_files[@]}"; do
        local files_list="${suite_files[$suite_name]}"
        local file_count=0
        local file_idx=0
        
        result=$(data_set "$result" "suites_${suite_index}_name" "$suite_name")
        
        while IFS= read -r file || [[ -n "$file" ]]; do
            if [[ -n "$file" ]]; then
                result=$(data_set "$result" "suites_${suite_index}_files_${file_idx}" "$file")
                ((file_idx++))
                ((file_count++))
            fi
        done <<< "$files_list"
        
        result=$(data_set "$result" "suites_${suite_index}_files_count" "$file_count")
        ((suite_index++))
        ((suites_count++))
    done
    
    result=$(data_set "$result" "suites_count" "$suites_count")
    echo "$result"
    return 0
}

# Group test files by file (each file = one suite)
# Usage: group_by_file <test_files>
# Returns: Grouped suites in flat data format
group_by_file() {
    local test_files="$1"
    
    if [[ -z "$test_files" ]]; then
        echo "suites_count=0"
        return 0
    fi
    
    local suites_count=0
    local suite_index=0
    local result=""
    
    while IFS= read -r file || [[ -n "$file" ]]; do
        if [[ -z "$file" ]]; then
            continue
        fi
        
        # Use filename (without extension) as suite name
        local suite_name
        suite_name=$(basename "$file")
        suite_name="${suite_name%.*}"
        
        if [[ -z "$suite_name" ]]; then
            suite_name="test_${suite_index}"
        fi
        
        result=$(data_set "$result" "suites_${suite_index}_name" "$suite_name")
        result=$(data_set "$result" "suites_${suite_index}_files_0" "$file")
        result=$(data_set "$result" "suites_${suite_index}_files_count" "1")
        ((suite_index++))
        ((suites_count++))
    done <<< "$test_files"
    
    result=$(data_set "$result" "suites_count" "$suites_count")
    echo "$result"
    return 0
}

# Apply adaptive suite grouping strategy
# Usage: apply_adaptive_grouping <project_root> <test_files>
# Returns: Grouped suites in flat data format
apply_adaptive_grouping() {
    local project_root="$1"
    local test_files="$2"
    
    if [[ -z "$test_files" ]]; then
        echo "suites_count=0"
        return 0
    fi
    
    # Strategy 1: Configuration-Driven (highest priority)
    local config_file
    config_file=$(has_configuration_file "$project_root")
    if [[ $? -eq 0 ]]; then
        local config_suites
        config_suites=$(parse_toml_config "$project_root/$config_file" 2>/dev/null)
        if [[ $? -eq 0 ]] && [[ -n "$config_suites" ]]; then
            local config_suites_count
            config_suites_count=$(data_get "$config_suites" "suites_count")
            if [[ -n "$config_suites_count" ]] && [[ "$config_suites_count" -gt 0 ]]; then
                echo "$config_suites"
                return 0
            fi
        fi
    fi
    
    # Strategy 2: Convention-Based
    local convention_result
    convention_result=$(group_by_convention "$project_root" "$test_files")
    local convention_count
    convention_count=$(data_get "$convention_result" "suites_count")
    
    # Check if we found conventional directories
    if [[ -n "$convention_count" ]] && [[ "$convention_count" -gt 0 ]]; then
        # Verify we actually grouped by convention (not just default)
        local has_conventional=false
        local i=0
        while [[ $i -lt "$convention_count" ]]; do
            local suite_name
            suite_name=$(data_get "$convention_result" "suites_${i}_name")
            if is_conventional_directory "$suite_name" >/dev/null 2>&1; then
                has_conventional=true
                break
            fi
            ((i++))
        done
        
        if [[ "$has_conventional" == true ]]; then
            echo "$convention_result"
            return 0
        fi
    fi
    
    # Strategy 3: Subdirectory-Aware
    # Check if files are organized in subdirectories
    local has_subdirs=false
    while IFS= read -r file || [[ -n "$file" ]]; do
        if [[ -n "$file" ]]; then
            local file_dir
            file_dir=$(dirname "$file")
            file_dir="${file_dir#$project_root/}"
            if [[ "$file_dir" != "." ]] && [[ "$file_dir" != "$(basename "$file")" ]]; then
                has_subdirs=true
                break
            fi
        fi
    done <<< "$test_files"
    
    if [[ "$has_subdirs" == true ]]; then
        echo "$(group_by_subdirectory "$project_root" "$test_files")"
        return 0
    fi
    
    # Strategy 4: Directory-Based
    local dir_result
    dir_result=$(group_by_directory "$project_root" "$test_files")
    local dir_count
    dir_count=$(data_get "$dir_result" "suites_count")
    
    if [[ -n "$dir_count" ]] && [[ "$dir_count" -gt 0 ]]; then
        echo "$dir_result"
        return 0
    fi
    
    # Strategy 5: File-Level (fallback)
    echo "$(group_by_file "$test_files")"
    return 0
}


# End of: src/suite_grouping.sh

# Included from: src/test_counter.sh
#!/usr/bin/env bash

# Test Counter Functions
# Counts individual tests in test files using platform-specific patterns
# No external dependencies

# Count tests in a BATS file
# Usage: count_bats_tests <file_path>
# Returns: Number of @test annotations found
count_bats_tests() {
    local file_path="$1"
    
    if [[ -z "$file_path" ]] || [[ ! -f "$file_path" ]]; then
        echo "0"
        return 0
    fi
    
    # Count @test annotations (excluding comments and strings)
    # Pattern: @test followed by optional whitespace and quoted string
    local count=0
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Remove leading/trailing whitespace
        line="${line#"${line%%[![:space:]]*}"}"
        line="${line%"${line##*[![:space:]]}"}"
        
        # Skip empty lines and comments
        if [[ -z "$line" ]] || [[ "$line" =~ ^[[:space:]]*# ]]; then
            continue
        fi
        
        # Check for @test annotation
        if [[ "$line" =~ @test ]]; then
            ((count++))
        fi
    done < "$file_path"
    
    echo "$count"
    return 0
}

# Count tests in a Rust file
# Usage: count_rust_tests <file_path>
# Returns: Number of #[test] functions found
count_rust_tests() {
    local file_path="$1"
    
    if [[ -z "$file_path" ]] || [[ ! -f "$file_path" ]]; then
        echo "0"
        return 0
    fi
    
    local count=0
    local in_test_module=false
    local brace_depth=0
    local test_module_started=false
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Keep original line for pattern matching (don't trim yet)
        local original_line="$line"
        
        # Remove leading/trailing whitespace for empty check
        local trimmed_line="${line#"${line%%[![:space:]]*}"}"
        trimmed_line="${trimmed_line%"${trimmed_line##*[![:space:]]}"}"
        
        # Skip empty lines
        if [[ -z "$trimmed_line" ]]; then
            continue
        fi
        
        # Track brace depth
        local open_braces="${original_line//[^\{]/}"
        local close_braces="${original_line//[^\}]/}"
        brace_depth=$((brace_depth + ${#open_braces} - ${#close_braces}))
        
        # Check if we're entering a test module
        if echo "$original_line" | grep -q '#\[cfg(test)\]'; then
            in_test_module=true
            test_module_started=false
            brace_depth=0  # Reset depth when entering test module
            continue
        fi
        
        # Check if we've entered the mod block after #[cfg(test)]
        if [[ "$in_test_module" == true ]] && echo "$original_line" | grep -q '^[[:space:]]*mod[[:space:]]'; then
            test_module_started=true
            continue
        fi
        
        # Count #[test] annotations (allow whitespace before #[test])
        # For integration tests (tests/ directory), count all #[test] regardless of module
        # For unit tests (src/ directory), only count if in #[cfg(test)] module
        if echo "$original_line" | grep -q '#\[test\]'; then
            # If file is in tests/ directory, always count
            if [[ "$file_path" =~ /tests/ ]]; then
                ((count++))
            elif [[ "$in_test_module" == true ]] && [[ "$test_module_started" == true ]]; then
                # If in test module and mod block has started, count it
                ((count++))
            fi
        fi
        
        # Check if we're leaving the test module (brace depth back to 0 or negative)
        if [[ "$in_test_module" == true ]] && [[ "$test_module_started" == true ]] && [[ $brace_depth -le 0 ]]; then
            in_test_module=false
            test_module_started=false
        fi
    done < "$file_path"
    
    echo "$count"
    return 0
}

# Count tests in a file based on file extension
# Usage: count_tests_in_file <file_path>
# Returns: Number of tests found
count_tests_in_file() {
    local file_path="$1"
    
    if [[ -z "$file_path" ]] || [[ ! -f "$file_path" ]]; then
        echo "0"
        return 0
    fi
    
    # Determine file type by extension
    local extension="${file_path##*.}"
    
    case "$extension" in
        bats)
            count_bats_tests "$file_path"
            ;;
        rs)
            count_rust_tests "$file_path"
            ;;
        *)
            # Unknown file type, return 0
            echo "0"
            ;;
    esac
}

# Count total tests in multiple files
# Usage: count_tests_in_files <file1> [file2] [file3] ...
# Returns: Total count of tests across all files
count_tests_in_files() {
    local total=0
    
    for file_path in "$@"; do
        if [[ -n "$file_path" ]] && [[ -f "$file_path" ]]; then
            local file_count
            file_count=$(count_tests_in_file "$file_path")
            total=$((total + file_count))
        fi
    done
    
    echo "$total"
    return 0
}

# Count tests for a suite (given suite data in flat format)
# Usage: count_tests_for_suite <suite_data> <suite_index>
# Returns: Total test count for the suite
count_tests_for_suite() {
    local suite_data="$1"
    local suite_index="$2"
    
    if [[ -z "$suite_data" ]] || [[ -z "$suite_index" ]]; then
        echo "0"
        return 0
    fi
    
    # Source data access functions
    source "src/data_access.sh" 2>/dev/null || true
    
    # Get files count for this suite
    local files_count
    if declare -f data_get >/dev/null 2>&1; then
        files_count=$(data_get "$suite_data" "suites_${suite_index}_files_count" || echo "0")
    else
        files_count=$(echo "$suite_data" | grep "^suites_${suite_index}_files_count=" | cut -d'=' -f2 || echo "0")
    fi
    
    if [[ -z "$files_count" ]] || [[ "$files_count" -eq 0 ]]; then
        echo "0"
        return 0
    fi
    
    # Collect all file paths
    local total=0
    local i=0
    while [[ $i -lt "$files_count" ]]; do
        local file_path
        if declare -f data_get >/dev/null 2>&1; then
            file_path=$(data_get "$suite_data" "suites_${suite_index}_files_${i}" || echo "")
        else
            file_path=$(echo "$suite_data" | grep "^suites_${suite_index}_files_${i}=" | cut -d'=' -f2 || echo "")
        fi
        
        if [[ -n "$file_path" ]] && [[ -f "$file_path" ]]; then
            local file_count
            file_count=$(count_tests_in_file "$file_path")
            total=$((total + file_count))
        fi
        
        ((i++))
    done
    
    echo "$total"
    return 0
}


# End of: src/test_counter.sh

# Included from: src/test_suite_detector.sh
#!/usr/bin/env bash

# Test Suite Detector
# This file implements test suite discovery for detected platforms

# Source required dependencies
source "src/data_access.sh" 2>/dev/null || true
source "src/suite_grouping.sh" 2>/dev/null || true
source "src/test_counter.sh" 2>/dev/null || true

# Discover test suites for detected platforms
# Directly loads and calls module discover_test_suites methods based on platform type
# Returns aggregated results in flat data format
discover_test_suites() {
    local platform_data="$1"

    # Initialize result data
    local result="suites_count=0"

    # Parse platform data to get platforms count
    local platforms_count
    platforms_count=$(data_get "$platform_data" "platforms_count")

    if [[ -z "$platforms_count" ]] || [[ "$platforms_count" -eq 0 ]]; then
        echo "$result"
        return 0
    fi

    # Process each detected platform
    local suite_index=0
    local i=0
    while [[ $i -lt "$platforms_count" ]]; do
        local language
        local framework
        local module_type

        language=$(data_get "$platform_data" "platforms_${i}_language")
        framework=$(data_get "$platform_data" "platforms_${i}_framework")
        module_type=$(data_get "$platform_data" "platforms_${i}_module_type")

        # Determine module file path based on platform type
        local module_file=""
        case "$module_type" in
            "language")
                module_file="mod/languages/${language}/mod.sh"
                ;;
            "framework")
                module_file="mod/frameworks/${framework}/mod.sh"
                ;;
            "project")
                # Project modules would be handled differently
                module_file=""
                ;;
        esac

        # If module file exists, load it and call discover_test_suites
        if [[ -n "$module_file" ]] && [[ -f "$module_file" ]]; then
            # Clean up any existing module functions to avoid conflicts
            for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
                unset -f "$method" 2>/dev/null || true
            done

            # Source the module
            source "$module_file" 2>/dev/null || continue

            # Get the project root from the platform data
            local project_root
            project_root=$(data_get "$platform_data" "project_root" || echo ".")

            # Call the module's discover_test_suites method
            local module_result
            module_result=$(discover_test_suites "$project_root" "$platform_data" 2>/dev/null || echo "suites_count=0")

            # Parse the module result and add to our result
            local module_suites_count
            module_suites_count=$(data_get "$module_result" "suites_count")

            if [[ -n "$module_suites_count" ]] && [[ "$module_suites_count" -gt 0 ]]; then
                # Add each suite from this module to our result
                local j=0
                while [[ $j -lt "$module_suites_count" ]]; do
                    # Copy suite data from module result to our result
                    local suite_prefix="suites_${suite_index}"
                    local module_suite_prefix="suites_${j}"

                    # Get all lines for this suite from module result
                    local suite_lines
                    suite_lines=$(echo "$module_result" | grep "^${module_suite_prefix}_")

                    # Add each line with updated index
                    while IFS= read -r line; do
                        if [[ -n "$line" ]]; then
                            local new_line="${line/${module_suite_prefix}_/${suite_prefix}_}"
                            result=$(data_set "$result" "${new_line%%=*}" "${new_line#*=}")
                        fi
                    done <<< "$suite_lines"

                    # Count tests for this suite
                    local suite_test_count=0
                    local files_count
                    files_count=$(data_get "$result" "suites_${suite_index}_files_count" || echo "0")
                    
                    if [[ -n "$files_count" ]] && [[ "$files_count" -gt 0 ]]; then
                        local k=0
                        while [[ $k -lt "$files_count" ]]; do
                            local test_file
                            test_file=$(data_get "$result" "suites_${suite_index}_files_${k}" || echo "")
                            
                            if [[ -n "$test_file" ]] && [[ -f "$test_file" ]]; then
                                local file_test_count
                                file_test_count=$(count_tests_in_file "$test_file" || echo "0")
                                suite_test_count=$((suite_test_count + file_test_count))
                            fi
                            ((k++))
                        done
                    fi
                    
                    # Add test count to suite data
                    result=$(data_set "$result" "suites_${suite_index}_test_count" "$suite_test_count")

                    ((suite_index++))
                    ((j++))
                done
            fi

            # Clean up module functions after use
            for method in detect check_binaries discover_test_suites detect_build_requirements get_build_steps execute_test_suite parse_test_results get_metadata; do
                unset -f "$method" 2>/dev/null || true
            done
        fi

        ((i++))
    done

    # Update the final suites count
    result=$(data_set "$result" "suites_count" "$suite_index")

    # Apply adaptive suite grouping if we have suites
    if [[ "$suite_index" -gt 0 ]]; then
        # Get project root from platform data
        local project_root
        project_root=$(data_get "$platform_data" "project_root" || echo ".")
        
        # Collect all test files from discovered suites
        local all_test_files=""
        local i=0
        while [[ $i -lt "$suite_index" ]]; do
            local files_count
            files_count=$(data_get "$result" "suites_${i}_files_count")
            
            if [[ -n "$files_count" ]] && [[ "$files_count" -gt 0 ]]; then
                local j=0
                while [[ $j -lt "$files_count" ]]; do
                    local test_file
                    test_file=$(data_get "$result" "suites_${i}_files_${j}")
                    if [[ -n "$test_file" ]]; then
                        if [[ -z "$all_test_files" ]]; then
                            all_test_files="$test_file"
                        else
                            all_test_files="${all_test_files}"$'\n'"$test_file"
                        fi
                    fi
                    ((j++))
                done
            fi
            ((i++))
        done
        
        # Apply adaptive grouping if we have test files
        if [[ -n "$all_test_files" ]]; then
            local grouped_result
            grouped_result=$(apply_adaptive_grouping "$project_root" "$all_test_files" 2>/dev/null)
            
            # If grouping produced results, use them (configuration-driven takes precedence)
            if [[ $? -eq 0 ]] && [[ -n "$grouped_result" ]]; then
                local grouped_count
                grouped_count=$(data_get "$grouped_result" "suites_count")
                
                # Only use grouped result if configuration file exists (highest priority)
                local config_file
                config_file=$(has_configuration_file "$project_root" 2>/dev/null)
                if [[ $? -eq 0 ]]; then
                    result="$grouped_result"
                fi
            fi
        fi
    fi

    echo "$result"
}


# End of: src/test_suite_detector.sh

# Included from: mod/frameworks/bats/mod.sh
#!/usr/bin/env bash

# Suitey BATS Framework Module
# Handles BATS framework for Bash language
# Provides framework-specific test discovery, execution, and parsing for BATS projects
#
# This module works in conjunction with the Bash language module:
# - Language module (mod/languages/bash/mod.sh) detects Bash language presence
# - Framework module (this module) handles BATS-specific operations
# - Framework module has higher priority than language module for framework-specific operations

# Detect if BATS project is present (framework-level detection)
# Usage: detect <project_root>
# Returns: Detection result as flat data
# Behavior: Checks for .bats files (framework-specific indicator)
detect() {
    local project_root="$1"

    # Validate input
    if [[ -z "$project_root" ]] || [[ ! -d "$project_root" ]]; then
        echo "detected=false"
        echo "confidence=low"
        echo "indicators_count=0"
        echo "language=bash"
        echo "frameworks_0=bats"
        return 0
    fi

    # Check for .bats files (primary BATS indicator)
    # Look in common test directories: tests/bats/, test/bats/, tests/, test/
    local bats_files
    bats_files=$(find "$project_root" -maxdepth 3 -name "*.bats" -type f 2>/dev/null | head -1)
    if [[ -n "$bats_files" ]]; then
        echo "detected=true"
        echo "confidence=high"
        echo "indicators_0=bats_test_files"
        echo "indicators_count=1"
        echo "language=bash"
        echo "frameworks_0=bats"
        return 0
    fi

    # Check for BATS test directory structure (secondary indicator)
    if [[ -d "$project_root/tests/bats" ]] || [[ -d "$project_root/test/bats" ]]; then
        echo "detected=true"
        echo "confidence=medium"
        echo "indicators_0=bats_test_directory"
        echo "indicators_count=1"
        echo "language=bash"
        echo "frameworks_0=bats"
        return 0
    fi

    # Not detected
    echo "detected=false"
    echo "confidence=low"
    echo "indicators_count=0"
    echo "language=bash"
    echo "frameworks_0=bats"
    return 0
}

# Check if required binaries are available
# Usage: check_binaries <project_root>
# Returns: Binary status as flat data
# Behavior: Checks for bats binary in PATH and reports version if available
check_binaries() {
    local project_root="$1"

    # Check for bats binary in PATH
    if command -v bats >/dev/null 2>&1; then
        local bats_version
        bats_version=$(bats --version 2>/dev/null | head -1 || echo "unknown")
        # Remove "bats " prefix if present
        bats_version="${bats_version#bats }"
        echo "available=true"
        echo "binaries_0=bats"
        echo "binaries_count=1"
        echo "versions_bats=$bats_version"
        echo "container_check=false"
    else
        echo "available=false"
        echo "binaries_0=bats"
        echo "binaries_count=1"
        echo "container_check=false"
    fi

    return 0
}

# Discover test suites in the project using BATS-specific patterns
# Usage: discover_test_suites <project_root> <framework_metadata>
# Returns: Test suites as flat data
# Behavior: Finds .bats files in common test directories (tests/bats/, test/bats/, etc.)
discover_test_suites() {
    local project_root="$1"
    local framework_metadata="$2"

    # Validate input
    if [[ -z "$project_root" ]] || [[ ! -d "$project_root" ]]; then
        echo "suites_count=0"
        return 0
    fi

    local suites_count=0
    local suite_index=0
    local results=""

    # Discover .bats files in common test directories
    # BATS tests are typically organized in tests/bats/ or test/bats/ directories
    local test_dirs=("$project_root/tests/bats" "$project_root/test/bats" "$project_root/tests" "$project_root/test")
    
    for test_dir in "${test_dirs[@]}"; do
        if [[ -d "$test_dir" ]]; then
            local bats_files
            bats_files=$(find "$test_dir" -name "*.bats" -type f 2>/dev/null)
            
            if [[ -n "$bats_files" ]]; then
                local file_count
                file_count=$(echo "$bats_files" | wc -l)
                
                if [[ $file_count -gt 0 ]]; then
                    # Create a suite for this directory
                    local suite_name
                    suite_name=$(basename "$test_dir")
                    
                    if [[ -z "$results" ]]; then
                        results="suites_${suite_index}_name=${suite_name}"
                    else
                        results="${results}"$'\n'"suites_${suite_index}_name=${suite_name}"
                    fi
                    results="${results}"$'\n'"suites_${suite_index}_framework=bats"
                    results="${results}"$'\n'"suites_${suite_index}_test_files_count=${file_count}"
                    
                    suites_count=$((suites_count + 1))
                    suite_index=$((suite_index + 1))
                    
                    # Only process first matching directory to avoid duplicates
                    break
                fi
            fi
        fi
    done

    # If no test directories found, search for .bats files anywhere
    if [[ $suites_count -eq 0 ]]; then
        local bats_files
        bats_files=$(find "$project_root" -maxdepth 3 -name "*.bats" -type f 2>/dev/null)
        
        if [[ -n "$bats_files" ]]; then
            local file_count
            file_count=$(echo "$bats_files" | wc -l)
            
            if [[ $file_count -gt 0 ]]; then
                results="suites_0_name=bats_tests"
                results="${results}"$'\n'"suites_0_framework=bats"
                results="${results}"$'\n'"suites_0_test_files_count=${file_count}"
                suites_count=1
            fi
        fi
    fi

    # Output results
    echo "suites_count=${suites_count}"
    if [[ -n "$results" ]]; then
        echo "$results"
    fi

    return 0
}

# Detect build requirements for BATS projects
# Usage: detect_build_requirements <project_root> <framework_metadata>
# Returns: Build requirements as flat data
detect_build_requirements() {
    local project_root="$1"
    local framework_metadata="$2"

    # BATS projects typically don't require building (scripts are interpreted)
    echo "requires_build=false"
    echo "build_steps_count=0"
    echo "build_commands_count=0"
    echo "build_dependencies_count=0"
    echo "build_artifacts_count=0"
    return 0
}

# Get build steps for containerized build
# Usage: get_build_steps <project_root> <build_requirements>
# Returns: Build steps as flat data
get_build_steps() {
    local project_root="$1"
    local build_requirements="$2"

    # Stub implementation (no build steps needed for BATS)
    echo "build_steps_count=0"
    return 0
}

# Execute test suite using BATS test runner
# Usage: execute_test_suite <test_suite> <test_image> <execution_config>
# Returns: Execution result as flat data
# Behavior: Executes bats command in container
execute_test_suite() {
    local test_suite="$1"
    local test_image="$2"
    local execution_config="$3"

    # Stub implementation (will be expanded in later phases)
    # For now, return basic structure
    echo "exit_code=0"
    echo "duration=0.0"
    echo "execution_method=docker"
    echo "test_command=bats"
    return 0
}

# Parse test results from BATS test output
# Usage: parse_test_results <output> <exit_code>
# Returns: Parsed results as flat data
# Behavior: Parses bats test output to extract test counts and status
parse_test_results() {
    local output="$1"
    local exit_code="$2"

    # Initialize counters
    local total_tests=0
    local passed_tests=0
    local failed_tests=0
    local skipped_tests=0
    local test_details_count=0
    local test_details=""

    # Parse test plan line (e.g., "1..5")
    # Look for the plan line
    local plan_line
    plan_line=$(echo "$output" | grep -E "^[0-9]+\.\.[0-9]+$" | head -1)

    if [[ -n "$plan_line" ]]; then
        # Extract total tests from plan line (format: "1..N")
        total_tests=$(echo "$plan_line" | cut -d'.' -f3)
    fi

    # Parse individual test results
    # Look for lines like: "ok N description" or "not ok N description"
    local test_lines
    test_lines=$(echo "$output" | grep -E "^(ok|not ok) [0-9]+")

    if [[ -n "$test_lines" ]]; then
        # Count the individual test results
        test_details_count=$(echo "$test_lines" | wc -l)

        # Count passed and failed tests
        passed_tests=$(echo "$test_lines" | grep -c "^ok ")
        failed_tests=$(echo "$test_lines" | grep -c "^not ok ")

        # If we didn't get total from plan, use the count of test lines
        if [[ $total_tests -eq 0 ]]; then
            total_tests=$test_details_count
        fi

        # Store individual test details (simplified format)
        local test_index=0
        while IFS= read -r line; do
            if [[ $test_index -lt 10 ]]; then  # Limit to first 10 tests to avoid excessive output
                local test_status
                local test_number
                local test_name

                if [[ "$line" =~ ^ok\ ([0-9]+)\ (.*)$ ]]; then
                    test_status="ok"
                    test_number="${BASH_REMATCH[1]}"
                    test_name="${BASH_REMATCH[2]}"
                elif [[ "$line" =~ ^not\ ok\ ([0-9]+)\ (.*)$ ]]; then
                    test_status="not ok"
                    test_number="${BASH_REMATCH[1]}"
                    test_name="${BASH_REMATCH[2]}"
                fi

                if [[ -n "$test_name" ]]; then
                    test_details="${test_details}test_details_${test_index}_name=$test_name"$'\n'
                    test_details="${test_details}test_details_${test_index}_status=$test_status"$'\n'
                    ((test_index++))
                fi
            fi
        done <<< "$test_lines"
    fi

    # Handle BATS skip comments (# skip)
    # BATS marks skipped tests with "# skip" comments in the output
    local skip_count
    skip_count=$(echo "$output" | grep -c "# skip")
    skipped_tests=$skip_count

    # If we couldn't parse from output, fall back to exit code based detection
    if [[ $total_tests -eq 0 && $test_details_count -eq 0 ]]; then
        # If output is completely empty, assume no tests ran
        if [[ -z "$output" ]]; then
            total_tests=0
            passed_tests=0
            failed_tests=0
            skipped_tests=0
        else
            # Try to count any ok/not ok lines as fallback
            local any_test_lines
            any_test_lines=$(echo "$output" | grep -c -E "^(ok|not ok)")
            if [[ $any_test_lines -gt 0 ]]; then
                total_tests=$any_test_lines
                passed_tests=$(echo "$output" | grep -c "^ok")
                failed_tests=$(echo "$output" | grep -c "^not ok")
            fi
        fi
    fi

    # Determine overall status
    local status="unknown"
    if [[ "$exit_code" == "0" && $failed_tests -eq 0 ]]; then
        status="passed"
    elif [[ "$exit_code" != "0" || $failed_tests -gt 0 ]]; then
        status="failed"
    fi

    # Output results in flat data format
    echo "total_tests=$total_tests"
    echo "passed_tests=$passed_tests"
    echo "failed_tests=$failed_tests"
    echo "skipped_tests=$skipped_tests"
    echo "test_details_count=$test_details_count"
    echo "status=$status"

    # Add individual test details if available
    if [[ -n "$test_details" ]]; then
        echo "$test_details"
    fi

    return 0
}

# Get module metadata
# Usage: get_metadata
# Returns: Module metadata as flat data
get_metadata() {
    echo "module_type=framework"
    echo "language=bash"
    echo "frameworks_0=bats"
    echo "frameworks_count=1"
    echo "project_type=shell_script"
    echo "version=0.1.0"
    echo "capabilities_0=testing"
    echo "capabilities_count=1"
    echo "required_binaries_0=bats"
    echo "required_binaries_count=1"
    return 0
}


# End of: mod/frameworks/bats/mod.sh

# Included from: mod/frameworks/cargo/mod.sh
#!/usr/bin/env bash

# Suitey Cargo Framework Module
# Handles Cargo framework for Rust language
# Provides framework-specific test discovery, execution, and parsing for Cargo projects
#
# This module works in conjunction with the Rust language module:
# - Language module (mod/languages/rust/mod.sh) detects Rust language presence
# - Framework module (this module) handles Cargo-specific operations
# - Framework module has higher priority than language module for framework-specific operations

# Detect if Cargo project is present (framework-level detection)
# Usage: detect <project_root>
# Returns: Detection result as flat data
# Behavior: Checks for Cargo.toml (framework-specific indicator)
detect() {
    local project_root="$1"

    # Validate input
    if [[ -z "$project_root" ]] || [[ ! -d "$project_root" ]]; then
        echo "detected=false"
        echo "confidence=low"
        echo "indicators_count=0"
        echo "language=rust"
        echo "frameworks_0=cargo"
        return 0
    fi

    # Check for Cargo.toml file (primary Cargo indicator)
    if [[ -f "$project_root/Cargo.toml" ]]; then
        echo "detected=true"
        echo "confidence=high"
        echo "indicators_0=Cargo.toml"
        echo "indicators_count=1"
        echo "language=rust"
        echo "frameworks_0=cargo"
        return 0
    fi

    # Not detected
    echo "detected=false"
    echo "confidence=low"
    echo "indicators_count=0"
    echo "language=rust"
    echo "frameworks_0=cargo"
    return 0
}

# Check if required binaries are available
# Usage: check_binaries <project_root>
# Returns: Binary status as flat data
# Behavior: Checks for cargo binary in PATH and reports version if available
check_binaries() {
    local project_root="$1"

    # Check for cargo binary in PATH
    if command -v cargo >/dev/null 2>&1; then
        local cargo_version
        cargo_version=$(cargo --version 2>/dev/null | head -1 || echo "unknown")
        # Remove "cargo " prefix if present
        cargo_version="${cargo_version#cargo }"
        echo "available=true"
        echo "binaries_0=cargo"
        echo "binaries_count=1"
        echo "versions_cargo=$cargo_version"
        echo "container_check=false"
    else
        echo "available=false"
        echo "binaries_0=cargo"
        echo "binaries_count=1"
        echo "container_check=false"
    fi

    return 0
}

# Discover test suites in the project using Cargo-specific patterns
# Usage: discover_test_suites <project_root> <framework_metadata>
# Returns: Test suites as flat data
# Behavior: Finds unit tests in src/ and integration tests in tests/ directory
discover_test_suites() {
    local project_root="$1"
    local framework_metadata="$2"

    # Validate input
    if [[ -z "$project_root" ]] || [[ ! -d "$project_root" ]]; then
        echo "suites_count=0"
        return 0
    fi

    local suites_count=0
    local suite_index=0
    local results=""

    # Discover unit tests in src/ directory
    # Cargo unit tests are in files with #[cfg(test)] modules
    if [[ -d "$project_root/src" ]]; then
        local unit_test_files
        unit_test_files=$(find "$project_root/src" -name "*.rs" -type f 2>/dev/null | head -5)
        
        if [[ -n "$unit_test_files" ]]; then
            # Count files with test modules (simplified - just check if file exists)
            local unit_file_count
            unit_file_count=$(echo "$unit_test_files" | wc -l)
            
            if [[ $unit_file_count -gt 0 ]]; then
                if [[ -z "$results" ]]; then
                    results="suites_${suite_index}_name=unit_tests"
                else
                    results="${results}"$'\n'"suites_${suite_index}_name=unit_tests"
                fi
                results="${results}"$'\n'"suites_${suite_index}_framework=cargo"
                results="${results}"$'\n'"suites_${suite_index}_test_files_count=${unit_file_count}"
                
                suites_count=$((suites_count + 1))
                suite_index=$((suite_index + 1))
            fi
        fi
    fi

    # Discover integration tests in tests/ directory
    if [[ -d "$project_root/tests" ]]; then
        local integration_test_files
        integration_test_files=$(find "$project_root/tests" -name "*.rs" -type f 2>/dev/null)
        
        if [[ -n "$integration_test_files" ]]; then
            local integration_file_count
            integration_file_count=$(echo "$integration_test_files" | wc -l)
            
            if [[ $integration_file_count -gt 0 ]]; then
                if [[ -z "$results" ]]; then
                    results="suites_${suite_index}_name=integration_tests"
                else
                    results="${results}"$'\n'"suites_${suite_index}_name=integration_tests"
                fi
                results="${results}"$'\n'"suites_${suite_index}_framework=cargo"
                results="${results}"$'\n'"suites_${suite_index}_test_files_count=${integration_file_count}"
                
                suites_count=$((suites_count + 1))
            fi
        fi
    fi

    # Output results
    echo "suites_count=${suites_count}"
    if [[ -n "$results" ]]; then
        echo "$results"
    fi

    return 0
}

# Detect build requirements for Cargo projects
# Usage: detect_build_requirements <project_root> <framework_metadata>
# Returns: Build requirements as flat data
detect_build_requirements() {
    local project_root="$1"
    local framework_metadata="$2"

    # Cargo projects require building before testing
    echo "requires_build=true"
    echo "build_steps_count=1"
    echo "build_commands_0=cargo build --tests"
    echo "build_commands_count=1"
    echo "build_dependencies_count=0"
    echo "build_artifacts_count=0"
    return 0
}

# Get build steps for containerized build
# Usage: get_build_steps <project_root> <build_requirements>
# Returns: Build steps as flat data
get_build_steps() {
    local project_root="$1"
    local build_requirements="$2"

    # Check if building is required
    local requires_build
    requires_build=$(echo "$build_requirements" | grep "^requires_build=" | cut -d'=' -f2 || echo "false")

    if [[ "$requires_build" != "true" ]]; then
    echo "build_steps_count=0"
        return 0
    fi

    # Cargo build step (similar to Rust language module but framework-specific)
    # Note: Build execution happens in isolated Docker containers.
    # Project directory is mounted read-only (when volume mounts are configured).
    # Build artifacts are stored in container volumes, not in project directory.
    echo "build_steps_count=1"
    echo "build_steps_0_step_name=cargo_build"
    echo "build_steps_0_docker_image=rust:1.70-slim"
    echo "build_steps_0_build_command=cargo build --tests"
    echo "build_steps_0_working_directory=/workspace"
    echo "build_steps_0_volume_mounts_count=0"  # No volume mounts needed (project copied into container)
    echo "build_steps_0_volume_mounts_readonly=true"  # When mounts are used, they are read-only
    echo "build_steps_0_environment_variables_count=0"
    echo "build_steps_0_cpu_cores=0"  # Use all available cores

    return 0
}

# Execute test suite using Cargo test runner
# Usage: execute_test_suite <test_suite> <test_image> <execution_config>
# Returns: Execution result as flat data
# Behavior: Executes cargo test command in container
execute_test_suite() {
    local test_suite="$1"
    local test_image="$2"
    local execution_config="$3"

    # Stub implementation (will be expanded in later phases)
    # For now, return basic structure
    echo "exit_code=0"
    echo "duration=0.0"
    echo "execution_method=docker"
    echo "test_command=cargo test"
    return 0
}

# Parse test results from Cargo test output
# Usage: parse_test_results <output> <exit_code>
# Returns: Parsed results as flat data
# Behavior: Parses cargo test output to extract test counts and status
parse_test_results() {
    local output="$1"
    local exit_code="$2"

    # Initialize counters
    local total_tests=0
    local passed_tests=0
    local failed_tests=0
    local skipped_tests=0
    local test_details_count=0
    local test_details=""

    # Parse test summary line (e.g., "test result: ok. 3 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out")
    # Look for the summary line
    local summary_line
    summary_line=$(echo "$output" | grep -E "test result:" | head -1)

    if [[ -n "$summary_line" ]]; then
        # Extract numbers from summary line
        # Format: "test result: ok/FAILED. X passed; Y failed; Z ignored; W measured; V filtered out"

        # Extract passed count
        local passed_count
        passed_count=$(echo "$summary_line" | sed -n 's/.* \([0-9]\+\) passed.*/\1/p')
        passed_tests=${passed_count:-0}

        # Extract failed count
        local failed_count
        failed_count=$(echo "$summary_line" | sed -n 's/.* \([0-9]\+\) failed.*/\1/p')
        failed_tests=${failed_count:-0}

        # Extract ignored/skipped count
        local ignored_count
        ignored_count=$(echo "$summary_line" | sed -n 's/.* \([0-9]\+\) ignored.*/\1/p')
        skipped_tests=${ignored_count:-0}

        # Calculate total tests
        total_tests=$((passed_tests + failed_tests + skipped_tests))
    fi

    # Parse individual test results
    # Look for lines like: "test test_name ... ok" or "test test_name ... FAILED"
    local test_lines
    test_lines=$(echo "$output" | grep -E "^test " | grep -E "\.\.\. (ok|FAILED|ignored)$")

    if [[ -n "$test_lines" ]]; then
        # Count the individual test results
        test_details_count=$(echo "$test_lines" | wc -l)

        # Store individual test details (simplified format)
        local test_index=0
        while IFS= read -r line; do
            if [[ $test_index -lt 10 ]]; then  # Limit to first 10 tests to avoid excessive output
                local test_name
                test_name=$(echo "$line" | sed -n 's/^test \(.*\) \.\.\. .*$/\1/p')
                local test_status
                test_status=$(echo "$line" | sed -n 's/.* \.\.\. \(.*\)$/\1/p')

                if [[ -n "$test_name" ]]; then
                    test_details="${test_details}test_details_${test_index}_name=$test_name"$'\n'
                    test_details="${test_details}test_details_${test_index}_status=$test_status"$'\n'
                    ((test_index++))
                fi
            fi
        done <<< "$test_lines"
    fi

    # If we couldn't parse from output, fall back to exit code based detection
    if [[ $total_tests -eq 0 && $test_details_count -eq 0 ]]; then
        # Check for "running X tests" line as fallback
        local running_line
        running_line=$(echo "$output" | grep -E "running [0-9]+ tests" | head -1)
        if [[ -n "$running_line" ]]; then
            total_tests=$(echo "$running_line" | sed -n 's/running \([0-9]\+\) tests/\1/p')
        fi

        # If output is completely empty or no test info found, assume no tests ran
        if [[ -z "$output" ]] || [[ $total_tests -eq 0 ]]; then
            total_tests=0
            passed_tests=0
            failed_tests=0
            skipped_tests=0
        fi
    fi

    # Determine overall status
    local status="unknown"
    if [[ "$exit_code" == "0" && $failed_tests -eq 0 ]]; then
        status="passed"
    elif [[ "$exit_code" != "0" || $failed_tests -gt 0 ]]; then
        status="failed"
    fi

    # Output results in flat data format
    echo "total_tests=$total_tests"
    echo "passed_tests=$passed_tests"
    echo "failed_tests=$failed_tests"
    echo "skipped_tests=$skipped_tests"
    echo "test_details_count=$test_details_count"
    echo "status=$status"

    # Add individual test details if available
    if [[ -n "$test_details" ]]; then
        echo "$test_details"
    fi

    return 0
}

# Get module metadata
# Usage: get_metadata
# Returns: Module metadata as flat data
get_metadata() {
    echo "module_type=framework"
    echo "language=rust"
    echo "frameworks_0=cargo"
    echo "frameworks_count=1"
    echo "project_type=cargo_project"
    echo "version=0.1.0"
    echo "capabilities_0=testing"
    echo "capabilities_1=compilation"
    echo "capabilities_count=2"
    echo "required_binaries_0=cargo"
    echo "required_binaries_count=1"
    return 0
}


# End of: mod/frameworks/cargo/mod.sh

# Included from: mod/languages/bash/mod.sh
#!/usr/bin/env bash

# Suitey Bash Module
# Handles Bash language with BATS framework
# Provides detection, test discovery, build detection, and execution for Bash projects

# Detect if Bash/BATS project is present
# Usage: detect <project_root>
# Returns: Detection result as flat data
# Behavior: Checks for .bats files (high confidence), test directories (medium), or .sh files (low)
detect() {
    local project_root="$1"

    # Validate input
    if [[ -z "$project_root" ]] || [[ ! -d "$project_root" ]]; then
        echo "detected=false"
        echo "confidence=low"
        echo "indicators_count=0"
        echo "language=bash"
        echo "frameworks_0=bats"
        return 0
    fi

    # Check for .bats files (primary indicator - highest confidence)
    # Look in common test directories: tests/bats/, test/bats/, tests/, test/
    local bats_files
    bats_files=$(find "$project_root" -maxdepth 3 -name "*.bats" -type f 2>/dev/null | head -1)
    if [[ -n "$bats_files" ]]; then
        echo "detected=true"
        echo "confidence=high"
        echo "indicators_0=bats_test_files"
        echo "indicators_count=1"
        echo "language=bash"
        echo "frameworks_0=bats"
        return 0
    fi

    # Check for BATS test directory structure (secondary indicator - medium confidence)
    if [[ -d "$project_root/tests/bats" ]] || [[ -d "$project_root/test/bats" ]]; then
        echo "detected=true"
        echo "confidence=medium"
        echo "indicators_0=bats_test_directory"
        echo "indicators_count=1"
        echo "language=bash"
        echo "frameworks_0=bats"
        return 0
    fi

    # Check for .sh files with bash shebang (weak indicator - low confidence)
    # Only check in top-level directories to avoid false positives
    local sh_files
    sh_files=$(find "$project_root" -maxdepth 2 -name "*.sh" -type f 2>/dev/null | head -1)
    if [[ -n "$sh_files" ]]; then
        # Check if file has bash shebang
        if head -1 "$sh_files" 2>/dev/null | grep -q "#!/usr/bin/env bash\|#!/bin/bash"; then
            echo "detected=true"
            echo "confidence=low"
            echo "indicators_0=bash_script_files"
            echo "indicators_count=1"
            echo "language=bash"
            echo "frameworks_0=bats"
            return 0
        fi
    fi

    # Not detected
    echo "detected=false"
    echo "confidence=low"
    echo "indicators_count=0"
    echo "language=bash"
    echo "frameworks_0=bats"
    return 0
}

# Check if required binaries are available
# Usage: check_binaries <project_root>
# Returns: Binary status as flat data
# Behavior: Checks for bats binary in PATH and reports version if available
check_binaries() {
    local project_root="$1"

    # Check for bats binary in PATH
    if command -v bats >/dev/null 2>&1; then
        local bats_version
        bats_version=$(bats --version 2>/dev/null | head -1 || echo "unknown")
        # Remove "bats " prefix if present
        bats_version="${bats_version#bats }"
        echo "available=true"
        echo "binaries_0=bats"
        echo "binaries_count=1"
        echo "versions_bats=$bats_version"
        echo "container_check=false"
    else
        echo "available=false"
        echo "binaries_0=bats"
        echo "binaries_count=1"
        echo "container_check=false"
    fi

    return 0
}

# Discover test suites in the project
# Usage: discover_test_suites <project_root> <framework_metadata>
# Returns: Test suites as flat data
discover_test_suites() {
    local project_root="$1"
    local framework_metadata="$2"

    # BATS tests are typically in tests/bats/ or test/bats/ directories
    # For now, return empty (stub implementation)
    echo "suites_count=0"
    return 0
}

# Detect build requirements
# Usage: detect_build_requirements <project_root> <framework_metadata>
# Returns: Build requirements as flat data
detect_build_requirements() {
    local project_root="$1"
    local framework_metadata="$2"

    # Bash projects typically don't require building (scripts are interpreted)
    echo "requires_build=false"
    echo "build_steps_count=0"
    echo "build_commands_count=0"
    echo "build_dependencies_count=0"
    echo "build_artifacts_count=0"
    return 0
}

# Get build steps for containerized build
# Usage: get_build_steps <project_root> <build_requirements>
# Returns: Build steps as flat data
get_build_steps() {
    local project_root="$1"
    local build_requirements="$2"

    # Stub implementation (no build steps needed for Bash)
    echo "build_steps_count=0"
    return 0
}

# Execute test suite
# Usage: execute_test_suite <test_suite> <test_image> <execution_config>
# Returns: Execution result as flat data
execute_test_suite() {
    local test_suite="$1"
    local test_image="$2"
    local execution_config="$3"

    # Stub implementation
    echo "exit_code=0"
    echo "duration=0.0"
    echo "execution_method=docker"
    return 0
}

# Parse test results from framework output
# Usage: parse_test_results <output> <exit_code>
# Returns: Parsed results as flat data
parse_test_results() {
    local output="$1"
    local exit_code="$2"

    # Stub implementation
    echo "total_tests=0"
    echo "passed_tests=0"
    echo "failed_tests=0"
    echo "skipped_tests=0"
    echo "test_details_count=0"
    echo "status=passed"
    return 0
}

# Get module metadata
# Usage: get_metadata
# Returns: Module metadata as flat data
get_metadata() {
    echo "module_type=language"
    echo "language=bash"
    echo "frameworks_0=bats"
    echo "frameworks_count=1"
    echo "project_type=shell_script"
    echo "version=0.1.0"
    echo "capabilities_0=testing"
    echo "capabilities_count=1"
    echo "required_binaries_0=bats"
    echo "required_binaries_count=1"
    return 0
}


# End of: mod/languages/bash/mod.sh

# Included from: mod/languages/rust/mod.sh
#!/usr/bin/env bash

# Suitey Rust Module
# Handles Rust language with Cargo framework
# Provides detection, test discovery, build detection, and execution for Rust projects

# Detect if Rust/Cargo project is present
# Usage: detect <project_root>
# Returns: Detection result as flat data
# Behavior: Checks for Cargo.toml (high confidence), Cargo.lock (medium), or .rs files (low)
detect() {
    local project_root="$1"

    # Validate input
    if [[ -z "$project_root" ]] || [[ ! -d "$project_root" ]]; then
        echo "detected=false"
        echo "confidence=low"
        echo "indicators_count=0"
        echo "language=rust"
        echo "frameworks_0=cargo"
        return 0
    fi

    # Check for Cargo.toml file (primary indicator - highest confidence)
    if [[ -f "$project_root/Cargo.toml" ]]; then
        echo "detected=true"
        echo "confidence=high"
        echo "indicators_0=Cargo.toml"
        echo "indicators_count=1"
        echo "language=rust"
        echo "frameworks_0=cargo"
        return 0
    fi

    # Check for Cargo.lock (secondary indicator - medium confidence)
    if [[ -f "$project_root/Cargo.lock" ]]; then
        echo "detected=true"
        echo "confidence=medium"
        echo "indicators_0=Cargo.lock"
        echo "indicators_count=1"
        echo "language=rust"
        echo "frameworks_0=cargo"
        return 0
    fi

    # Check for .rs files (weak indicator - low confidence)
    # Only check in top-level directories to avoid false positives
    if find "$project_root" -maxdepth 2 -name "*.rs" -type f 2>/dev/null | head -1 | grep -q .; then
        echo "detected=true"
        echo "confidence=low"
        echo "indicators_0=rust_source_files"
        echo "indicators_count=1"
        echo "language=rust"
        echo "frameworks_0=cargo"
        return 0
    fi

    # Not detected
    echo "detected=false"
    echo "confidence=low"
    echo "indicators_count=0"
    echo "language=rust"
    echo "frameworks_0=cargo"
    return 0
}

# Check if required binaries are available
# Usage: check_binaries <project_root>
# Returns: Binary status as flat data
# Behavior: Checks for cargo binary in PATH and reports version if available
check_binaries() {
    local project_root="$1"

    # Check for cargo binary in PATH
    if command -v cargo >/dev/null 2>&1; then
        local cargo_version
        cargo_version=$(cargo --version 2>/dev/null | head -1 || echo "unknown")
        # Remove "cargo " prefix if present
        cargo_version="${cargo_version#cargo }"
        echo "available=true"
        echo "binaries_0=cargo"
        echo "binaries_count=1"
        echo "versions_cargo=$cargo_version"
        echo "container_check=false"
    else
        echo "available=false"
        echo "binaries_0=cargo"
        echo "binaries_count=1"
        echo "container_check=false"
    fi

    return 0
}

# Discover test suites in the project
# Usage: discover_test_suites <project_root> <framework_metadata>
# Returns: Test suites as flat data
discover_test_suites() {
    local project_root="$1"
    local framework_metadata="$2"

    # Cargo tests are typically in src/ or tests/ directories
    # For now, return empty (stub implementation)
    echo "suites_count=0"
    return 0
}

# Detect build requirements
# Usage: detect_build_requirements <project_root> <framework_metadata>
# Returns: Build requirements as flat data
detect_build_requirements() {
    local project_root="$1"
    local framework_metadata="$2"

    # Rust projects typically require building before testing
    echo "requires_build=true"
    echo "build_steps_count=1"
    echo "build_commands_0=cargo build --tests"
    echo "build_commands_count=1"
    echo "build_dependencies_count=0"
    echo "build_artifacts_count=0"
    return 0
}

# Get build steps for containerized build
# Usage: get_build_steps <project_root> <build_requirements>
# Returns: Build steps as flat data
get_build_steps() {
    local project_root="$1"
    local build_requirements="$2"

    # Check if building is required
    local requires_build
    requires_build=$(echo "$build_requirements" | grep "^requires_build=" | cut -d'=' -f2 || echo "false")

    if [[ "$requires_build" != "true" ]]; then
    echo "build_steps_count=0"
        return 0
    fi

    # Rust build step
    # Note: Build execution happens in isolated Docker containers.
    # Project directory is mounted read-only (when volume mounts are configured).
    # Build artifacts are stored in container volumes, not in project directory.
    echo "build_steps_count=1"
    echo "build_steps_0_step_name=rust_build"
    echo "build_steps_0_docker_image=rust:1.70-slim"
    echo "build_steps_0_build_command=cargo build --tests"
    echo "build_steps_0_working_directory=/workspace"
    echo "build_steps_0_volume_mounts_count=0"  # No volume mounts needed (project copied into container)
    echo "build_steps_0_volume_mounts_readonly=true"  # When mounts are used, they are read-only
    echo "build_steps_0_environment_variables_count=0"
    echo "build_steps_0_cpu_cores=0"  # Use all available cores

    return 0
}

# Execute test suite
# Usage: execute_test_suite <test_suite> <test_image> <execution_config>
# Returns: Execution result as flat data
execute_test_suite() {
    local test_suite="$1"
    local test_image="$2"
    local execution_config="$3"

    # Stub implementation
    echo "exit_code=0"
    echo "duration=0.0"
    echo "execution_method=docker"
    return 0
}

# Parse test results from framework output
# Usage: parse_test_results <output> <exit_code>
# Returns: Parsed results as flat data
parse_test_results() {
    local output="$1"
    local exit_code="$2"

    # Stub implementation
    echo "total_tests=0"
    echo "passed_tests=0"
    echo "failed_tests=0"
    echo "skipped_tests=0"
    echo "test_details_count=0"
    echo "status=passed"
    return 0
}

# Get module metadata
# Usage: get_metadata
# Returns: Module metadata as flat data
get_metadata() {
    echo "module_type=language"
    echo "language=rust"
    echo "frameworks_0=cargo"
    echo "frameworks_count=1"
    echo "project_type=cargo"
    echo "version=0.1.0"
    echo "capabilities_0=testing"
    echo "capabilities_1=compilation"
    echo "capabilities_count=2"
    echo "required_binaries_0=cargo"
    echo "required_binaries_count=1"
    return 0
}


# End of: mod/languages/rust/mod.sh

# Included from: mod/tools/shellcheck/mod.sh
#!/usr/bin/env bash

# Suitey Tool Module: ShellCheck
# Orchestrates ShellCheck for shell script code quality analysis
# No external dependencies

# Detect if ShellCheck should be run on this project
# Looks for shell scripts (.sh files) in the project
detect() {
    local project_root="$1"

    # Check if project root exists and is readable
    if [[ ! -d "$project_root" || ! -r "$project_root" ]]; then
        echo "detected=false"
        echo "confidence=low"
        echo "indicators_count=0"
        return 0
    fi

    # Look for shell scripts (.sh files)
    local shell_files
    shell_files=$(find "$project_root" -name "*.sh" -type f 2>/dev/null | head -10)

    if [[ -n "$shell_files" ]]; then
        local file_count
        file_count=$(echo "$shell_files" | wc -l)

        echo "detected=true"
        echo "confidence=high"
        echo "indicators_count=1"
        echo "indicators_0=$file_count shell script files found"
        echo "language=shell"
        echo "frameworks_count=0"
    else
        echo "detected=false"
        echo "confidence=low"
        echo "indicators_count=0"
    fi
}

# Check if required binaries are available
# For ShellCheck, this is typically handled in containers
check_binaries() {
    echo "available=true"
    echo "binaries_count=0"
}

# Discover test suites (shell scripts to check)
discover_test_suites() {
    local project_root="$1"
    local framework_metadata="$2"

    # Find all shell scripts in the project
    local shell_files
    shell_files=$(find "$project_root" -name "*.sh" -type f 2>/dev/null)

    if [[ -z "$shell_files" ]]; then
        echo "suites_count=0"
        return 0
    fi

    local file_count=0
    local suite_index=0

    # Group shell scripts into a single suite for efficiency
    echo "suites_count=1"
    echo "suites_0_name=shellcheck"
    echo "suites_0_framework=code-quality"
    echo "suites_0_metadata_0=container_image=koalaman/shellcheck:latest"
    echo "suites_0_metadata_1=command=shellcheck --format=json"
    echo "suites_0_metadata_count=2"
    echo "suites_0_execution_config_count=0"

    # Add all shell files to the suite
    while IFS= read -r file; do
        if [[ -f "$file" && -r "$file" ]]; then
            echo "suites_0_test_files_$file_count=$file"
            ((file_count++))
        fi
    done <<< "$shell_files"

    echo "suites_0_test_files_count=$file_count"
}

# Detect build requirements
# ShellCheck doesn't require building
detect_build_requirements() {
    local project_root="$1"
    local framework_metadata="$2"

    echo "requires_build=false"
    echo "build_steps_count=0"
    echo "build_commands_count=0"
    echo "build_dependencies_count=0"
    echo "build_artifacts_count=0"
}

# Get build steps
# No build steps needed for ShellCheck
get_build_steps() {
    local project_root="$1"
    local build_requirements="$2"

    echo "build_steps_count=0"
}

# Execute test suite (run ShellCheck)
execute_test_suite() {
    local test_suite="$1"
    local test_image="$2"
    local execution_config="$3"

    # For now, return a mock result
    # In real implementation, this would run ShellCheck in a Docker container
    echo "exit_code=0"
    echo "duration=1.2"
    echo "output=[]"
    echo "container_id=mock-container-123"
    echo "execution_method=docker"
    echo "test_image=koalaman/shellcheck:latest"
}

# Parse test results from ShellCheck JSON output
parse_test_results() {
    local output="$1"
    local exit_code="$2"

    # Parse ShellCheck JSON output
    # For now, return mock results
    # In real implementation, this would parse actual JSON output
    if [[ $exit_code -eq 0 ]]; then
        echo "total_tests=1"
        echo "passed_tests=1"
        echo "failed_tests=0"
        echo "skipped_tests=0"
        echo "test_details_count=0"
        echo "status=passed"
    else
        echo "total_tests=1"
        echo "passed_tests=0"
        echo "failed_tests=1"
        echo "skipped_tests=0"
        echo "test_details_0=Shell script contains issues"
        echo "test_details_count=1"
        echo "status=failed"
    fi
}

# Get module metadata
get_metadata() {
    echo "module_type=tool"
    echo "language=shell"
    echo "frameworks_count=0"
    echo "project_type=code-quality"
    echo "version=0.1.0"
    echo "capabilities_0=code-quality"
    echo "capabilities_count=1"
    echo "required_binaries_count=0"
}
# End of: mod/tools/shellcheck/mod.sh

# Main Suitey functionality will be added here

# Exit code constants
# 0 = success
# 1 = tests failed (for future use)
# 2 = suitey error (invalid arguments, internal errors, etc.)
readonly EXIT_SUCCESS=0
readonly EXIT_TESTS_FAILED=1
readonly EXIT_SUITEY_ERROR=2

show_help() {
    cat << 'HELP_EOF'
Suitey v0.1.0 - Cross-platform test runner

Usage: suitey.sh [OPTIONS] [COMMAND]

DESCRIPTION
    Suitey is a cross-platform test runner that automatically detects test suites,
    builds projects, and executes tests in isolated Docker containers.

OPTIONS
    -h, --help          Show this help message and exit
    -v, --version       Show version information and exit

COMMANDS
    (Commands will be implemented in future phases)

EXAMPLES
    suitey.sh --help          Show help information
    suitey.sh --version       Show version information
    suitey.sh                 Show help (default behavior)

For more information, see the Suitey documentation.
HELP_EOF
}

show_version() {
    echo "Suitey v0.1.0"
    echo "Build system functional - ready for implementation"
}

# Run all environment validation checks
# Returns 0 if all checks pass, 1 if any check fails
run_environment_checks() {
    local check_failed=0

    # Run all environment checks
    if ! check_bash_version; then
        check_failed=1
    fi

    if ! check_docker_installed; then
        check_failed=1
    fi

    if ! check_docker_daemon_running; then
        check_failed=1
    fi

    if ! check_tmp_writable; then
        check_failed=1
    fi

    # Return failure if any check failed
    if [[ $check_failed -eq 1 ]]; then
        return 1
    fi

    return 0
}

# Validate and normalize directory path
# Returns normalized absolute path on success, exits with error on failure
validate_directory() {
    local dir_path="$1"
    local original_path="$1"
    
    # Check if directory exists first (before normalization)
    if [[ ! -e "$dir_path" ]]; then
        echo "Error: Directory does not exist: $original_path" >&2
        echo "Run '$0 --help' for usage information." >&2
        return 1
    fi
    
    # Check if it's actually a directory (not a file)
    if [[ ! -d "$dir_path" ]]; then
        echo "Error: Path is not a directory: $original_path" >&2
        echo "Run '$0 --help' for usage information." >&2
        return 1
    fi
    
    # Normalize path: resolve to absolute path
    # Use cd to resolve symlinks and normalize . and ..
    local normalized_path
    if normalized_path="$(cd "$dir_path" 2>/dev/null && pwd)"; then
        dir_path="$normalized_path"
    else
        # If cd failed, try to construct absolute path
        if [[ "$dir_path" != /* ]]; then
            # Relative path - make absolute
            dir_path="$(pwd)/$dir_path"
        fi
    fi
    
    # Check if directory is readable
    if [[ ! -r "$dir_path" ]]; then
        echo "Error: Directory is not readable: $original_path" >&2
        echo "Run '$0 --help' for usage information." >&2
        return 1
    fi
    
    # Return normalized absolute path
    echo "$dir_path"
    return 0
}

main() {
    local target_directory=""
    
    # Parse command-line arguments
    # Options take precedence over directory arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -h|--help)
                show_help
                exit $EXIT_SUCCESS
                ;;
            -v|--version)
                show_version
                exit $EXIT_SUCCESS
                ;;
            -*)
                # Unknown option
                echo "Error: Unknown option '$1'" >&2
                echo "Run '$0 --help' for usage information." >&2
                exit $EXIT_SUITEY_ERROR
                ;;
            *)
                # Non-option argument - treat as directory
                if [[ -n "$target_directory" ]]; then
                    echo "Error: Multiple directory arguments provided. Please specify only one directory." >&2
                    echo "Run '$0 --help' for usage information." >&2
                    exit $EXIT_SUITEY_ERROR
                fi
                target_directory="$1"
                ;;
        esac
        shift
    done
    
    # If no arguments provided, show help
    if [[ -z "$target_directory" ]]; then
        show_help
        exit $EXIT_SUCCESS
    fi
    
    # Validate directory
    local normalized_dir
    normalized_dir=$(validate_directory "$target_directory")
    if [[ $? -ne 0 ]]; then
        exit $EXIT_SUITEY_ERROR
    fi
    
    # Run environment checks before execution
    if ! run_environment_checks; then
        echo "" >&2
        echo "Environment validation failed. Please fix the issues above and try again." >&2
        exit $EXIT_SUITEY_ERROR
    fi
    
    # Register built-in modules
    # These are the core language and framework modules included in the bundle
    if [[ -f "mod/languages/rust/mod.sh" ]]; then
        source "mod/languages/rust/mod.sh" 2>/dev/null || true
        register_module "rust-module" "rust-module" 2>/dev/null || true
    fi

    if [[ -f "mod/languages/bash/mod.sh" ]]; then
        source "mod/languages/bash/mod.sh" 2>/dev/null || true
        register_module "bash-module" "bash-module" 2>/dev/null || true
    fi

    if [[ -f "mod/frameworks/cargo/mod.sh" ]]; then
        source "mod/frameworks/cargo/mod.sh" 2>/dev/null || true
        register_module "cargo-module" "cargo-module" 2>/dev/null || true
    fi

    if [[ -f "mod/frameworks/bats/mod.sh" ]]; then
        source "mod/frameworks/bats/mod.sh" 2>/dev/null || true
        register_module "bats-module" "bats-module" 2>/dev/null || true
    fi

    if [[ -f "mod/tools/shellcheck/mod.sh" ]]; then
        source "mod/tools/shellcheck/mod.sh" 2>/dev/null || true
        register_module "shellcheck-module" "shellcheck-module" 2>/dev/null || true
    fi

    # Perform platform detection
    echo "Suitey v0.1.0"
    echo "Analyzing project: $normalized_dir"
    echo ""

    # Detect platforms in the target directory
    local detection_results
    detection_results=$(detect_platforms "$normalized_dir" 2>/dev/null || echo "platforms_count=0")

    # Display container environment status
    echo "Container Environment:"
    if echo "$detection_results" | grep -q "docker_command_available=true"; then
        echo "   Docker command available"
    else
        echo "   Docker command not found"
    fi

    if echo "$detection_results" | grep -q "docker_daemon_available=true"; then
        echo "   Docker daemon running"
    else
        echo "   Docker daemon not accessible"
    fi

    if echo "$detection_results" | grep -q "container_operations=true"; then
        echo "   Container operations functional"
    else
        echo "   Container operations failed"
    fi

    if echo "$detection_results" | grep -q "network_access=true"; then
        echo "   Network access available"
    else
        echo "   Network access issues"
    fi

    # Display warnings if any
    local warning_count
    warning_count=$(echo "$detection_results" | grep "^docker_warnings_count=" | cut -d'=' -f2)
    if [[ "$warning_count" -gt 0 ]]; then
        echo ""
        echo "Warnings:"
        local i=0
        while [[ $i -lt "$warning_count" ]]; do
            local warning
            warning=$(echo "$detection_results" | grep "^docker_warnings_${i}=" | cut -d'=' -f2-)
            if [[ -n "$warning" ]]; then
                echo "   $warning"
            fi
            ((i++))
        done
    fi

    echo ""
    echo "Platform Detection:"

    # Get platform count
    local platforms_count
    platforms_count=$(echo "$detection_results" | grep "^platforms_count=" | cut -d'=' -f2)

    if [[ "$platforms_count" -eq 0 ]]; then
        echo "  No supported platforms detected in this project."
        echo ""
        echo "Supported platforms:"
        echo "  - Rust (Cargo.toml projects)"
        echo "  - Bash (BATS test projects)"
        exit $EXIT_SUCCESS
    fi

    # Display detected platforms
    # Collect unique language+framework combinations with highest confidence
    local detected_projects=""
    local i=0
    while [[ $i -lt "$platforms_count" ]]; do
        local language
        local framework
        local confidence
        local module_type

        language=$(echo "$detection_results" | grep "^platforms_${i}_language=" | head -1 | cut -d'=' -f2)
        framework=$(echo "$detection_results" | grep "^platforms_${i}_framework=" | head -1 | cut -d'=' -f2)
        confidence=$(echo "$detection_results" | grep "^platforms_${i}_confidence=" | head -1 | cut -d'=' -f2)
        module_type=$(echo "$detection_results" | grep "^platforms_${i}_module_type=" | head -1 | cut -d'=' -f2)

        if [[ -n "$language" ]]; then
            local project_key="$language"
            if [[ -n "$framework" ]]; then
                project_key="$project_key-$framework"
            fi

            # Check if we already have this project combination
            if ! echo "$detected_projects" | grep -q "^$project_key:"; then
                # New project combination
                detected_projects="${detected_projects}$project_key:$confidence:$framework"$'\n'
                echo "   $language project detected (confidence: $confidence)"
                if [[ -n "$framework" ]]; then
                    echo "    Framework: $framework"
                fi
            fi
        fi

        ((i++))
    done

    echo ""
    echo "Full workflow execution will be implemented in future phases."
    exit $EXIT_SUCCESS
}

# Run main function
main "$@"
